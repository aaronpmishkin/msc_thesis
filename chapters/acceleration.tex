%! TEX root = ../main.tex

\chapter{Acceleration}~\label{ch:acceleration}

The focus in Chapters~\ref{ch:sgd} and~\ref{ch:line-search} was on relatively simple first-order methods: \ac{SGD} with a fixed step-size and with a stochastic line-search, respectively.
Now, we move on to Nesterov's famous \ac{AGD} algorithm~\cite{nesterov2004lectures}, which is guaranteed faster convergence in the deterministic setting. 
First, a small degree of background for accelerated methods is developed, with a particular focus on why stochastic acceleration is an interesting question for oracles satisfying interpolation.
Then, the following convergence results are established for \ac{SAGD}:
\begin{enumerate}
    \item \( O\rbr{\sqrt{\frac{\sgc L}{\mu}} \log \rbr{\inv{\epsilon}}} \) iteration complexity for strongly convex \( f \) when \( \rbr{f, \oracle{}} \) satisfies strong growth; this rate is tight with the deterministic analysis when \( \sgc = 1 \).
    \item \( O\rbr{\sqrt{\frac{\sgc L}{\epsilon}} \,} \) iteration complexity for convex \( f \) when \( \rbr{f, \oracle{}} \) satisfies strong growth; again, this rate is tight with \ac{AGD} when \( \sgc = 1 \).
\end{enumerate}
These convergence rates are tighter than existing work by a factor of \( \sqrt{\rho} \)~\citep[Theorems 1-2]{vaswani2019fast}, which we show is comparable to the speed-up obtained by \ac{AGD} over gradient descent in the deterministic setting.
The chapter ends with a brief discussion of acceleration under the weak growth condition, which is an open question.
\newpage

\section{Background}~\label{sec:agd-background}

Recall that Theorems~\ref{thm:sgc-convex} and~\ref{thm:wgc-convex} establish fast linear and sub-linear convergence rates for \ac{SGD} that are comparable with analyses for deterministic problems (e.g. gradient descent).
Unfortunately, the classic work by \citet{nemirovsky1983problem} in 1983 shows that the convergence of gradient descent is not tight with lower-bounds for convex, Lipschitz-smooth functions with deterministic oracles. 
Subsequent by developments by Nesterov~\citep{nesterov1983method, nemirovsky1985optimal, nesterov1988approach} defined a series of first-order methods (terminating with \ac{AGD}) achieving the optimal \( O(\frac{1}{K^2}) \) convergence in the setting analyzed by Nemirovsky and Yudin. 
Algorithms with this rate are called \emph{accelerated} and subsequent research has generated a large number of accelerated algorithms for deterministic problems.
That acceleration literature firmly places gradient descent as a sub-optimal algorithm and also casts the analyses in \autoref{ch:sgd} in a new light.
In particular, the obvious parallels between \ac{SGD} under interpolation and deterministic gradient descent hint that \ac{SAGD} may be a faster algorithm than \ac{SGD} in restricted settings. 

In fact, restricting the \ac{SFO} is necessary for any hope of proving an accelerated rate for \ac{SAGD}.
Black-box accelerated methods which do not leverage structural information about \( f \) rapidly accumulate errors when used with general stochastic oracles \( \oracle{} \)~\citep{devolder2014first, schmidt2011convergence}. 
Such error accumulation prevents acceleration, as shown by tight \( \Omega(\epsilon^{-2}) \) and \( \Omega(\epsilon^{-1}) \) lower bounds for the iteration complexity of convex and strongly-convex minimization with general \acp{SFO}, respectively~\citep{nemirovsky1983problem, agarwal2012information, raginsky2011information}.
The key question is whether or not the interpolation setting is restrictive enough to exclude these lower bounds and permit acceleration;
the goal of this chapter is to extend the estimating sequences framework~\citep{nesterov2004lectures} to show that \ac{SAGD} does in fact achieve an accelerated convergence rate when the strong growth condition holds.

\begin{figure}[t]
    \centering
    \begin{procedure}{Stochastic \ac{AGD} (\ac{SAGD})}
    \item Choose an initial point \( \w_0 = y_0 \in \R^d \) and \( \alpha_0 \in \rbr{0,1} \).
        \item For each iteration \( k \geq 0 \):
            \begin{enumerate}
                \item Query \oracle{} for \( \grad(\wk, \zk) \). 
                \item Update input as\vspace{-1ex}%
                \[ \wkk = \wk - \eta \grad(\yk, \zk). \]
                \item Compute 
                    \( \alpha_{k+1}^2 = (1 - \alpha_k)\alpha_k^2 + \frac{\mu}{L} \alpha_{k+1} \).
                \item Set \( \beta_{k} = \frac{\alpha_k (1-\alpha_k)}{\alpha_k^2 + \alpha_{k+1}} \) and extrapolate as
                    \[ \ykk = \wkk + \beta_k (\wkk - \wk).  \] 
            \end{enumerate}
    \end{procedure}
    \caption[Classic procedural definition of Nesterov's accelerated gradient descent algorithm.]%
            {Classical form of \ac{AGD} with stochastic gradients. 
            This algorithm is equivalent to that proposed and analyzed by \citet{vaswani2019fast}.}%
    \label{procedure:accelerated-sgd}
\end{figure} 

We now introduce \ac{AGD} in greater detail before discussing its analysis using estimating sequences. 
\autoref{procedure:accelerated-sgd} presents the classic procedural definition for \ac{AGD} with stochastic gradients; the major differences to \ac{SGD} are
\begin{inparaenum}[(i)]
\item \ac{AGD} introduces a secondary sequence of points \( \seq{\yk} \) that are calculated by extrapolating from the primary sequence \( \seq{\wk} \),
\item the primary sequence is updated using stochastic gradient steps from the extrapolation points \( \yk \), rather than the proceeding iterate \( \wk \), and 
\item the extrapolation step-size is computed from a quadratic equation that depends on \( \mu \) and \( L \), where \( \mu = 0 \) for convex functions. 
\end{inparaenum}
The procedure for selecting the \( \betak \) step-sizes is particularly unintuitive and has lead some authors to describe the fast convergence of \ac{AGD} as an ``algebraic trick''~\citep{allen2014linear}.
Partly for this reason, we shall re-express \ac{AGD} as alternating steps of gradient descent on \( f \) and a new function \( \phi : \R^d \rightarrow \R \).


\section{Estimating Sequences}~\label{sec:estimating-sequences}

Let \( \seq{\wk} \) be the sequence of iterates produced by \ac{SGD}.\@
\autoref{lemma:sgc-decrease-condition} shows that the following descent condition holds if \( f \) is \( L \)-smooth and \( \rbr{f, \oracle{}} \) satisfies strong growth:
\[ \Ek\sbr{f(\wkk)} \leq f(\wk) - \eta \rbr{1 - \frac{\sgc L \eta}{2}}\norm{\grad(\wk)}^2. \]
Choosing \( \eta = \frac{1}{\sgc L} \) minimizes the right-hand-side, in which case \ac{SGD} can be viewed as iterative minimization of the quadratic upper-bound on \( f \) given by smoothness and strong growth. 
This procedure decreases \( f \) at each iteration by assuming \emph{worst-case} curvature and noise at every iterate \( \wk \). 
Inuitively, \ac{SGD} is sub-optimal exactly because it employs global worst-case bounds regardless of past knowledge --- e.g.\ the \( \seq{f(\wk, \zk)} \) and \( \seq{\grad(\wk, \zk)} \) sequences. 

We can instead consider an algorithm which builds local approximations to \( f \) based on all past information accumulated by the procedure.
Sufficiently accurate approximations can replace or augment the worst-case curvature assumption and allow for faster convergence. 
This intuition is formalized by the notion of estimating sequences~\citep{nesterov2004lectures}.

\begin{definition}[Estimating Sequences]~\label{def:estimating_sequences}
    The two sequences \( \seq{\lambda_k}_{k=0}^\infty \) and \( \seq{\phi_k}_{k=0}^\infty \) are called estimating sequences for \( f : \R^d \rightarrow \R \) if
    \begin{inparaenum}[(i)]
        \item \( \lambda_k \geq 0 \) for all \( k \in \bbN \);
        \item \( \lim_{k\rightarrow \infty} \lambda_k = 0 \); and
        \item for all \( k \in \bbN \) and \( \w \in \R^d \), the functions \( \phi_k: \R^d \rightarrow \R \) satisfy
        \[ \phi_k(\w) \leq (1 - \lambda_k) f(\w) + \lambda_k \phi_0(\w). \]
    \end{inparaenum}
\end{definition}

If \( \phi_0 \) is chosen so that \( \phi_0(\w) \approx f(\w) \) in a neighborhood of \( \w_0 \), then \( \seq{\phi_k}_{k=0}^\infty \) matches our intuition of a sequence of improving, local approximations to \( f \).
This is particularly true since the conditions on \( \lambda_k \) guarantee \( \lim_{k \rightarrow \infty} \phi_k \leq f \) point-wise --- i.e.\ \( \phi \) eventually becomes a \emph{global} lower bound on \( f \).
Finally, and most importantly, if \( f(\wk) \leq \inf_x \phi_k(x) \) for all \( k \), then we obtain
\begin{align*}
    f(\wk) \leq \phi_k(\wopt) &\leq (1 - \lambda_k) f(\wopt) + \phi_0(\wopt)\\
    \implies f(\wk) - f(\wopt) &\leq \lambda_k \rbr{\phi_0(\wopt) - f(\wopt)},
\end{align*}
and the convergence rate of \( \lambda_k \) controls convergence of \( \wk \) to \( \wopt \).
Establishing this last property will be the core of our analysis.
Let us now fix a choice of estimating sequences following~\citet[Lemma 2.2.2]{nesterov2004lectures}. 

\newpage

\begin{restatable}{lemma}{estimatingSequence}\label{lemma:estimating-sequence}
    Suppose \( f \) is a \( \mu \)-strongly-convex function (with \( \mu = 0 \) in the convex case).
    Let \( \seq{\alphak}_{k=0}^\infty \) be a sequence of real numbers such that \( \alphak \in (0,1) \) and \( \sum_{i=1}^\infty \alphak = \infty \). 
    Let \( \seq{\yk}_{k=0}^\infty \) be an arbitrary of sequence of points in \( \R^n \).
    Then the following pair of sequences are estimating sequences for \( f \):
    \begin{align*}
        \lambda_{k+1} &= (1- \alpha_k) \lambda_k\\
        \phi_{k+1}(\w) &= (1-\alpha_k) \phi_k(\w) + \alpha_k \rbr{f(\yk) + \abr{\grad(\yk), \w - \yk} + \frac{\mu}{2} \norm{\w - \yk}^2},
    \end{align*}
    where \( \lambda_0 = 1 \) and \( \phi_0(x) = \phi_0^* + \frac{\gamma_0}{2}\norm{x - v_0}^2 \) with arbitrary \( \gamma_0 \geq 0 \), \( v_0 \in \R^n \) and \( \phi_0^* \in \R \).
\end{restatable}

\autoref{lemma:estimating-sequence} provides a recipe for generating local approximations of \( f \) by updating \( \phi_k \) with a global minorant of \( f \) ``centered'' at \( \yk \). 
This minorant is quadratic when \( f \) is strongly-convex and linear when \( f \) is convex (since \( \mu = 0 \)).
An important property of the update is that it preserves the canonical form
\begin{align}
    \phi_k(\w) = \phi_k^* + \frac{\gamma_k}{2}\norm{\w - \vk}^2,\label{eq:canonical-form}
\end{align}
where \( \gamma_{k+1} = \rbr{1 - \alphak}\gamma_k + \alphak \mu \), \( \vk = \inv{\gamma_{k+1}} \rbr{(1-\alphak)\vk + \alphak \mu \, y_k - \alphak \grad(\yk)}, \) and 
\begin{align*}
    \phi^*_{k+1} &= (1 - \alphak) \phi^*_k + \alphak\bigg(f(\yk) - \frac{\alphak}{2 \gamma_{k+1}} \norm{\grad(\yk)}^2 + \frac{(1 - \alphak)\gamma_k}{\gamma_{k+1}} \big(\frac{\mu}{2}\norm{\yk - \vk}^2\\ 
    & \hspace{20em} + \abr{\grad(\yk), \vk - \yk} \big) \bigg). 
\end{align*}
See~\citet[Lemma 2.2.3]{nesterov2004lectures} for proof.
While cumbersome, the canonical form for \( \phi_k \) allows \ac{AGD} to be reformulated as alternating descent steps on \( \phi_k \) and \( f \).
We do this now.

\begin{figure}[t]
    \centering
    \begin{procedure}{\textsc{Reformulated} \ac{SAGD} (\acs{RSAGD})}
        \item Choose an initial point \( \w_0 = y_0 \in \R^d \) and \( \gamma_0 \geq 0 \).
        \item For each iteration \( k \geq 0 \):
            \begin{enumerate}
                \item Compute 
                    \( \alpha^2_k = \frac{\gamma_{k+1}}{\sgc L } = \frac{(1-\alpha_k)\gamma_k + \alpha_k \mu}{\sgc L} \).
                \item Extrapolate as
                    \[ \yk = \wk - \frac{\alpha_k}{\gamma_k + \alpha_k \mu} \nabla \phi_k(\wk) \]

                \item Query \oracle{} for \( \grad(\yk, \zk) \). 
                \item Update input as\vspace{-1ex}%
                    \[ \wkk = \wk - \frac{1}{\sgc L} \grad(\yk, \zk). \]
            \end{enumerate}
    \end{procedure}
    \caption[Reformulation of Nesterov's accelerated gradient descent as an alternating descent procedure.]%
        {Reformulation of \ac{AGD} as alternating steps of gradient descent on \( \phi \) and \ac{SGD} on \( f \).}%
    \label{procedure:reformulated-agd}
\end{figure}

\autoref{procedure:reformulated-agd} re-expresses \ac{SAGD} in the notation of estimating sequences.
We call the resulting algorithm \acs{RSAGD}: reformulated stochastic accelerated gradient descent.\acused{RSAGD}
Note that the order of the \( \seq{\yk} \) and \( \seq{\wk} \) sequences has been reversed for convenience; this does not affect the algorithm.
Formulating \ac{SAGD} as an estimating sequence procedure is particularly nice for two reasons:
\begin{inparaenum}[(1)]
\item the extrapolation step \[ \ykk = \wkk + \beta_k (\wkk - \wk), \] takes on an intuitive interpretation as gradient descent on \( \phi_{k+1} \), and 
\item if \[  \E [\inf \phi_k(x)] \geq \E [f(\wk)], \] holds for all \( k \), then convergence of \ac{SAGD} is determined entirely by convergence of \( \seq{\lambda_k} \).
\end{inparaenum}
We will show this second property shortly, but first we formally state the equivalence of \ac{RSAGD} and \ac{SAGD}, with proof given in \autoref{app:estimating-sequences}.

\begin{restatable}{lemma}{agdEquivalence}\label{lemma:agdEquivalence}
    Let \( \seq{\lambda_k} \) and \( \seq{\phi_k} \) be as defined in \autoref{lemma:estimating-sequence}.
    Then the \ac{RSAGD} and \ac{SAGD} algorithms are equivalent.
\end{restatable}

Now we establish the main result of this section; convergence for convex and \( \mu \)-strongly-convex functions will follow almost immediately in Sections~\ref{sec:agd-sc} and~\ref{sec:agd-convex}.
The proof relies on the expected decrease condition given by \autoref{lemma:sgc-decrease-condition}, which holds for \( L \)-smooth \( f \) when \( \rbr{f, \oracle{}} \) satisfies strong growth.
Intuitively, this condition shows that \ac{SGD} obtains similar per-iteration improvement (in expectation) to deterministic gradient descent if strong growth holds. 
Finally, note that the expectations in the lemma below are taken with respect to the entire sequence of random variables \( \seq{\z_t}_{t=0}^k \).

\begin{restatable}{lemma}{estimatingSequenceBound}~\label{lemma:estimating-sequence-bound}
    Let \( f \) be a \( \mu \)-strongly-convex, \( L \)-smooth function (with \( \mu = 0 \) in the convex case) and \oracle{} a \ac{SFO} such that \( \rbr{f, \oracle{}} \) satisfies strong growth with parameter \( \sgc \).
    If \( \phi_0^* = f(\w_0) \) and \( \gamma_0 \geq 0 \) is independent of the random process \( \seq{\zk} \), then the iterates generated by \ac{RSAGD} satisfy 
    \[ \E [\inf_{w} \phi_k(\w)] = \E \sbr{\phi_k^*} \geq \E [f(\wk)], \]
    for all \( k \).
\end{restatable}
\noindent See \autoref{app:estimating-sequences} for proof.

\section{Convergence for Strongly-Convex Functions}\label{sec:agd-sc}

It is now straightforward to prove \ac{RSAGD} converges at an accelerated rate. 
Earlier, we showed that \autoref{lemma:estimating-sequence-bound} and the fact that \( \seq{\lambda_k} \), and \( \seq{\phi_k} \) are estimates sequences for \( f \) together imply
\[ f(\wk) - f(\wopt) \leq \lambda_k \rbr{\phi_0(\wopt) - f(\wopt)} = \lambda_k \rbr{f(\w_0) - f(\wopt) + \frac{\gamma_0}{2} \norm{\w_0 - \wopt}^2}. \]
This is \emph{almost} an explicit convergence rate. 
The last step of the analysis is to select an appropriate value for \( \gamma_0 \) and analyze convergence of the \( \lambda_k \) sequence, which is done in the following theorem.

\begin{restatable}{theorem}{scAGD}~\label{thm:sc-agd}
    Let \( f \) be a \( \mu \)-strongly-convex, \( L \)-smooth function with \( \mu > 0 \) and \oracle{} a \ac{SFO} such that \( \rbr{f, \oracle{}} \) satisfies strong growth with parameter \( \sgc \).
    Moreover, choose \( \gamma_0 = \mu \), \( v_0 = \w_0 \), and \( \phi_0^* = f(\w_0) \).
    Then \ac{RSAGD} converges as  
    \[ \E\sbr{f(\w_K)} - f(\wopt) \leq \rbr{1 - \sqrt{\frac{\mu}{\sgc L}}}^{K} \rbr{ f(w_0) - f(\wopt) + \frac{\mu}{2} \norm{\wopt - \w_0}^2 }.  \]
\end{restatable}
\noindent See \autoref{app:agd-sc} for proof.\hfill \break

Accelerated convergence of \ac{SAGD} is an immediate corollary of the equivalence of \ac{RSAGD} and \ac{SAGD} (\autoref{lemma:agdEquivalence}) and \autoref{thm:sc-agd}.
The only catch here is that \( \alpha_0 \) must be selected to correspond to \( \gamma_0 = \mu \).
It is easy to see that choosing \( \alpha_0 = \sqrt{\frac{\mu}{\sgc L}} \) for \ac{SAGD} is identical to \( \gamma_0 = \mu \) in \ac{RSAGD} since \( \alpha_0^2 = \rbr{(1 - \alpha_0)\gamma_0 + \alpha_0 \mu} / (\sgc L) \).

\begin{corollary}~\label{cor:sc-agd}
    Let \( f \) be a \( \mu \)-strongly-convex, \( L \)-smooth function with \( \mu > 0 \) and \oracle{} a \ac{SFO} such that \( \rbr{f, \oracle{}} \) satisfies strong growth with parameter \( \sgc \).
    Moreover, choose \( \alpha_0 = \sqrt{\frac{\mu}{\sgc L}} \). 
    Then \ac{SAGD} converges as  
    \[ \E\sbr{f(\w_K)} - f(\wopt) \leq \rbr{1 - \sqrt{\frac{\mu}{\sgc L}}}^{K} \rbr{ f(w_0) - f(\wopt) + \frac{\mu}{2} \norm{\wopt - \w_0}^2 }.  \]
\end{corollary}

\autoref{cor:sc-agd} looks like a potential positive answer to the problem of whether or not acceleration is possible in the interpolation setting.
\ac{SAGD}'s \( O\rbr{ \exp \cbr{- \sqrt{\frac{\mu}{\sgc L}} \, K}} \) convergence certainly improves on that of constant step-size \ac{SGD}.
Moreover, the form of improvement --- taking the square-root of the condition number --- is identical to that obtained by \ac{AGD} over deterministic gradient descent~\citep{nesterov2004lectures}.
In this sense, \ac{SAGD} is a true accelerated algorithm.

However, \ac{SAGD} is, in general, not optimal for \( L \)-smooth, strongly-convex functions. 
For example, if \( \oracle{} \) is \( \Lmax \) individually-smooth, then \( \sgc \leq \frac{\Lmax}{\mu} \) and the worst-case complexity is \( O\rbr{ \exp \cbr{- \sqrt{\mu^2 / \Lmax L} \, K}}\).
This is roughly equivalent to gradient descent, implying \ac{SAGD} is not accelerated. 
Overall, we conclude \ac{SAGD} is an accelerated method only relative to the complexity of other stochastic algorithms under interpolation.

The analysis of \ac{SAGD} for strongly-convex functions presented here is strictly tighter than prior work by \citet[Theorem 1]{vaswani2019fast}, who also studied convergence under strong growth.
\autoref{table:agd-comparison} compares the two complexities; the major difference is that \( \sgc \) is squared in the rate obtain by Vaswani et al. 
Recalling \( \sgc = \sfrac{\Lmax}{\mu} \) in the worst-case implies that our result is faster by a multiple of the condition number for individually-smooth \( \oracle{} \) satisfying minimizer interpolation, which is comparable to the improvement obtained by \ac{AGD} over deterministic gradient descent.
This tighter rate addresses the criticism of~\citet{liu2020accelerating}, who showed that the convergence speed shown by Vaswani et al.\ could be slower than \ac{SGD} in some circumstances. 

\begin{table}[t]
    \centering
    \begin{tabular}{c c c  }\toprule
        \multirow{2}{*}{Assumptions} & \multicolumn{2}{c}{Convergence Rate}\\%
        \cmidrule(lr){2-3} 
                 & \multicolumn{1}{c}{Ours} & \multicolumn{1}{c}{VBS}\\ \midrule
        \( \mu \)-SC & \( O\rbr{\sqrt{\frac{\rho L}{\mu}} \log\rbr{\frac{1}{\epsilon}}} \)% 
                     & \( O\rbr{\sqrt{\frac{\rho^2 L}{\mu}} \log\rbr{\frac{1}{\epsilon}}} \) \\ \addlinespace
        Convex       & \( O\rbr{\sqrt{\frac{\sgc L}{\epsilon}}\,} \)%
                     & \( O\rbr{\sqrt{\frac{\sgc^2 L}{\epsilon}}\,} \)\\ \addlinespace 
        \end{tabular}
        \caption[Comparison of iteration complexities for stochastic acceleration schemes under strong growth.]%
           {Comparison of iteration complexities for stochastic acceleration schemes under the strong growth condition.
            For each result, we report the complexity obtained with the optimal step-size and momentum parameter according to the analysis.
            Our results are tighter than VBS~\citep{vaswani2019fast} by a factor of \( \sqrt{\rho} \).
            \citet{liu2020accelerating} also consider stochastic acceleration under interpolation, but make substantially different assumptions about \( f \) and thus obtain difficult-to-compare rates.
            Accordingly, we omit their algorithm, MaSS, from this table. }%
    \label{table:agd-comparison}
\end{table}

\section{Convergence for Convex Functions}\label{sec:agd-convex}

Our final result for \ac{SAGD} addresses convergence for convex functions under the strong growth condition. 
The following theorem shows that \ac{RSAGD} obtains an accelerated \( O\rbr{\frac{\sgc L}{K^2}} \) complexity.
Invoking the correspondence between \ac{RSAGD} and \ac{SAGD} leads an identical result for the latter algorithm as a corollary.
This improves on prior analyses by a factor of \( \sqrt{\sgc} \)~\citep[Theorem 2]{vaswani2019fast}. 
See~\autoref{table:agd-comparison} for additional details.

\begin{restatable}{theorem}{convexAGD}~\label{thm:convex-agd}
    Let \( f \) be a convex, \( L \)-smooth function and \oracle{} a \ac{SFO} such that \( \rbr{f, \oracle{}} \) satisfies strong growth with parameter \( \sgc \).
    Moreover, choose \( \gamma_0 = 2 \sgc L \), \( v_0 = \w_0 \), and \( \phi_0^* = f(\w_0) \).
    Then \ac{RSAGD} converges as  
    \[ \E\sbr{f(\w_K)} - f(\wopt) \leq \frac{2}{\rbr{K+1}^2}\rbr{f(\w_0) - f(\wopt) + \sgc L \norm{\w_0 - \wopt}^2 }. \]
\end{restatable}

\noindent The proof is given in \autoref{app:agd-convex}.
Again, we have the following corollary from the equivalence of \ac{RSAGD} and \ac{SAGD}. 
Note that the choice of \( \alpha_0 = \sqrt{2} - 1 \) for \ac{SAGD} is identical to the choice of \( \gamma_0 = 2 \sgc L \) in \ac{RSAGD}, since \( \alpha_0^2 = (1 - \alpha_0) \gamma_0 / (\sgc L) \).

\begin{corollary}
    Let \( f \) be a convex, \( L \)-smooth function and \oracle{} a \ac{SFO} such that \( \rbr{f, \oracle{}} \) satisfies strong growth with parameter \( \sgc \).
    Moreover, choose \( \alpha_0 = \sqrt{2} - 1 \).
    Then \ac{RSAGD} converges as  
    \[ \E\sbr{f(\w_K)} - f(\wopt) \leq \frac{2}{\rbr{K+1}^2}\rbr{f(\w_0) - f(\wopt) + \sgc L \norm{\w_0 - \wopt}^2 }. \]
\end{corollary}

\section{Acceleration under Weak Growth}~\label{sec:agd-weak-growth}

 An disappointing aspect of \autoref{thm:convex-agd} is that it uses the strong growth condition, rather than weak growth.
 Recall that sufficient conditions for strong growth are minimizer interpolation, individual-smoothness, and the \ac{PL} condition (\autoref{lemma:interpolation-to-sgc}). 
 Thus, requiring strong growth in this case has the \emph{flavour} of requiring \( f \) to be strongly-convex, but only leads to sub-linear convergence.
This is quite disappointing, but it is not obvious that the assumption can be relaxed.

The natural condition for convex functions used throughout this work is the weak growth condition.
The main issue using weak growth in \autoref{thm:convex-agd} is that we lose access to the progress condition 
\[ \Ek \sbr{f(\wkk)} \leq f(\wk) - \etak \rbr{1 - \frac{\sgc L \eta}{2}} \norm{\grad(\wk)}^2, \]
from \autoref{lemma:sgc-decrease-condition}, which was a critical piece of our analysis for \ac{SAGD}. 
The direct proof using estimating sequences collapses without obvious recourse.
\iflong%
An alternative, which shows progress in terms of the optimality gap and distance to the minimizer can be derived, which we state now.

\begin{restatable}{lemma}{wgcDecreaseCondition}~\label{lemma:wgc-decrease-condition}
     Let \( f \) be an \( L \)-smooth, convex function and \oracle{} an individually-smooth and convex \ac{SFO} such that \( \rbr{f, \oracle{}} \) satisfies the weak growth condition with parameter \( \wgc \). 
    Then the following decrease condition holds for stochastic gradient descent with step-size \( \eta \leq \frac{1}{\Lmax} \):
    \[ \E_{\zk} \sbr{f(\wkk) + \wgc L^2 \eta \norm{\wkk - \wopt}^2} \leq f(\wk) + \wgc L^2 \eta \norm{\wk - \wopt}^2 - \eta \norm{\grad(\wk)}^2. \]
\end{restatable}
\noindent See \autoref{app:agd-additional-lemmas} for proof. \hfill \break

It is not clear whether the analysis of \ac{SAGD} presented here can be modified to use \autoref{lemma:wgc-decrease-condition}. 
The naive approach is to change the choice of estimating sequences so that \( \phi_k \)  depends on \( \norm{\wk - \wopt}^2 \); a version of \autoref{lemma:estimating-sequence-bound} would then be proved using the new decrease condition.
However, such work is outside the scope of this thesis and is left for subsequent research.
Accelerated convergence under weak growth remains an open question.
\fi

\section{Conclusions}~\label{sec:acceleration-conclusions}

This chapter has tackled the challenging question of stochastic Nesterov acceleration under interpolation-type conditions.
Chapters~\ref{ch:sgd} and~\ref{ch:line-search} showed that \ac{SGD} converges nearly as quickly as deterministic gradient descent when minimizer interpolation is satisfied.
These positive results suggested that stochastic acceleration might also be possible in a similarly restricted setting.
Following \citet{vaswani2019fast}, we investigated a straightforward modification of \ac{AGD} to use stochastic gradients. 
However, unlike \citet{vaswani2019fast}, we develop an argument based on estimating sequences~\citep{nesterov2004lectures}.
This allowed us to cast \ac{SAGD} as an alternating minimization procedure on \( f \) and a sequence of curvature estimates, \( \phi_k \).
Careful analysis of this procedure showed that \ac{SAGD} obtains accelerated rates of convergence in both the strongly-convex and convex settings (Theorems~\ref{thm:sc-agd} and~\ref{thm:convex-agd}) when strong growth holds.
Our theorems improve over \citet{vaswani2019fast} by a factor of \( \sqrt{\sgc} \), which is equal to \( \sqrt{\frac{\Lmax}{\mu}} \) in the worst case. 



