%! TEX root = ../main.tex

\chapter{Acceleration}~\label{ch:acceleration}

The focus in Chapters~\ref{ch:sgd} and~\ref{ch:line-search} was on relatively simple first-order methods: SGD with a fixed step-size and with a stochastic line-search, respectively.
Now, we move on to a more complicated algorithm which is guaranteed faster rates of convergence in the deterministic setting.
The last first-order method which is analyzed in this work is Nesterov's famous accelerated gradient method (AGD)~\cite[Eq. 2.2.20]{nesterov2004lectures}.
First, a small degree of background for accelerated methods is developed, with a particular focus on why stochastic acceleration is an interesting question for oracles satisfying interpolation.
Then, the following convergence results are established for stochastic AGD:
\begin{enumerate}
    \item \( O\rbr{\exp\cbr{- \sqrt{\frac{\mu}{\rho L}} \, K}} \) convergence for strongly convex \( f \) when \( \rbr{f, \oracle{}} \) satisfies strong growth; this rate is tight with the deterministic analysis when \( \rho = 1 \).
    \item \( O(\frac{1}{K^2}) \)  convergence for convex \( f \) when \( \rbr{f, \oracle{}} \) satisfies strong growth; again, this rate is tight with AGD for deterministic oracles when \( \rho = 1 \).
\end{enumerate}
The chapter ends with a brief discussion of acceleration under the weak growth condition, which is an open question.

Recall that Theorems~\ref{thm:sgc-convex} and~\ref{thm:wgc-convex} established fast linear and sub-linear convergence rates for SGD that were comparable with analyses for deterministic problems (e.g. gradient descent).
Unfortunately, the classic work by \citet{nemirovsky1983problem} in 1983 shows that the convergence of gradient descent is not tight with lower-bounds for convex, Lipschitz-smooth functions with deterministic oracles. 
Subsequent by developments by Nesterov~\citep{nesterov1983method, nemirovskii1985optimal, nesterov1988approach} defined a series of first-order methods (terminating with AGD) achieving optimal \( O(\frac{1}{K^2}) \) convergence in the setting analyzed by Nemirovsky and Yudin. 
Algorithms with this optimal rate are called \emph{accelerated} and subsequent research has generated a large number of accelerated algorithms for deterministic problems.
That acceleration literature firmly places gradient descent as a sub-optimal algorithm and also casts the analyses in \autoref{ch:sgd} in a new light.
In particular, the close relationship between SGD under interpolation and deterministic gradient descent indicates that stochastic AGD may be a faster algorithm than SGD in restricted settings. 

In fact, restricting the SFO is necessary for any hope of proving an accelerated rate for AGD.
Black-box accelerated methods which do not leverage structural information about \( f \) rapidly accumulate errors when used with general stochastic oracles \( \oracle{} \)~\citep{devolder2014first, schmidt2011convergence}. 
Such error accumulation prevents convergence. 
The key question now is whether or not the interpolation setting is restricted enough to permit acceleration;
The remainder of this chapter extends the estimating sequences framework~\citep{nesterov2004lectures} to show that stochastic AGD does in fact achieve an accelerated convergence rate when the strong growth condition holds.

\begin{figure}[t]
    \centering
    \begin{procedure}{Stochastic AGD}
        \item Choose an initial point \( \w_0 = y_0 \in \R^d \).
        \item For each iteration \( k \geq 0 \):
            \begin{enumerate}
                \item Query \oracle{} for \( f(\wk, \zk), \: \grad(\wk, \zk) \). 
                \item Update input as\vspace{-1ex}%
                \[ \wkk = \wk - \eta \grad(\yk, \zk). \]
                \item Compute 
                    \( \alpha_{k+1}^2 = (1 - \alpha_k)\alpha_k^2 + \frac{\mu}{L} \alpha_{k+1} \).
                \item Set \( \beta_{k} = \frac{\alpha_k (1-\alpha_k)}{\alpha_k^2 + \alpha_{k+1}} \) and extrapolate as
                    \[ \ykk = \wkk + \beta_k (\wkk - \wk).  \] 
            \end{enumerate}
    \end{procedure}
     \caption{Nesterov's accelerated gradient method with stochastic gradients.}%
    \label{procedure:accelerated-sgd}
\end{figure}

\begin{table}[t]
    \centering
    \begin{tabular}{c c c  }\toprule
        \multirow{2}{*}{Assumptions} & \multicolumn{2}{c}{Convergence Rate}\\%
        \cmidrule(lr){2-3} 
                 & \multicolumn{1}{c}{Ours} & \multicolumn{1}{c}{\citet{vaswani2019painless}}\\ \midrule
        \( \mu \)-SC & \( O\rbr{\exp\cbr{- \frac{\mu}{\Lmax} \, K}} \)% 
                     & \( O\rbr{\exp\cbr{- \frac{\bar \mu}{\Lmax} \, K}} \) \\ \addlinespace
    Convex       & \( O\rbr{\frac{\Lmax}{2 \, K}} \)%
                 & \( O\rbr{\frac{3 \Lmax}{K}} \)\\ \addlinespace 
        \end{tabular}
        \caption{Comparison of convergence rates for stochastic acceleration schemes.}%
    \label{table:ls-comparison}
\end{table}



\section{Estimating Sequences}\label{sec:estimating-sequences}

\section{Convergence for Strongly-Convex Functions}\label{sec:agd-sc}

\section{Convergence for Convex Functions}\label{sec:agd-convex}


Suppose that we are trying to minimize a convex function \( f : \R^n \rightarrow \R \) using a deterministic gradient method of the form
\[ \wkk = \wk - \etak \grad(\wk). \]
For L-smooth functions, this approach enforces a descent condition on \( f(\wkk) \) via the quadratic upper bound
\[ f(w) \leq f(\wk) + \abr{\grad(\wk, w - \wk)} + \frac{L}{2}\norm{w - \wk}^2. \]
However, gradient methods of this form assume worst-case curvature of \( f \).
We can instead consider building a sequence of approximations to \( f \) based on local information.
This idea is formalized by the notion of estimating sequences~\citep{nesterov2004lectures}.\\

\begin{definition}[Estimating Sequences]~\label{def:estimating_sequences}
    The two sequences \( \cbr{\lambda_k}_{k=0}^\infty \) and \( \cbr{\phi_k}_{k=0}^\infty \) are called estimating sequences of the function \( f \) if
    \begin{itemize}
        \item \( \lambda_k \geq 0 \) and for all \( k \) and \( \lim_{k\rightarrow \infty} \lambda_k = 0 \);
        \item for all \( k \in \bbN \) and \( x \in \R^n \), the functions \( \phi_k: \R^n \rightarrow \R \) satisfy
        \[ \phi_k(x) \leq (1 - \lambda_k) f(x) + \lambda_k \phi_0(x). \]
    \end{itemize}
\end{definition}

\textbf{Comments on Definition 1}:
\begin{enumerate}
    \item As \( k \rightarrow \infty \), we are guaranteed that the functions \( \phi_k \) approach a global minorant of \( f \).
    \item If \( \phi_0 \) is chosen so that for all \( x \in \R^d \)
    \[ \phi_0(x) \geq f(x), \]
    then \( {\phi_k}_{k=0}^\infty \) can be (intuitively) thought of as a sequence of improving estimates of \( f \).
    \item If \( \inf_x \phi_k(x) \geq f(\wk) \), then we obtain
    \begin{align*}
        f(\wk) \leq \phi_k(\wopt) &\leq (1 - \lambda_k) f(\wopt) + \phi_0(\wopt)\\
        \implies f(\wk) - \fopt &\leq \lambda_k \rbr{f(\wopt) + \phi_0(\wopt)},
    \end{align*}
    by definition of the estimating sequence. Moreover, the convergence rate of \( \lambda_k \) controls convergence of \( \wk \) to \( \wopt \).\\
\end{enumerate}

Let's fix a choice of estimating sequences and proceed with an analysis of the resulting algorithm.
Our choice follows \citet{nesterov2004lectures}.\\

\textbf{Fixing Our Estimating Sequences}:

Let \( \cbr{\alpha_k}_{k=0}^\infty \) be a sequence of real numbers such that \( \alpha_k \in (0,1) \) and \( \sum_{i=1}^\infty \alpha_k = \infty \). Let \( \cbr{\yk}_{k=0}^\infty \) be an arbitrary of sequence of points in \( \R^n \).
The estimating sequences which we will analyze are given as
\begin{itemize}
    \item \( \lambda_0 = 1 \);
    \item \( \lambda_{k+1} = (1- \alpha_k) \lambda_k \);
    \item \( \phi_0(x) = \phi_0^* + \norm{x - v_0}^2 \), where \( v_0 \in \R^n \) and \( \phi_0^* \in \R \) are arbitrary;
    \item \( \phi_{k+1}(x) = (1-\alpha_k) \phi_k(x) + \alpha_k \rbr{ f(\yk) + \abr{\grad(\yk), x - \yk} + \frac{\mu}{2} \norm{x - \yk}^2 } \).
\end{itemize}

See~\citet[Lemma 2.2.2]{nesterov2004lectures} for proof that these are indeed estimating sequences. Moreover, the sequences given above satisfy the following key Lemma~\cite[Lemma~2.2.3]{nesterov2004lectures}.\\

\begin{lemma}~\label{lemma:cannonical_form}
    The estimating sequences \( \cbr{\lambda_k}_{k=0}^\infty \), \( \cbr{\phi_k}_{k=0}^\infty \) defined above preserve the canonical form
    \[ \phi_{k}(x) = \phi_k^* + \frac{\gamma_k}{2}\norm{x - v_k}^2, \]
    where \( \phi_k^*, \gamma_k, \) and \( v_k \) evolve over time as
    \begin{itemize}
        \item \( \gamma_{k+1} = (1-\alpha_k) \gamma_k + \alpha_k \mu \);
        \item \( v_{k+1} = \frac{1}{\gamma_{k+1}} \rbr{ (1-\alpha_k) \gamma_k v_k + \alpha_k \mu \yk - \alpha_k \grad(\yk) + } \);
        \item \( \phi_{k+1}^* = (1- \alpha_k) \phi_k^* + \alpha_k f(\yk) - \frac{\alpha_k^2}{2\gamma_{k+1}}\norm{\grad(\yk)}^2 + \frac{\alpha_k(1-\alpha_k)\gamma_k}{\gamma_{k+1}}\rbr{\frac{\mu}{2} \norm{\yk - v_k}^2 + \abr{\grad(\yk), v_k - \yk}} \).
    \end{itemize}
\end{lemma}

Our goal is now to pick sequences \( \cbr{\wk}_{k=0}^\infty \) and \( \cbr{\yk}_{k=0}^\infty \) so that \( \phi_k(x) \geq f(\wk) \) for all \( x \in \R^d \).

\section{Stochastic Acceleration}~\label{sec:stochastic-acceleration}

Let's further assume that \( f \) satisfies the strong growth condition (SGC)~\citep{solodov1998incremental, tseng1998incremental, schmidt2013fast, vaswani2019fast} with parameter \( \rho \),
\[ \E \norm{\grad(x, \z)}^2 \leq \rho \norm{\grad(x)}^2, \]
where \( \grad(x,\z) \) denotes a stochastic gradient.
We will use this to establish a version of Lemma~\ref{lemma:characterization_of_estimating_sequences} that holds when \( \wk \) is obtained by stochastic gradient descent.
However, we first establish that stochastic gradient descent satisfies a decrease condition similar to the Descent Lemma when the SGC is satisfied.\\

Let us now prove that \( \E[\inf_x \phi_k(x)] \geq \E[f(\wk)] \) when using a stochastic gradient update.
Throughout what follows, we use \( \E[\cdot] \) to denote the expectation taken with respect to all past noise vectors \( \zk, \dots, \z_0 \).\\

\begin{restatable}{lemma}{stochasticEstimatingSeq}~\label{lemma:stochastic-estimating-seq}
    Let \( \phi^*_0 = f(x_0) \) and assume we generate the sequences \( \cbr{\alpha_k}_{k=0}^\infty \), \( \cbr{\wk}_{k=0}^\infty \), \( \cbr{\yk}_{k=0}^\infty \) as follows:
    \begin{align*}
        \alpha^2_k &= \frac{\gamma_{k+1}}{\rho L } = \frac{(1-\alpha_k)\gamma_k + \alpha_k \mu}{\rho L}, \\
        \yk &= \wk - \frac{\alpha_k}{\gamma_k + \alpha_k \mu} \nabla \phi_k(\wk),\\
        \wkk &= \yk - \frac{1}{\rho L} \grad\rbr{\yk, \zk}.
    \end{align*}
    If \( f \) satisfies \( \rho \)-SGC and the initial value \( \gamma_0 \) is independent of \( \cbr{\zk}_{k=0}^\infty \), then for all \( \wk \) the following holds:
    \[ \E [\inf \phi_k(x)] \geq \E [f(\wk)]. \]
\end{restatable}


We are now ready to prove an accelerated convergence rate for the \( \cbr{\wk}_{k=0}^\infty \) sequence.\\

\begin{restatable}{theorem}{scAcceleration}~\label{thm:sc-acceleration}
    Let \( f \) be a \( \mu \)-strongly-convex, \( L \)-smooth function satisfying the strong growth condition with parameter \( rho \).
    Let \( \phi^*_0 = f(x_0) \) and assume we generate the sequences \( \cbr{\alpha_k}_{k=0}^\infty \), \( \cbr{\wk}_{k=0}^\infty \), \( \cbr{\yk}_{k=0}^\infty \) as in Lemma~\ref{stochastic-estimating-seq}.
    Moreover, choose \( \gamma_0 = \mu > 0 \) and \( v_0 = \w_0 \).
    Then Nesterov's accelerated gradient method converges as  
    \[ \E\sbr{f(\wk)} - f(\wopt) \leq \rbr{1 - \sqrt{\frac{\mu}{\rho L}}}^{k} \rbr{ f(w_0) - f(\wopt) + \frac{\mu}{2} \norm{\wopt - \w_0}^2 }.  \]
\end{restatable}

\begin{restatable}{theorem}{cAcceleration}~\label{thm:convex-acceleration}
    Let \( f \) be a convex, \( L \)-smooth function satisfying the strong growth condition with parameter \( \rho \).
    Then  
\end{restatable}

\textbf{Comments}:
\begin{itemize}
    \item The strong-growth parameter is \( \rho = \frac{\Lmax}{\mu} \) when \( f \) is \( \mu \)-strongly convex, satisfies interpolation, and is ``individually smooth'', meaning \( \grad(\w, \z) \) is \( L_\z \) Lipschitz continuous and \( \Lmax = \max_\z L_\z \).
    This implies the final convergence rate is
    \[ \E\sbr{f(\wk)} - f(\wopt) \leq \rbr{1 - \frac{\mu}{\sqrt{\Lmax L}}}^{k} \rbr{ f(w_0) - f(\wopt) + \frac{\mu}{2} \norm{\wopt - \w_0}^2 }, \]
    which is nearly the rate for deterministic gradient descent (\( \sqrt{\Lmax L} \) vs \( L \)).
    \item In comparison to the above, the accelerated rate given by~\citet{vaswani2019fast} in the same setting is
    \[ \E\sbr{f(\wk)} - f(\wopt) \leq \rbr{1 - \frac{\mu}{\Lmax} \sqrt{\frac{\mu}{L}}}^{k} \rbr{ f(w_0) - f(\wopt) + \frac{\mu}{2} \norm{\wopt - \w_0}^2 }. \]
    This is worse by a factor of \( \sqrt{\frac{\Lmax}{L}} \kappa^{-1/2} \), which is approximately the difference Nesterov acceleration and gradient descent in the deterministic setting.
    \item The rate given in Theorem~\ref{thm:accelerated_convergence_rate} is ``accelerated'' with respect to the convergence of stochastic gradient descent in the same setting.
    It is \emph{not} accelerated in the sense that it obtains the optimal convergence rate for first-order methods on strongly-convex functions.
\end{itemize}



