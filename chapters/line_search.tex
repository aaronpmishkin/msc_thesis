%! TEX root = ../main.tex

\chapter{Line Search}~\label{ch:line-search}

A major weakness of constant step-size \ac{SGD} is that the problem constants must be known a priori in order to select a step-size for which convergence is guaranteed.
Consider \autoref{thm:sgc-convex} as an example. 
A fast linear rate of convergence is guaranteed only when \( \eta~\leq~\frac{2}{\sgc\rbr{\mu + L}} \), but optimization speed is sub-optimal for \( \eta \ll \frac{2}{\sgc\rbr{\mu + L}} \).
Meeting this requirement demands knowledge of the strong-convexity and smoothness constants, \( \mu \) and \( L \), as well as the strong growth parameter \( \sgc \).
In practice, we are unlikely to have knowledge of these coefficients or to possess straightforward means for estimating them.
This leaves a dilemma: over-estimate the optimal step-size and risk divergence, or use a small step-size that yields slow convergence. 

This chapter tackles the problem of step-size selection by augmenting \ac{SGD} with a stochastic Armijo line-search algorithm that \emph{automatically} finds a suitable step-size at each iteration.
Unlike meta-optimization schemes such as grid search, random search, or Bayesian optimization, line-search techniques do not require multiple, costly executions of the optimization procedure~\citep{snoek2012bayesian}.
Accordingly, practical line-search implementations are typically very fast in comparison to meta-optimization approaches~\citep{nocedal1999numerical}.
Moreover, as we will show in the following sections, the convergence speed of \ac{SGD} with a stochastic line-search is tight with that of \ac{SGD} with the best fixed step-size when minimizer interpolation is satisfied. 

The work in this chapter was originally conducted by the author as part of \citet{vaswani2019painless}.
The proposal to augment \ac{SGD} with the stochastic Armijo line-search, the algorithmic procedure in \autoref{procedure:armijo-ls}, \autoref{lemma:step-size-bound}, and \autoref{thm:non-convex-line-search} are reproduced from \citet{vaswani2019fast}. 
In contrast, Theorems~\ref{thm:sc-line-search} and~\ref{thm:convex-line-search} are new, unpublished improvements over the originally published theorems. 

\section{Background}~\label{sec:line-search-background}

Line-search algorithms are a classic solution to the problem of step-size selection in the deterministic setting.
The basic notion is to search along the gradient direction (or, more generally, a descent direction) for a step-size that satisfies a desirable property.
A common rule is the Armijo condition, 
 \begin{align}
     f(\wk - \etak v_k) &\leq f(\wk) + c \cdot \etak \abr{\grad(\wk), v_k},~\label{eq:deterministic-armijo-ls}
\end{align}
which demands sufficient decrease in the function value when taking a step along search direction \( v_k \).
The vector \( v_k \) is required to satisfy \( \abr{\grad(\wk), v_k} < 0 \), in which case it is called a \emph{descent direction}.
An obvious (and locally steepest) descent direction is the negative gradient \( v_k = - \grad(\wk) \). 
In comparison, the negative stochastic gradient \( - \grad(\wk, \zk) \) direction used by \ac{SGD} is only guaranteed to be a descent direction in expectation.


A simple mechanism for enforcing the Armijo condition is iteratively decreasing the step-size, or \emph{backtracking}, from a maximal step-size \( \etamax \) until the condition is satisfied.
Lipschitz-smoothness of \( f \) guarantees that for all \( \w \) there exists sufficiently small \( \etak \) such that \autoref{eq:deterministic-armijo-ls} holds~\citep{nocedal1999numerical}; we will establish a specific case of this for \ac{SGD} in the stochastic setting (\autoref{lemma:step-size-bound}). 
Practical implementations of backtracking reduce the step-size as \( \etak~\gets~\beta \cdot \etak \) when the Armijo condition does not hold, where \( \beta \in (0,1) \) is a tunable parameter. 
Alternatively, cubic or other interpolation schemes can be used to model and approximately minimize the one-dimensional function \( g(\etak) = f(\wk - \etak v_k) \); see \citet{nocedal1999numerical} for more details.
In this work we assume that backtracking can be accomplished exactly, meaning that \( \etak \) is the largest step-size less than \( \etamax \) for which \autoref{eq:deterministic-armijo-ls} is satisfied.

\begin{figure}[t]
    \centering
    \input{figures/line_search.tex}
    \caption[Progress made by stochastic gradient descent when using the stochastic Armijo line-search with and without minimizer interpolation.]%
       {Progress when using the stochastic Armijo line-search with and without minimizer interpolation.
        The function \( f_{v_k} \) is the restriction of \( f \) to the descent direction \( v_k = -\grad(\wk) \), while \( \ell_{v_k} \) is the Armijo condition as a function of the step-size, \( \eta \).
        The dashed red line shows the restriction of the stochastic function \( f(\w, \z) \) to \( v_k \).
        The next iterate \( \wkk \) is obtained by choosing the largest step-size satisfying the line-search. 
        When interpolation is not satisfied, we ``over-shoot'' significantly and \( f(\wkk) > f(\wk) \) as a result.
        In contrast, minimizer interpolation ensures that the iteration makes progress towards \( \wopt \). 
        Note that \( f(\wk,\z) = f(\wk) \) for convenience only; this need not hold in general. 
    }%
    \label{fig:interpolation-ls}
\end{figure}

The Armijo line-search as given above is not suitable for stochastic optimization problems.
Remember that the optimizer does not have access to exact function and gradient evaluations --- it can only access noisy estimates \( f(\w, \zk) \) and \( \grad(\w, \zk) \).
Even when exact evaluations are available, as in the finite-sum setting, the cost of computing function and gradient values is much larger than the cost of querying \oracle{}. 
An alternative to the full Armijo line-search is the following stochastic version of the Armijo condition:
\begin{align}
    f(\wk - \etak v_k, \zk) &\leq f(\wk, \zk) + c \cdot \etak \abr{\grad(\wk, \zk), v_k}. \label{eq:armijo-ls}
\end{align}
This condition is identical to \autoref{eq:deterministic-armijo-ls}, but uses stochastic function and gradient evaluations queried from the \ac{SFO}.
Intuitively, the stochastic Armijo condition requires sufficient decrease on \( f(\cdot, \zk) \), rather than \( f \).
Note that practical backtracking on \autoref{eq:armijo-ls} will require \( f(\cdot, \zk) \) to be evaluated multiple times in the same iteration.
This is straightforward in the finite-sum setting, but may be impossible when \( \zk \) reflects an inherently noisy measurement process. 

The main issue with stochastic line-search conditions is that the oracle queries may not be representative of the true function. 
That is, progress as measured by \( f(\cdot, \zk) \) may be uninformative or even lead to ascent in \( f \). 
\autoref{fig:interpolation-ls} illustrates such a situation, as well as why minimizer interpolation may preclude this problem. 
Intuitively, progress on the stochastic functions \( f(\cdot, \zk) \) should be sufficient for progress on \( f \) when \( \calX^* \subseteq \bigcap_{\z \in \calZ} \calX^*_{\z} \) --- i.e.\ when \( \rbr{f, \oracle{}} \) satisfies minimizer interpolation 
The remainder of this chapter formalizes this intuition and develops non-asymptotic convergence rates for \ac{SGD} with the stochastic Armijo line-search. 
In particular, we establish the following results under minimizer interpolation:
\begin{enumerate}
    \item Linear convergence for strongly-convex \( f \) and \( \Lmax \) individually-smooth and convex \oracle{}; moreover, this rate is tight with fixed step-size \ac{SGD}. 
    \item Sub-linear convergence for convex \( f \) and \( \Lmax \) individually-smooth and convex \oracle{}; again, this rate is tight with the fixed step-size case.
    \item Sub-linear convergence to a stationary point for non-convex \( f \) when \( \rbr{f, \oracle{}} \) satisfies strong growth. Note that this result is reproduced from \citet{vaswani2019painless}.
\end{enumerate}

\begin{figure}[t]
    \begin{procedure}{\ac{SGD} with Armijo Line-Search}
    \item Choose an initial point \( \w_0 \in \R^d \).
    \item For each iteration \( k \geq 0 \):
        \begin{enumerate}
            \item Query \oracle{} for \( f(\wk, \zk), \: \grad(\wk, \zk) \). 
            \item Set \( \etak =  \etamax \) and backtrack until 
                \[  \hspace{-1em} f(\wk - \etak \grad(\wk, \zk), \zk) \leq f(\wk, \zk) - c \cdot \etak \norm{\grad(\wk, \zk)}^2. \]
            \item Update input as\vspace{-1ex}%
                \[ \wkk = \wk - \etak \grad(\wk, \zk). \]
        \end{enumerate}
    \end{procedure}
    \caption[Procedural definition of stochastic gradient descent with the stochastic Armijo line-search.]%
       {Stochastic gradient descent with the stochastic Armijo line-search, as proposed by \citet{vaswani2019painless}.
        We assume that the backtracking procedure can be evaluated exactly, meaning the maximal step-size \( \etak \) satisfying the Armijo is selected.
        Note that \oracle{} must be queried for additional function values \( f(\wkk, \zk) \) during backtracking.}%
    \label{procedure:armijo-ls}
\end{figure}

Now, let us introduce a formal definition of \ac{SGD} with the Armijo line-search before proceeding with our analysis.
\autoref{procedure:armijo-ls} presents the basic procedure of the algorithm.
There are several key differences from fixed step-size \ac{SGD}, which are noted as follows:
\begin{inparaenum}[(i)]
    \item the step-size \( \etak \) is defined per-iteration and initialized at \( \etamax \),
    \item \( \etak \) is then chosen by exact backtracking on \autoref{eq:armijo-ls} evaluated at \( v_k = \grad(\wk, \zk) \), and
    \item the backtracking procedure implicitly requires additional queries to \oracle{} for \( f(\wk - \etak \grad(\wk, \zk), \zk) \).
\end{inparaenum}
While \autoref{procedure:armijo-ls} uses an idealized backtracking procedure, in practice the hyper-parameter \( \beta \) may need to be tuned to avoid excessive oracle queries. 

\begin{figure}[t]
    \centering
    \input{figures/step_size_bound.tex}
    \caption[Step-size intervals accepted and rejected by the Armijo line-search for a Lipschitz-smooth function.]%
       {Graphical depiction of \autoref{lemma:step-size-bound}.
        The function \( f_{v_k} \) is the restriction of \( f \) to the descent direction \( v_k = -\grad(\wk) \), while \( \ell_{v_k} \) is the Armijo condition as a function of the step-size, \( \eta \).
        The dashed red line shows the upper-bound on \( f_{v_k} \) due to Lipschitz-smoothness.
        We can see that a large range of step-sizes make sufficient progress (yellow region) and will be accepted by the Armijo condition; step-sizes for which the graph of \( \ell_{v_k} \) lies below \( f_{\vk} \) (red region) do not make enough progress and will be rejected. 
        Smoothness guarantees that \( \ell_{v_k} \) is above \( f_{v_k} \) for a non-empty interval of step-sizes (green region), which will always be accepted.%
    }%
    \label{fig:step-size-bound}
\end{figure}

\section{Convergence for Strongly-Convex Functions}~\label{sec:ls-sc}

The first step of our analysis is to control the step-size \( \etak \) using smoothness and the line-search condition.
Lower-bounding the step-size relies on smoothness of \( f(\cdot, \zk) \), which is why the results in this chapter require \oracle{} to be individually-smooth. 
This fact also motivates our transition from assuming the strong/weak growth conditions to directly reasoning with individually-smooth oracles where \( \rbr{f, \oracle{}} \) satisfies minimizer interpolation.
The following lemma is reproduced from \citet{vaswani2019painless}; the proof in \autoref{app:line-search} is modified from the original for greater clarity. 
\begin{restatable}{lemma}{stepSizeBound}~\label{lemma:step-size-bound}
    Let \( f \) be an \( L \)-smooth function and \oracle{} an \( \Lmax \) individually-smooth \ac{SFO} such that \( \rbr{f, \oracle{}} \) satisfies minimizer interpolation. 
    Then the maximum possible step-size returned by the stochastic Armijo line-search constrained to lie in the \( (0, \etamax \, ] \) range satisfies the following inequalities:
    \[ \min\cbr{\frac{2 (1-c)}{\Lmax}, \etamax} \leq \etak \leq \frac{f(\wk, \zk) - f(\wopt, \zk)}{c \norm{\grad(\wk, \zk)}^2}. \]
\end{restatable}

\autoref{lemma:step-size-bound} is the main tool needed to analyze \ac{SGD} with the Armijo line-search in the convex setting.
Specifically, it allows us to establish the following progress condition, which will be at the core of our proofs for convex and strongly convex functions. 
\begin{restatable}{lemma}{lsIntermediate}~\label{lemma:ls-intermediate}
    Let \( f \) be a convex, \( L \)-smooth function and \oracle{} an \( \Lmax \) individually-smooth and convex \ac{SFO} such that \( \rbr{f, \oracle{}} \) satisfy minimizer interpolation.
    Then stochastic gradient descent using the Armijo line-search with \( c \geq \half \) satisfies the following
    inequality:
    \[ f(\wk)- f(\wopt) \leq \half \max\cbr{\frac{\Lmax}{2(1-c)},\inv{\etamax}} \rbr{\norm{\wk - \wopt}^2 - \E_{\zk}\sbr{\norm{\wkk - \wopt}^2}}, \]
    where \( \wopt \in \calX^* \) is arbitrary.
\end{restatable}
\noindent See \autoref{app:line-search} for proof. \hfill \break

\autoref{lemma:ls-intermediate} is the ideal inequality for 
\begin{inparaenum}[(a)]
    \item applying \( \mu \)-strong-convexity to obtain a recursion for \( \norm{\wk - \wopt}^2 \) and  a linear convergence rate, or
    \item summing over iterations to obtain sub-linear convergence of the average iterate \( \bar w_K \).
\end{inparaenum}
Indeed, these are exactly the strategies used to prove Theorems~\ref{thm:sc-line-search} and~\ref{thm:convex-line-search}.
We start with the strongly-convex case, with for the following theorem proof given in \autoref{app:sc-line-search}.

\begin{restatable}{theorem}{scLineSearch}~\label{thm:sc-line-search}
    Let \( f \) be a \( \mu \)-strongly-convex, \( L \)-smooth function and \( \oracle{} \) an \( \Lmax \) individually-smooth and convex \ac{SFO} such that \( \rbr{f, \oracle{}} \) satisfies minimizer interpolation.
    Then stochastic gradient descent using the Armijo line-search with \( c \geq \half \) converges as
    \begin{align*}
        \E\sbr{\norm{\w_{K} - \wopt}^2} &\leq \rbr{1 - \mu \min\cbr{\frac{2(1-c)}{\Lmax}, \etamax}}^K \norm{\w_0 - \wopt}^2. 
    \end{align*}
\end{restatable}%

Setting \( c = \half \) and \( \etamax = \infty \) in \autoref{thm:sc-line-search} gives  
\[ \E\sbr{\norm{\w_{K} - \wopt}^2} \leq \rbr{1 - \frac{\mu}{\Lmax}}^K \norm{\w_0 - \wopt}^2, \]
which is exactly the rate obtained for fixed step-size \ac{SGD} in the same setting (cf. \autoref{thm:sgc-ind-convex}). 
The convergence speed is also the same as the worst-case under the weak growth condition when interpolation and individual smoothness hold~\citep[Theorem 5]{vaswani2019fast}.
While this latter result does not use individual convexity, we (intuitively) need this additional assumption when using a line-search in order for the step-size to yield reliable progress towards the global minimizer.
Indeed, general non-convex \( f(\cdot, \zk) \) can satisfy minimizer interpolation and still have stochastic gradients \( \grad(\cdot, \zk) \) which point away from the minimizer \( \wopt \).
This problem is presented formally in \autoref{sec:ls-nc-challenges}, which discusses challenges in establishing convergence for non-convex functions.

The \( c \geq \half \) requirement in \autoref{thm:sc-line-search} is notable because it is the opposite of the constraint on \( c \) required for Newton or quasi-Newton methods (cf. \citet[Theorem 3.6]{nocedal1999numerical}). 
The key issue here is that the Armijo line-search with \( c > \half \) can exclude unit step-lengths (particularly for quadratic functions), which must be accepted in some neighborhood of \( \wopt \) for Newton and quasi-Newton methods to obtain local super-linear convergence. 
Unit step-sizes are not required for \ac{SGD}, where the stochastic Armijo line-search is mainly used to obtain sufficient progress at each iteration.
Moreover, if \oracle{} is individually-smooth and \( c \in \rbr{0,1} \), then \autoref{lemma:step-size-bound} guarantees \ac{SGD} will satisfy \autoref{eq:armijo-ls} for some \( \etak > 0 \).
As such, larger \( c \) values --- which demand greater decrease at each iteration --- arise naturally in our analysis.

\autoref{table:ls-comparison} contrasts \autoref{thm:sc-line-search} with the other known result for \ac{SGD} with Armijo line-search on strongly-convex functions \citep[Theorem 1]{vaswani2019painless}. 
In particular, note that the convergence rate presented here depends on the strong-convexity constant of the overall function, \( \mu \), instead of the expected constant \( \bar \mu = \Ek \sbr{\mu_{\zk}} \).
This is a nice theoretical improvement for the following reason: our analysis permits all stochastic functions to be convex only, while that of \citet{vaswani2019painless} requires \( f(\cdot, \z) \) to be strongly-convex for at least one \( \z \in \calZ \).
Such a condition is unlikely to hold in practice and can lead to degenerate problems (see the discussion in \autoref{sec:sgd-sgc-ind-convex}).
\autoref{thm:sc-line-search} is motivated by the work of \citet{loizou2020sps}, who previously obtained a similar improvement in the context of the stochastic Polyak step-size.

\section{Convergence for Convex Functions}~\label{sec:ls-convex}

Now we derive a sub-linear convergence rate for \ac{SGD} with the stochastic Armijo line-search when \( f \) is convex, \oracle{} is individually-smooth, and minimizer interpolation is satisfied.
Convergence is established for the averaged iterate \( \bar \w_K \) similarly to the case of constant step-size \ac{SGD};\@
non-asymptotic, final-iterate rates are also an open problem when \( \etak \) is chosen by line-search.
The proof of the following theorem follows almost immediately from \autoref{lemma:ls-intermediate} as briefly described in the previous section; it can be found in \autoref{app:convex-line-search}.

\begin{restatable}{theorem}{convexLineSearch}~\label{thm:convex-line-search}
    Let \( f \) be a convex, \( L \)-smooth function and \( \oracle{} \) an \( \Lmax \) individually-smooth and convex \ac{SFO} such that \( \rbr{f, \oracle{}} \) satisfies minimizer interpolation.
    Then stochastic gradient descent using the Armijo line-search with \( c \geq \half \) converges as 
    \begin{align*}
        \E\sbr{f(\bar \w_K)} - f(\wopt) &\leq \frac{1}{2 \, K} \max\cbr{\frac{\Lmax}{2(1-c)}, \frac{1}{\etamax}} \norm{\w_0 - \wopt}^2, 
    \end{align*} 
    where \( \bar \w_K = \sum_{k=0}^{K-1} \wk \) and \( \wopt = \Pi_{\calX^*}(\w_0) \).
\end{restatable}
This result is tight in the following sense: when \( c = \half \) and \( \etamax = \infty \), \autoref{thm:convex-line-search} yields 
\[  \E\sbr{f(\bar \w_K)} - f(\wopt) \leq \frac{\Lmax}{2 \, K} \norm{\w_0 - \wopt}^2. \]
This rate is identical to that for constant step-size \ac{SGD} with \( \eta = \frac{1}{\Lmax} \) if we assume the worst-case value for the weak-growth parameter \( \wgc \) in \autoref{thm:wgc-convex}.
If \( f(\cdot, \z) = f \) for each \( z \in \calZ \), then \( \Lmax = L \) and this rate is comparable to the best known convergence results for gradient descent on convex functions~\citep{bubeck2015convex}. 

\autoref{table:ls-comparison} compares \autoref{thm:convex-line-search} with the original result from  \citet[Theorem 2]{vaswani2019painless}.
The sub-linear rate established here is faster by constant factors and has a simpler proof.
Furthermore, our result holds for all \( c \geq \half \), while \( c > \half \) must hold strictly for their theorem. 
The unnecessary strictness of the original result prevents the natural case where \( c = \half \),  \( v_k = \grad(\wk, \zk) \) and \autoref{eq:armijo-ls} becomes equivalent to the quadratic upper-bound implied by Lipschitz-smoothness.\\

\section{Convergence for Non-Convex Functions}~\label{sec:ls-nc}

\begin{table}[t]
    \centering
    \begin{tabular}{c l l c c }\toprule
        \multirow{2}{*}{Assumptions} & \multicolumn{2}{c}{Minimum \( c \)} & \multicolumn{2}{c}{Convergence Rate}\\%
        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
                 & \multicolumn{1}{c}{Ours} & \multicolumn{1}{c}{VML+}%
                 & \multicolumn{1}{c}{Ours} & \multicolumn{1}{c}{VML+}\\ \midrule
    \( \mu \)-SC & \( c \geq \half \)%
                 & \( c \geq \half \)%
                 & \( O\rbr{\frac{\Lmax}{\mu} \log\rbr{\frac{1}{\epsilon}}} \)% 
                 & \( O\rbr{\frac{\Lmax}{\bar \mu} \log\rbr{\frac{1}{\epsilon}}} \) \\ \addlinespace
    Convex       & \( c \geq \half \)%
                 & \( c > \half \)%
                 & \( O\rbr{\frac{\Lmax}{2 \, \epsilon}} \)%
                 & \( O\rbr{\frac{3 \Lmax}{\epsilon}} \)\\ \addlinespace \bottomrule 
        \end{tabular}
        \caption[Comparison of iteration complexities for stochastic gradient descent with the stochastic Armijo line-search.]%
        {Comparison of iteration complexities for \ac{SGD} with the stochastic Armijo line-search.
         We omit results for non-convex functions, which are identical between the two works.
         Results are shown for \( \etamax = \infty \) and \( c = \frac{1}{2} \), excepting the convex case of VML+ \citep{vaswani2019painless} where \( c = \frac{2}{3} \) is used as suggested by the authors.
         The constant \( \bar \mu = \min_k \cbr{\E_{\zk} \sbr{\mu_{\zk}} } \) requires that at least one stochastic function in the support of \( \zk \) is strongly convex for each iteration \( k \);
         in contrast, our strongly-convex proof relies only on \( \mu \), the parameter of the true function.  }%
    \label{table:ls-comparison}
\end{table}

The complexity of \ac{SGD} with the stochastic Armijo line-search for general non-convex functions is particularly challenging to determine.
As discussed below, proofs which analyze the sequence of distances to a minimizer \( \seq{\norm{\wk - \wopt}^2} \) fail without convexity.
This includes the technique used to prove Theorems~\ref{thm:sc-line-search} and~\ref{thm:convex-line-search}.
Instead, convergence to a stationary point is typically established through the quadratic majorant provided by \( L \)-smoothness of \( f \).
The following theorem shows that such convergence does indeed hold, but mandates a severe upper-bound on the line-search.
A major weakness of our result is that the setting of \( \etamax \) requires explicit knowledge of \( \sgc \) and \( \Lmax \). 

\begin{restatable}{theorem}{nonConvexLineSearch}~\label{thm:non-convex-line-search}
    Let \( f \) be an \( L \)-smooth function and \( \oracle{} \) an \( \Lmax \) individually-smooth \ac{SFO} such that \( \rbr{f, \oracle{}} \) satisfies the strong growth condition with parameter \( \sgc \).
    Then stochastic gradient descent using the Armijo line-search with \( c >  1 - \frac{\Lmax}{\sgc L}\) and \( \etamax <  \frac{2}{\sgc L} \) converges as
    \[ \min_{k \in [K]} \norm{\grad(\wk)}^2 \leq \frac{1}{\delta \, K} \rbr{f(w_0) - f(\wopt)}, \]
    where \( \delta = \rbr{\etamax + \frac{2(1-c)}{\Lmax}} - \sgc \rbr{\etamax - \frac{2(1-c)}{\Lmax} + L \etamax^2} \).
\end{restatable}
\noindent See \autoref{app:non-convex-line-search} for proof. \hfill \break 

The step-size constraint \( \etak \leq \etamax < \frac{2}{\sgc L} \) forces the Armijo line-search to behave like constant step-size \ac{SGD}.
Indeed, \autoref{lemma:sgc-decrease-condition} shows that \( \eta < \frac{2}{\sgc L} \) is exactly the condition required for constant step-size \ac{SGD} to make guaranteed progress for an \( L \)-smooth function such that \( \rbr{f, \oracle{}} \) satisfies strong growth.
It is not clear if the upper-bound on \( \etamax \) is a fundamental property of \ac{SGD} for non-convex functions, or an artifact of the proof for \autoref{thm:non-convex-line-search}. 
In either case, it is worthwhile to see how the requirement on \( \etamax \) emerges.

\subsection{Challenges in the Analysis}\label{sec:ls-nc-challenges}

A main object in our proofs so far has been the inner-product 
\begin{align*}
    \etak \abr{\grad(\wk, \zk), \wk - \wopt}&.
    \intertext{When \( \etak \) is independent of \( \zk \), linearity of expectation gives the following:}
    \Ek\sbr{\etak \abr{\grad(\wk, \zk), \wk - \wopt}} &= \etak \abr{\grad(\wk), \wk - \wopt}.
    \intertext{It is now straightforward to use convexity of \( f \) to control this term, which is how the proofs for \autoref{ch:sgd} proceed. However, the stochastic Armijo line-search yields step-sizes which are corollated with \( \grad(\wk, \zk) \) and thus are not independent of \( \zk \). A straighforward solution is to use individual convexity and minimizer interpolation to obtain }
    \etak \abr{\grad(\wk, \zk), \wk - \wopt} &\geq \etak \rbr{f(\wk, \zk) - f(\wopt, \zk)} \geq 0.
\end{align*}
\autoref{lemma:step-size-bound} then provides the necessary tool to lower-bound \( \etak \), which is how the proofs for this chapter proceed.
The major challenge of non-convex functions is now apparent: the inner-product is not guaranteed to be non-negative\footnote{Non-negativity of \( \abr{\grad(\w), \w - \wopt} \) is sometimes called monotonicity of the gradient and is equivalent to convexity if it holds for all \( \w \)~\citep{bubeck2015convex}.} and the proof cannot proceed by bounding \( \etak \).
Intuitively, disentangling \( \etak \) and \( \grad(\wk, \zk) \) becomes the key to establishing convergence in the non-convex setting. 

The successful convergence proof for non-convex functions starts from \( L \)-smoothness of \( f \), rather than ``going through iterates'' as do the proofs for Theorems~\ref{thm:sc-line-search} and~\ref{thm:convex-line-search}.
Again, a correlated inner-product \( \etak \abr{\grad(\wk), \grad(\wk, \zk)} \) is encountered; the main innovation is to expand this as
\begin{align*}
    -2\etak \abr{\grad(\wk), \grad(\wk, \zk)} &= \etak \rbr{\norm{\grad(\wk, \zk) - \grad(\wk)}^2 - \norm{\grad(\wk)}^2 - \norm{\grad(\wk, \zk)}^2}\\
                                              &\leq \etamax \norm{\grad(\wk, \zk) - \grad(\wk)}^2 - \etamin \big(\norm{\grad(\wk)}^2 \\ 
                                              & \hspace{18em} + \norm{\grad(\wk, \zk)}^2\big),
\end{align*}
where the inequality stems from separately bounding \( \etak \) on each positive and negative terms.
This bound is worst-case, but separates the step-size and stochastic gradient, allowing the proof to proceed.
As a side-effect, we require a tight upper-bound on the maximum step-size: \( \etamax < \frac{2}{\sgc L} \).

\section{Conclusions}~\label{sec:line-search-conclusions}

The main focus of this chapter was augmenting \ac{SGD} with an automatic mechanism for selecting \( \eta \), the step-size parameter. 
In particular, we sought to develop a tuning-free approach that obtains comparable convergence speed to \ac{SGD} with a fixed step-size (as established in \autoref{ch:sgd}) \emph{without} restarts or a meta-optimization procedure. 
This was achieved by combining \ac{SGD} with the stochastic Armijo line-search, as proposed by \citet{vaswani2019painless}. 
We then developed new convergence results for strongly-convex (\autoref{thm:sc-line-search}) and convex (\autoref{thm:convex-line-search}) functions that improve over those given by \citet{vaswani2019painless} by constant factors. 
These rates show that \ac{SGD} with the stochastic Armijo line-search obtains a converge rate for (strongly-) convex functions that is tight with fixed step-size \ac{SGD} despite requiring no knowledge of the problem constants. 
Finally, we considered general non-convex functions and showed \ac{SGD} with the stochastic Armijo line-search converges to a stationary point if an aggressive upper-bound on the step-size is enforced (\autoref{thm:non-convex-line-search}). 




