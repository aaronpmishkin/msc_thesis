%! TEX root = ../main.tex

\chapter{Line Search}~\label{ch:line-search}

The Armijo line-search is 
\begin{align}
    f(\wk - \etak \grad(\wk, \zk), \zk) &\leq f(\wk, \zk) - c \cdot \etak \norm{\grad(\wk, \zk)}^2. \label{eq:armijo-ls}
\end{align}

\begin{restatable}{lemma}{stepSizeBound}~\label{lemma:step-size-bound}
    Let \( f \) be an \( L \)-smooth function and suppose that the stochastic function \( f(\cdot, \z) \) is individually smooth and satisfies interpolation.
    Then the step-size returned by the Armijo line-search constrained to lie in the \( (0, \etamax] \) range satisfies the following inequalities:
    \[ \min\cbr{\frac{2 (1-c)}{\Lk}, \etamax} \leq \etak \leq \frac{f(\wk, \zk) - f(\wopt, \zk)}{c \norm{\grad(\wk, \zk)}^2}. \]
\end{restatable}

See \autoref{app:line-search} for proof.

The following theorem improves upon that in \citet{vaswani2019painless}.
In particular, the convergence rate depends on the strong-convexity constant of the overall function, \( \mu \), instead of the expected constant \( \E \sbr{\mu_\z} \).
As such, this theorem permits all stochastic functions to be convex only.

\begin{restatable}{theorem}{scLineSearch}~\label{thm:sc-line-search}
    Let \( f \) be a \( \mu \)-strongly-convex, \( L \)-smooth function.
    Moreover, suppose that the stochastic function \( f(\cdot, \z) \) is individually smooth. 
    Then stochastic gradient descent using the Armijo line-search with \( c \geq \half \) converges as
    \begin{align*}
        \E \sbr{f(\wkk)} - f(\wopt) &\leq \rbr{\frac{L}{\mu}}\rbr{1 - \mu \min\cbr{\frac{2(1-c)}{\Lmax}, \etamax}}^k \rbr{f(w_0) - f(\wopt)}. 
    \end{align*}
\end{restatable}

In particular, when \( c = \half \) and \( \etamax = \infty \) we obtain 
\[\E \sbr{f(\wkk)} - f(\wopt) \leq \rbr{\frac{L}{\mu}}\rbr{1 - \frac{\mu}{\Lmax}}^k \rbr{f(w_0) - f(\wopt)}. \]


\begin{restatable}{theorem}{convexLineSearch}~\label{thm:convex-line-search}
    Let \( f \) be a convex, \( L \)-smooth function.
    Moreover, suppose that the stochastic function \( f(\cdot, \z) \) is individually smooth. 
    Then stochastic gradient descent using the Armijo line-search with \( c \geq \half \) converges as 
    \begin{align*}
        \E\sbr{f(\bar \w_K)} - f(\wopt) &\leq \frac{1}{2 \, K} \max\cbr{\frac{\Lmax}{2(1-c)}, \frac{1}{\etamax}} \norm{\w_0 - \wopt}^2, 
        \intertext{where \( \bar \w_K = \sum_{k=0}^{K-1} \wk \).}
    \end{align*} 
\end{restatable}
In particular, when \( c = \half \) and \( \etamax = \infty \) we obtain
\[  \E\sbr{f(\bar \w_K)} - f(\wopt) \leq \frac{\Lmax}{2 \, K} \norm{\w_0 - \wopt}^2. \]

\begin{restatable}{theorem}{nonConvexLineSearch}~\label{thm:non-convex-line-search}
    Let \( f \) be a \( L \)-smooth function.
    Moreover, suppose that the stochastic function \( f(\cdot, \z) \) is individually smooth. 
    Then stochastic gradient descent using the Armijo line-search with \( c >  1 - \frac{\Lmax}{\rho L}\) and \( \etamax <  \frac{2}{\rho L} \) converges as
    \[ \min_{k \in [K]} \norm{\grad(\wk)}^2 \leq \frac{1}{\delta \, K} \rbr{f(w_0) - f(\wopt)}, \]
    where \( \delta = \rbr{\etamax + \etamin} - \rho \rbr{\etamax - \etamin + L \etamax^2} \).
\end{restatable}

