%! TEX root = ../main.tex

\chapter{Line Search}~\label{ch:line-search}

The Armijo line-search is 
\begin{align}
    f(\wk - \etak \grad(\wk, \zk), \zk) &\leq f(\wk, \zk) - c \cdot \etak \norm{\grad(\wk, \zk)}^2. \label{eq:armijo-ls}
\end{align}

\begin{restatable}{lemma}{stepSizeBound}~\label{lemma:step-size-bound}
    Let \( f \) be an \( L \)-smooth function and suppose that the stochastic function \( f(\cdot, \z) \) is individually smooth and satisfies interpolation.
    Then the step-size returned by the Armijo line-search constrained to lie in the \( (0, \etamax] \) range satisfies the following inequalities:
    \[ \min\cbr{\frac{2 (1-c)}{\Lk}, \etamax} \leq \etak \leq \frac{f(\wk, \zk) - f(\wopt, \zk)}{c \norm{\grad(\wk, \zk)}^2}. \]
\end{restatable}

See \autoref{app:line-search} for proof.

The following theorem improves upon that in \citet{vaswani2019painless}.
In particular, the convergence rate depends on the strong-convexity constant of the overall function, \( \mu \), instead of the expected constant \( \E \sbr{\mu_\z} \).
As such, this theorem permits all stochastic functions to be convex only.

\begin{restatable}{theorem}{scLineSearch}~\label{thm:sc-line-search}
    Let \( f \) be a \( \mu \)-strongly-convex, \( L \)-smooth function.
    Moreover, suppose that the stochastic functions \( f(\cdot, \z) \) are \( L_\z \) smooth, with \( \Lmax = \max_\z L_\z \). 
    Then stochastic gradient descent using the Armijo line-search with \( c \geq \half \) converges as 
    \[ \E\sbr{f(\wkk)} - f(\wopt) \leq \rbr{\frac{L}{\mu}}\rbr{1 - \frac{\mu}{\Lmax}}^k\rbr{f(\w_0) - f(\wopt)}. \]
\end{restatable}
