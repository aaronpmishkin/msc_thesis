% !TEX root = ../main.tex
\acresetall 
\chapter{Conclusion}~\label{ch:conclusion}

This thesis develops the convergence theory of stochastic first-order methods under interpolation conditions. 
Unlike existing work which is confined to the finite-sum setting, we propose a highly general model for interpolation that applies to black-box objective functions and \acp{SFO}. 
The subsequent discussion relates interpolation to the strong and weak growth conditions.
Specifically, we provide upper-bounds on the weak and strong growth parameters for interpolating oracles that satisfy an intuitive Lipschitz-smoothness property. 
Informally, this means that \emph{global} regularity of the stochastic gradients is guaranteed from a \emph{local} correspondence between the oracle and objective as long the stochastic gradient mapping is smooth in the model parameters. 
Such a property is satisfied for smooth supervised learning problems. 

The theoretical analysis of first-order methods focuses on \ac{SGD} and a stochastic version of Nesterov's \acl{AGD}. 
For \ac{SGD}, we analyze the iteration complexity with a fixed step-size as well as with a stochastic Armijo line-search.
In both cases, our results are tighter than existing convergence rates and allow for a wider range of parameters.
The improvement in convergence speed ranges from constant factors to exponential speed-up.\footnote{See \autoref{thm:sc-line-search}, where our rate is exponentially faster than that of \citet{vaswani2019painless} when \( \bar \mu = 0 \).} 
In all cases, specific care is taken to understand the impact of additional assumptions, such as smoothness or convexity of the oracle.

Our study of \ac{SAGD} uses the estimating sequences framework developed by \citet{nesterov2004lectures}.
Estimating sequences allow us to cast \ac{SAGD} as alternating descent procedure on the objective and a sequence of improving local approximations, avoiding the so-called ``analytical tricks'' used in many accelerated convergence proofs. 
Moreover, we are able to derive tighter convergence rates than existing work using this framework. 
For instance, our results improve upon \citet{vaswani2019fast} by a multiple of the condition number for smooth, interpolating oracles.
This is comparable to the improvement attained by \acl{AGD} over gradient descent. 

Although this thesis attempts to be a comprehensive analysis of stochastic gradient methods under interpolation, the scope is often too narrow and several important methods are not studied.
We briefly discuss the role of growth conditions in structural optimization in \autoref{ch:beyond-interpolation}.
The emphasis falls on \( L_2 \)-regularized problems where the regularized function satisfies weak growth.
In this limited setting, we derive linear convergence to a neighbourhood of the optimal solution and precisely characterize the volume in terms of the regularization parameter. 
The far more general and interesting problem of composite smooth/non-smooth optimization where the smooth component satisfies interpolation remains completely unaddressed.
Deriving exact convergence neighborhoods for proximal-gradient methods in this setting is an interesting open problem.

Open problems in optimization under interpolation are not limited to the largely unexplored area of structural optimization. 
Fundamental questions \emph{not} answered in this thesis or in the literature include the following: 
\begin{enumerate}
    \item Is there a finite-time convergence rate for the final iterate generated by constant step-size SGD when the objective is convex and weak growth is satisfied? 
    \item If so, can this rate be extended to \ac{SGD} with the stochastic Armijo line-search? 
    \item Does \ac{SAGD} obtain an accelerated convergence rate for convex functions under the weak growth condition?
    \item Can analyses for the stochastic Armijo line-searches be extended to \ac{SAGD}? 
    \item Can tighter convergence rates be established for \ac{SAGD} when additional assumptions are made on the oracle (e.g.\ smoothness or convexity) as they can for \ac{SGD}?
\end{enumerate}
Many of these questions build on the problems tackled in this thesis, but require new tools or insights to answer. 
The analyses contributed here are merely a first step towards a larger understanding of optimization under interpolation. 



