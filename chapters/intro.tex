%% The following is a directive for TeXShop to indicate the main file
%!TEX root = ../main.tex

\chapter{Introduction}\label{ch:Introduction}

Stochastic first-order methods are the most popular optimization algorithms in modern machine learning.
In particular, \ac{SGD}~\citep{robbins1951sgd} and its adaptive variants~\citep{duchi2011adagrad, tieleman2012rmsprop, zeiler2012adadelta, kingma2015adam} are widely used in large-scale supervised learning, where they are frequently referred to as fundamental ``workhorse'' algorithms~\citep{qian2019improvedrates, assran2019sgpush, grosse2015scaling}. 
The main advantage of these methods for \edit{machine learning} is that they only use the gradient of a single or small sub-sample of training examples to update the model parameters at each iteration.
The computational cost of \ac{SGD} (and variants) is thus independent of the training set size and scales to very large datasets and models.
This property is also why stochastic first-order methods are the dominant approach to training highly expressive models, such as deep neural networks~\citep{zhang2017understanding, bengio2012practical} and non-parametric kernels~\citep{liang2018just, belkin2019datainterp}.

Despite their successes, stochastic first-order methods suffer from two well known problems. 
The step-size, momentum term, and other algorithmic hyper-parameters must be carefully tuned to obtain good performance~\citep{bengio2012practical, schaul2013no, li2019convergence, choi2019empirical}; and
they converge slowly compared to deterministic~\citep{nesterov2004lectures} or variance-reduced algorithms~\citep{leroux2012sag, johnson2013svrg, defazio2014saga} even when well-tuned.
The issue of hyper-parameter tuning for \ac{SGD} is the focus of intense research, with approaches ranging from adaptive step-sizes inspired by online learning~\citep{luo2019adabound, li2019convergence, orabona2017coin} to meta-learning procedures~\citep{baydin2018hypergradient, schraudolph1999local, sutton1992gain, almeida1998parameter, plagianakos2001learning, shao2000rates} and heuristics for online estimation~\citep{schaul2013no, rolinek2018l4, tan2016bb}.
In contrast, the slow convergence of first-order methods in the general stochastic setting cannot be improved, with tight lower-bounds showing \( \Theta(\epsilon^{-4}) \) complexity for convergence to a stationary point~\citep{drori2019complexity, arjevani2019lower}.
This compares poorly to deterministic gradient methods, which are \( \Theta(\epsilon^{-2}) \) for the same problem class~\citep{carmon2019lower}.

Increasing experimental and theoretical evidence shows that the slow optimization speed of stochastic first-order methods is mitigated when training over-parameterized models~\citep{ma2018power, arora2018overparameterization, zhou2019analysis}.
For example, variance-reduced algorithms typically underperform \ac{SGD} in this setting despite using increased memory or computation~\citep{defazio2019effectiveness, ma2018power}. 
A key property of over-parameterized problems is that they satisfy \emph{interpolation}, meaning the model can exactly fit all of the available training data~\citep{belkin2019datainterp}.
\edit{While this may seem strong, interpolation has been observed in practice for popular methods such as boosting~\citep{schapire1997boosting}, kernel learning~\citep{belkin2019datainterp}, and over-parameterized neural networks~\citep{belkin2019reconciling, zhang2013gradient}}. 
\edit{
Under additional assumptions, interpolation is a sufficient condition for the strong~\citep{solodov1998incremental, tseng1998incremental, schmidt2013fast} or weak~\citep{vaswani2019fast, bassily2018exponential} growth conditions, which imply the second-moment of the stochastic gradients is bounded by a linear function of either the gradient-norm, or the optimality gap, respectively.
} 
A form of automatic reduction in gradient noise occurs~\citep{liu2020accelerating} when strong or weak growth is satisfied, explaining why variance reduction may be unnecessary. 

The connection between over-parameterization and fast stochastic optimization has led to a wave of interest in analyzing first-order methods under interpolation and weak/strong growth.
A number of works have shown that \ac{SGD} obtains the fast convergence rate of deterministic gradient methods for interpolating models~\citep{schmidt2013fast, bassily2018exponential, vaswani2019fast, cevher2018linear, jain2018accelerating}, while closely related research has established accelerated convergence rates under an additional requirement for convexity~\citep{liu2020accelerating, vaswani2019fast, jain2018accelerating}.
Sub-sampled second-order methods have also been explored and proved to have local linear convergence for specific function classes~\citep{meng2020fastandfurious}.
These positive results show the interpolation setting is restrictive enough to break the \( \Omega\rbr{\epsilon^{-4}} \) barrier for stochastic first-order methods and attain the optimal \( \Theta(\epsilon^{-1}) \) complexity (up to problem-specific constants) \edit{for finding stationary points of} smooth, convex functions~\citep{nemirovsky1985optimal, arjevani2016iteration}. 

Despite these rapid advances, theoretical rates and practical performance for \ac{SGD} under interpolation still rely on carefully selected hyper-parameters.
A number of approaches from the general stochastic setting are rapidly being adopted to tackle this problem.
For instance, several works have considered a stochastic version of the Polyak step-size, which does not require knowledge of the smoothness or convexity parameters~\citep{loizou2020sps, berrada2019training}. 
Stochastic line-searches using the classic Armijo condition~\citep{armijo1966ls} have also been proposed and shown to obtain fast convergence under interpolation~\citep{vaswani2019painless}.
Very recently, adaptive variants of \ac{SGD} without momentum have analyzed both with and without the Armijo line-search~\citep{vaswani2020adaptive}.

This thesis analyzes a small, but core group of methods in stochastic optimization under interpolation.
The focus is on first-order algorithms; we consider constant step-size \ac{SGD}, \ac{SGD} with a stochastic Armijo line-search, and a version of Nesterov's accelerated gradient descent with stochastic gradients~\citep{nesterov2004lectures}. 
In nearly all cases, we show that existing analyses can be tightened to yield faster convergence rates with a larger range of step-sizes. 
In the case of acceleration, the improvement is comparable to taking the square-root of the condition number and addresses criticisms that previous analyses yield inferior rates to those of \ac{SGD} in some circumstances~\citep{liu2020accelerating}. 

The thesis is organized as follows: 
In \autoref{ch:interpolation-gc}, we formalize interpolation and the strong/weak growth conditions in the context of general stochastic oracles. 
Connections between interpolation, smoothness properties of the stochastic oracle, and growth of the stochastic gradients are derived.
\autoref{ch:sgd} analyzes the complexity of \ac{SGD} with a fixed step-size, drawing comparisons with previous work as well as convergence rates in the deterministic setting. 
Asymptotic convergence of SGD with a constant step-size to
\begin{inparaenum}[(i)]
\item stationary points of general non-convex functions, and
\item minimizers of convex functions 
\end{inparaenum}
is shown under the strong and weak growth conditions.
Chapters~\ref{ch:line-search} and~\ref{ch:acceleration} then address the convergence of \ac{SGD} with a stochastic Armijo line-search and stochastic accelerated gradient descent, respectively. 
Finally, \autoref{ch:beyond-interpolation} leaves the basic interpolation setting behind and considers structural minimization with interpolating functions as components. 
Convergence to an explicit neighborhood is derived for \( L_2 \)-regularized problems as a special case. 


\section{Preliminaries and Assumptions}\label{sec:setup}

%% Basic Assumptions
This work considers the problem of minimizing a continuous function \( f : \R^d \into \R \) under interpolation conditions.
We assume that \( f \) is bounded below with at least one finite minimizer.
That is, there exists \( \wopt \in \R^d \) such that 
\begin{align*}
    f(w) \geq f(\wopt) \quad \quad \forall w \in \R^d. 
\end{align*}
Additionally, \( f \) is required to be differentiable and \( L \)-smooth, meaning the gradient mapping \( \w \mapsto \grad(\w) \) is an \( L \)-Lipschitz function,
\[ \norm{\grad(\w) - \grad(v)} \leq L \norm{\w - v} \quad \quad \forall \, \w, v \in \R^d. \]
This is equivalent to the existence of the following quadratic upper-bound on \( f \):
\begin{align*}
    f(v) &\leq f(w) \abr{\grad(\w), v - \w} + \frac{L}{2}\norm{v - \w}^2 &\forall \, \w,v \in \R^d. \tag{\( L \)-Smoothness} 
    \intertext{At times, we will further assume that \( f \) is convex or \( \mu \)-strongly-convex,}
    f(v) &\geq f(\w) + \abr{\grad(\w), v - \w} &\forall \, \w,v \in \R^d, \tag{Convexity} \\
    f(v) &\geq f(\w) + \abr{\grad(\w), v - \w} + \frac{\mu}{2}\norm{v - \w}^2 &\forall \, \w,v \in \R^d. \tag{\( \mu \)-Strong-Convexity}
\end{align*}
Convexity is satisfied for many optimization problems occurring in machine learning, including linear and logistic regression. 

Convexity can be relaxed to a more limited property called invexity.
Formally, we say a \edit{differentiable} function \( f \) is invex if all stationary points are also global minimizers of \( f \)~\citep{ben1986invexity}, meaning
\[ \grad(\w) = 0 \implies f(\w) \leq f(v) \quad \quad \forall \, v \in \R^d.  \]
Invexity is formally weaker than convexity and follows immediately from the first-order conditions for convexity given above.
The analogue of strong-convexity for invex functions is the Polyak-≈Åojasiewicz (PL) condition~\citep{karimi2016linear}.
A function is said to be \( \mu \)-PL if
\[ \frac{1}{2 \mu} \norm{\grad(\w)}^2 \leq f(\w) - f(\wopt), \]
holds for all \( \w \in \R^d \).
Again, the Polyak-≈Åojasiewicz condition is weaker than strong convexity; a \( \mu \)-strongly-convex function is \( \mu \)-PL, but the converse does not hold.

\edit{
In several cases, it will be useful to interpret our results in the context of finite-sum functions.
The function \( f \) is said to be a finite-sum if it can be written as 
\[ f(\w) = \frac{1}{n} \sum_{i=0}^n f_i(\w),  \]
where the individual (or sub-) functions \( f_i : \R^d \rightarrow \R \) may be strongly-convex, convex, or non-convex depending on the problem.
Such objective functions arise naturally in empirical risk minimization, where \( f_i \) typically corresponds to a single training example \( (x_i, y_i) \) in a training set.
For example, the classic least-squares regression objective can be written as 
\[ f(\w) = \frac{1}{n} \sum_{i=0}^n \rbr{\abr{\w, x_i} - y_i}^2, \] 
which is finite-sum with sub-functions \( f_i(\w) =  \rbr{\abr{\w, x_i} - y_i}^2 \).
}

\section{Related Work}~\label{sec:related-work}

\noindent \textbf{Interpolation}:
Existing work focuses on interpolation in the context of finite-sum objectives. 
\edit{In this setting, \citet{bassily2018exponential} define interpolation in terms of sequences converging to global minima of \( f \). 
They say interpolation holds if, for every sequence \( \seq{\wk} \) satisfying \( \lim_{k \rightarrow \infty} f(\wk) = f(\wopt) \), we also have 
\[ \forall i \in [n], \quad \lim_{k \rightarrow \infty} f_i(\wk) = f(\wopt). \]
\citet{berrada2019training} propose \( \epsilon \)-interpolation, which requires the sub-functions to be lower-bounded and \( \epsilon \) sub-optimal for all \( \wopt \in \argmin f \): 
\[ \forall i \in [n], \quad \inf_\w f_i(\w) \geq B \quad \text{and} \quad f_i(\wopt) - B \leq \epsilon. \]
A larger body of work considers interpolation to hold when the optimal points \( \wopt \) are also stationary points or global minimizers of each sub-function \( f_i \)~\citep{vaswani2019fast, vaswani2019painless, vaswani2020adaptive, meng2020fastandfurious, loizou2020sps},
\begin{gather*}
    \wopt \in \argmin_{\w} f(\w) \implies \wopt \in \argmin_{\w} f_i(\w) \quad \forall i \in [n],\\
          \text{or} \\
    \wopt \in \argmin_{\w} f(\w) \implies \grad_i(\wopt) = 0 \quad \forall i \in [n].
\end{gather*}
We will focus on interpolation conditions of this form, which we extend to general stochastic optimization problems.\\ 
}

\noindent \textbf{Growth Conditions}:
The strong growth condition was first proposed in the context of incremental gradient methods by \citet{solodov1998incremental} and \citet{tseng1998incremental} as a bound on the squared-norm of the stochastic gradients.
\edit{Suppose \( f \) is finite-sum and \( P_i \) is an arbitrary distribution used to sub-sample the finite sum. 
    Then strong growth with parameter \( \rho > 0 \) may be written as 
\[ \text{Maximal Strong Growth:} \quad \norm{\grad_i(\w)}^2 \leq \rho \norm{\grad(\w)}^2, \hspace{8.5em} \]
where the inequality holds almost-surely with respect to \( P_i \).}
This definition was later used by~\citet{schmidt2013fast} to derive linear convergence rates for \ac{SGD} with a constant step-size. 
\citet{vaswani2019fast} propose a modified version of strong growth which holds in expectation,
\edit{\[ \text{Strong Growth:} \quad \E_{P_i} \sbr{\norm{\grad_i(\w)}^2} \leq \rho \norm{\grad(\w)}^2. \hspace{6.6em} \]}
We call this latter condition strong growth and refer to the earlier definition as maximal strong growth.
\citet{vaswani2019fast} also propose the following weak growth condition as a natural relaxation of strong growth: 
\edit{\[ \text{Weak Growth}: \quad \E_{P_i}\sbr{\norm{\grad_i(\w)}^2} \leq 2 \rho L \rbr{f(\w) - f(\wopt)}. \hspace{2.2em} \]}
\citet{cevher2018linear} refer to strong growth simply as ``the growth condition'' and suggest strong growth with an additive noise parameter as an alternative relaxation, 
\edit{\[ \text{Strong Growth + Noise:} \quad \E_{P_i} \sbr{\norm{\grad_i(\w)}^2} \leq \rho \norm{\grad(\w)}^2 + \sigma^2, \hspace{8em} \]}
which they also call weak growth.
For clarity, we refer to this assumption as strong growth with additive noise.
For unbiased \( P_i \),\footnote{\( P_i \) is unbiased if \( \E_{P_i}\sbr{\grad_i(\w)} = \grad(\w) \).} strong with with noise is slightly weaker than assuming a bound on the variance of the stochastic gradients~\citep{khaled2020better, ghadimi2012optimal1}.
\citet{cevher2018linear} study the convergence of proximal-gradient methods under strong growth with additive noise and also prove that strong growth is both \emph{sufficient and necessary} for \ac{SGD} to converge linearly.\\


\noindent \textbf{Stochastic Acceleration}: 
Many authors have considered accelerating stochastic gradient methods.
\citet{schmidt2011convergence} provide sufficient conditions on gradient noise for acceleration of proximal-gradient methods.
\edit{D'Aspremont}~(\citeyear{aspremont2008approximate}) and \citet{devolder2014first} investigate accelerated gradient methods under the assumption of approximate oracles with deterministic, bounded errors and derive rates for convergence and error accumulation in this setting.
\edit{\citet{honorio2012biased} consider accelerated proximal-gradient methods under biased stochastic oracles and show accumulated errors prevent a high-probability convergence guarantee.} 
In contrast, \citet{cohen2018acceleration} assume access to an unbiased oracle with additive gradient noise and propose a noise-resistant acceleration scheme.
Very recently, \citet{chen2020understanding} analyze accelerated methods for strongly-convex functions under strong growth with additive noise. 

\edit{An alternative approach to stochastic acceleration splits the convergence rate into stochastic and deterministic parts.
Using this framework, many authors have shown that acceleration schemes speed-up convergence for the deterministic component and achieve nearly optimal rates in the stochastic approximation setting~\citep{ghadimi2012optimal1, ghadimi2013optimal2, hu2009accelerated}.
For finite-sum functions, variance reduction techniques can be combined with acceleration to improve convergence on the stochastic component~\citep{allen-zhu2017katyusha, allen-zhou2018katyushax, tang2018restkatyusha, kovalev2020loopless}.
Such methods are as fast as deterministic acceleration up to a dependence on \( n \), the number of finite-sum functions.}
Alternative approaches also leveraging finite-sum structure are based on the proximal-point algorithm and include Catalyst~\citep{lin2017catalyst} and accelerated APPA~\citep{frostig2015unregularizing}.

Several works have recently considered acceleration under interpolation.
The most similar to the investigation here is that of \citet{vaswani2019fast}, who analyze a slightly altered version of Nesterov's accelerated gradient descent under the strong growth condition. 
\citet{liu2020accelerating} propose a different modification, called MaSS, and analyze its convergence for convex quadratics as well as strongly-convex functions with additional assumptions. 
These assumptions imply strong growth, but yield hard-to-compare rates. 
\citet{jain2018accelerating} prove accelerated rates for squared-losses under interpolation using a tail-averaging scheme. 
Finally, \citet{assran2020convergence} study the stochastic approximation setting and prove that accelerated gradient descent may fail to accelerate even when interpolation is satisfied.\\

\noindent \textbf{Line Search}:
Line-search is a classic technique to set the step-size in deterministic optimization (see \citet{nocedal1999numerical}), but extensions to stochastic optimization are non-obvious. 
\citet{mahsereci2017pls} use a Gaussian process model to define probabilistic Wolfe conditions~\citep{wolfe1969convergence, wolfe1971convergence}; however, the convergence of \ac{SGD} with this procedure is not known.
\citet{fridovich2019choosing} propose line-search conditions based on golden-section search~\citep{avriel1968golden}, but again only provide empirical evidence for \ac{SGD}. 
\edit{
\citet{paquette2020stochastic}, \citet{krejic2013line}, and \citet{ogaltsov2019adaptive} prove convergence of \ac{SGD} with the Armijo condition in the general stochastic setting with several caveats. 
\citet{paquette2020stochastic} require adaptive batch-sizes, while \citet{krejic2013line} assume the stochastic gradients are bounded and \citet{ogaltsov2019adaptive} use explicit knowledge of an upper-bound on the variance.} 
Other authors consider similar approaches based on multiple function and/or gradient evaluations at each iteration~\citep{friedlander2012hybrid, byrd2012sample, de2016big}.
Such approaches also use growing batch-sizes to ensure convergence.
The work in the thesis follows \citet{vaswani2019painless}, who investigated stochastic versions of the Armijo line-search under interpolation; we provide tighter analyses in a more general setting.\\ 


\noindent \textbf{Asymptotic Convergence}:
The original paper by \citet{robbins1951sgd} establishes asymptotic, almost-sure convergence for \ac{SGD} to the zero of a convex function if the stochastic gradients are bounded and a decreasing step-size is used.
Highly general analyses have since shown almost-sure convergence for non-convex functions when strong growth with additive noise is satisfied~\citep{bertsekas2000gradient, bottou1991approche}.
Alternative work directly derives these conditions from properties of strongly-convex and non-convex functions, also for the purpose of proving almost-sure convergence~\citep{nguyen2018sgd, lei2019stochastic}.
Recently, asymptotic convergence was shown with Adagrad-style step-sizes instead of a fixed, decreasing schedule~\citep{li2019convergence}.


\endinput
