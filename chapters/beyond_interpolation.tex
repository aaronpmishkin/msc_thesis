% !TEX root = ../main.tex

\chapter{Beyond Interpolation}~\label{ch:beyond-interpolation}

So far we have analyzed the performance of stochastic gradient methods for unconstrained minimization of a \( L \)-smooth function \( f \) given access to a SFO \oracle{} such that \( \rbr{f, \oracle{}} \) satisfies interpolation or a growth condition. 
This chapter extends these analyses to \( L_2 \)-regularized minimization of \( f \), 
\[ \min_{\w \in \R^d} F(\w) := f(\w) + \frac{\lambda}{2} \norm{\w}^2, \]
where \( \lambda > 0 \) is a tuning parameter which controls the degree of regularization. 
To maintain consistency with common notation, \( \wopt \) will refer to the global minimizer of this regularized problem, while minimizers of \( f \) will now be denoted as \( \wbar \).

The canonical SFO for \( L_2 \)-regularized minimization evaluates the regularization term exactly, giving the following stochastic function and gradient evaluations: 
\[ F(\w, \zk) = f(\w, \zk) + \frac{\lambda}{2} \norm{\w}^2 \quad \quad \nabla F(\w, \zk) = \grad(\w, \zk) + \lambda \w.  \]
We call this the \( L_2 \)-regularization of \( \oracle{} \) and denote it as \( \oracle{}_2 \).
Except in degenerate circumstances,\footnote{For example, when \( f \) is minimized at \( 0 \).} the pair \( \rbr{F, \oracle{}_2} \) will \emph{not} satisfy interpolation or weak/strong growth even when \( \rbr{f, \oracle{}} \) does. 
This means the analyses from previous chapters cannot be applied directly to regularized problems.

There is still significant reason to believe the \( L_2 \)-regularization of \( f \) can be minimized efficiently when \( \rbr{f, \oracle{}} \) satisfy the weak growth condition.
For example, \( f(\wopt) - f(\wbar) \) will to be small when \( \lambda \ll L \) and so we might expect \( \rbr{F, \oracle{}_2} \) to approximately satisfy weak growth. 
On the other hand, \( \oracle{}_2 \) will become nearly deterministic as \( \lambda \rightarrow \infty \) and \( \frac{\lambda}{2} \norm{\w}^2 \) dominates \( f \).
Indeed, intuition indicates that it is only when \( \lambda \) is moderate that minimizing \( F \) will be challenging.
The following lemma formalizes these observations into a version of weak growth which holds for \( \rbr{F, \oracle{}_2} \).

\begin{restatable}{lemma}{regularizedWGC}\label{lemma:regularized-wgc}
    Let \( f \) be an \( L \)-smooth function and oracle{} a SFO such that \( \rbr{f, \oracle{}} \) satisfy the weak growth condition with parameter \( \rho \).
    Then the \( L_2 \)-regularized problem \( \rbr{F, \oracle{}_2} \) satisfies the following inequality for all \( \w \in \R^d \) and \( k \geq 0 \):
    \[ \Ek\sbr{\norm{\nabla F(\w, \zk)}^2} \leq 4 \max\cbr{\rho L, \lambda} \rbr{F(\w) - F(\wopt) - \frac{\lambda - L}{2} \norm{\wopt - \wbar}^2 + \frac{\lambda}{2} \norm{\wbar}^2}, \]
    where \( \wopt \) minimizes the regularized function \( F \).
    Moreover, this can be improved to 
    \[ \Ek\sbr{\norm{\nabla F(\w, \zk)}^2} \leq 4 \max\cbr{\rho L, \lambda} \rbr{F(\w) - F(\wopt) - \frac{\lambda + \mu}{2} \norm{\wopt - \wbar}^2 + \frac{\lambda}{2} \norm{\wbar}^2}, \]
    if \( f \) is \( \mu \)-strongly-convex (with \( \mu = 0 \) giving the convex case).
\end{restatable}
See \autoref{app:beyond-interpolation} for proof.

\section{Convergence for \( L_2 \)-Regularized Convex Functions}~\label{sec:regularized-convex}

\begin{restatable}{theorem}{sgdReguarlizedWGC}\label{thm:sgd-regularized-wgc}
    Let \( f \) be a \( \mu \)-strongly-convex (with \( \mu = 0 \) in the convex case), \( L \)-smooth function with at least one finite minimizer \( \wbar \) and \( \oracle{} \) a SFO such that the pair \( \rbr{f, \oracle{}} \) satisfies the weak growth condition with constant \( \rho \).
    Then SGD with constant step-size \( \eta \leq \frac{\mu + \lambda}{\max\cbr{\rho L, \lambda}\rbr{(\mu + \lambda) + (L + \lambda)}} \) obtains the following convergence rate for the \( L_2 \)-regularized problem \( \rbr{F, \oracle{}_2} \):
    \[  \E\sbr{\norm{\wkk - \wopt}^2} \leq \rbr{1 - \frac{2 \eta (\mu + \lambda)(L + \lambda)}{(\mu + \lambda) + (L + \lambda)}}^k \norm{\w_0 - \wopt}^2 + \frac{\gamma}{2}\rbr{\lambda \norm{\wbar}^2 - \rbr{\lambda + \mu}\norm{\wopt - \wbar}^2}, \]
    where \( \gamma = \frac{\eta \max\cbr{\rho L, \lambda} \sbr{(\mu + \lambda) + (L + \lambda)}}{(\mu + \lambda)(L + \lambda)}. \)  
\end{restatable}
See \autoref{app:regularized-convex} for proof.
