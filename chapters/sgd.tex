%!TEX root = ../main.tex

\chapter{Stochastic Gradient Descent}\label{ch:sgd}

\section{Convergence for Convex Functions}

We establish the convergence rate of SGD for convex functions under the weak growth condition. These results improve on those given by \citet{vaswani2019fast} by constant factors. 
Moreover, we provide cleaner and simpler proofs.
\todo{Prove better version of this theorem with coercivity of the gradient.}

\begin{theorem}
    Let \( f  \) be a convex, \( L  \)-smooth function satisfying the weak growth condition with constant \( \rho  \). 
    Stochastic gradient descent with step-size \( \eta < \frac{2}{L} \) obtains the following convergence rate:
    \[ f(\wkk) - f(\wopt) \leq   \]
\end{theorem}

\endinput
