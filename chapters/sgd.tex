%!TEX root = ../main.tex

\chapter{Stochastic Gradient Descent}~\label{ch:sgd}

The discussion in the previous chapter formalized interpolation for general stochastic optimization problems and derived connections between interpolation and the weak/strong growth conditions.  
Now, we turn to the main interest of this work: the complexity of iterative algorithms for \( \rbr{f, \oracle{}} \) when interpolation is satisfied. 
This chapter analyzes the convergence of \ac{SGD} for strongly-convex and convex functions, while the next two chapters tackle \ac{SGD} with the Armijo line-search (\autoref{ch:line-search}) and stochastic acceleration (\autoref{ch:acceleration}).
In particular, this chapter establishes the following non-asymptotic results for \ac{SGD} with a fixed step-size: 
\begin{enumerate}
    \item Linear convergence \emph{in-expectation} for strongly-convex \( f \) when \( \rbr{f, \oracle{}} \) satisfies strong growth; this rate is tight with the best-known deterministic rates when \( \sgc = 1 \).
    \item \emph{Almost sure} linear convergence for strongly-convex \( f \) and individually-smooth and strongly-convex \ac{SFO} \oracle{}. 
    \item Sub-linear convergence for convex \( f \) when \( \rbr{f, \oracle{}} \) satisfies weak growth; our proof is simpler than existing analyses and permits a larger step-size.
    \item Faster sub-linear convergence for convex \( f \) when \( \rbr{f, \oracle{}} \) satisfies weak growth \emph{and} \oracle{} is individually-smooth; this rate is tight with the deterministic case when \( \wgc = 1 \).
\end{enumerate}
\autoref{sec:almost-sure} at the end of this chapter leaves the finite-time regime and considers asymptotic, almost-sure convergence of \ac{SGD} with a fixed step-size under strong and weak growth, respectively.
The following is proved: 
\begin{enumerate}
    \item Almost-sure convergence to a stationary point when \( f \) is a general non-convex function and \( \rbr{f, \oracle{}} \) satisfies strong growth.
    \item Almost-sure convergence to a global minimum when \( f \) is convex and \( \rbr{f, \oracle{}} \) satisfies weak growth.
\end{enumerate}
This last result is particularly interesting because it concerns convergence of the last input generated by \ac{SGD};
we shall see that such results are not straightforward in the non-asymptotic regime, where convergence is instead shown for an averaged input. 

\begin{figure}[t]
\begin{procedure}{Stochastic Gradient Descent}
\item Choose an initial point \( \w_0 \in \R^d \).
\item For each iteration \( k \geq 0 \):
    \begin{enumerate}
        \item Query \oracle{} for \( \grad(\wk, \zk) \).
        \item Update input as\vspace{-1ex}%
            \[ \wkk = \wk - \eta \grad(\wk, \zk). \]
    \end{enumerate}
\end{procedure}
\caption[Procedural definition of stochastic gradient descent with a fixed step-size.]%
        {Stochastic gradient descent with a fixed step-size \( \eta \). 
         Note that only one query to the stochastic oracle is needed per-iteration.}~\label{procedure:sgd}
\end{figure}

Now, let us briefly introduce \ac{SGD} with a fixed step-size before diving into the analysis.
The basic procedure is given in \autoref{procedure:sgd}; the key components of the algorithm are %
\begin{inparaenum}[(i)] 
    \item the use of a stochastic gradient \( \grad(\wk, \zk) \) queried from the oracle at every iteration,
    \item the fixed step-size \( \eta > 0 \), and
    \item the sequence of inputs \( \seq{\wk} \) generated by the algorithm, which are called the \emph{iterates}. 
\end{inparaenum}
The non-asymptotic rates in this chapter will be derived by analyzing the sequence of distances to a minimizer, \( \seq{\norm{\wk - \wopt}^2} \). 
The minimizer \( \wopt \) is unique for strongly-convex functions and we show \( \wk \rightarrow \wopt \) in the iteration limit. 
In contrast, we only establish \( f(\bar \w_K) \rightarrow f(\wopt) \) for convex functions, where \( \bar \w_K  = \frac{1}{K}\sum_{k=0}^{K-1} \wk \).
\autoref{ch:line-search} discusses these proof techniques in greater detail and with specific reference to the case where \( \eta \) is itself a random variable that depends on \( \grad(\wk, \zk) \).
We will see that this introduces significant challenges compared to \ac{SGD} with a fixed and deterministic step-size.\\

\section{Convergence for Strongly-Convex Functions}~\label{sec:sgd-sc}

The analysis of fixed step-size \ac{SGD} for strongly-convex functions is divided into two sub-sections based on the properties of \oracle{}.
First, we consider general \acp{SFO} satisfying strong growth. 
Then, the setting is restricted to individually-smooth and convex \acp{SFO}, where we show that existing convergence results can be slightly improved.
Finally, we analyze the case when \oracle{} is both individually-smooth and individually-strongly-convex.
This setting degenerates to a simple deterministic optimization problem if \( \grad(\cdot, z) \) is directly accessible to the optimizer, which is true for mini-batch oracles. 

\subsection{General Oracles}~\label{sec:sgd-sc-general}

We first establish the convergence rate of \ac{SGD} for strongly-convex \( f \) when \( \rbr{f, \oracle} \) satisfies the strong growth condition. 
Recall from \autoref{lemma:interpolation-to-sgc} that this is more general than assuming \oracle{} is \( \Lmax \) individually-smooth and \( \rbr{f, \oracle} \) satisfies minimizer interpolation. 
Furthermore, in the case that individual smoothness and minimizer interpolation do hold, we are guaranteed \( \sgc \leq \frac{\Lmax}{\mu} \).
This value should be kept in mind, as it informs the worst-case rate that can be obtained in the following theorem. 

\begin{restatable}{theorem}{sgcConvex}~\label{thm:sgc-convex}
    Let \( f \) be a \( \mu \)-strongly-convex, \( L \)-smooth function and \( \oracle{} \) a \ac{SFO} such that \( \rbr{f, \oracle{}} \) satisfies the strong growth condition with parameter \( \sgc \).
    Then stochastic gradient descent with fixed step-size \( \eta \leq \frac{2}{\sgc(\mu + L)} \) converges as 
    \[ \E\sbr{\norm{\w_{K} - \wopt}^2} \leq \rbr{1 - \frac{2\eta \mu L}{\mu + L}}^K \norm{\w_0 - \wopt}^2. \] 
\end{restatable}%
\noindent See \autoref{app:sgd-sc} for proof.\hfill \break

We compare this convergence rate to the original result given by \citet[Section 6]{schmidt2013fast} in \autoref{table:sgd-sc-comparison}.
Our analysis allows for a larger step-size and establishes asymptotically faster convergence.
The improvement is most significant for ill-conditioned problems, where \( \mu \ll L \) implies \( \frac{2 L}{\mu + L} \approx 2 \). 
Finally, this result is tight in the sense that when \( \sgc = 1 \), which holds in the deterministic setting, it recovers the best known convergence rate for gradient descent on strongly-convex functions~\citep[Theorem 3.12]{bubeck2015convex}.

When \( \rbr{f, \oracle{}} \) satisfies minimizer interpolation and \oracle{} is individually-smooth, the complexity given by \autoref{thm:sgc-convex} can be worse than that achieved under the weak-growth condition.
To see this, consider when the worst-case values \( \sgc = \frac{\Lmax}{\mu} \) and \( \wgc = \frac{\Lmax}{L} \) are attained in the interpolation setting. 
If \( \eta = \frac{2}{\sgc \rbr{\mu + L}} = \frac{2 \mu}{\Lmax\rbr{\mu + L}} \), then \autoref{thm:sgc-convex} guarantees  
\[ \E\sbr{\norm{\w_{K} - \wopt}^2} \leq \rbr{1 - \frac{4 \mu^2 L }{\Lmax \rbr{\mu + L}^2}}^K \norm{\w_0 - \wopt}^2. \]
In contrast, \citet[Theorem 5]{vaswani2019fast} show 
\[ \E\sbr{\norm{\w_{K} - \wopt}^2} \leq \rbr{1 - \frac{\mu}{\Lmax}}^K \norm{\w_0 - \wopt}^2, \]
with a bigger step-size of \( \eta = \frac{1}{\Lmax} \).
Noting \( \frac{4 \mu L}{\rbr{\mu + L}^2} \leq 1 \) --- with equality when \( \mu = L \) --- establishes that the rate given in this work is slower.

The discrepancy in convergence rates for smooth, interpolating oracles emerges from the use of smoothness and strong-convexity in the proof of \autoref{thm:sgc-convex}.
The worst-case value for \( \rho \) is proved using \( \Lmax \)-smoothness of \( f(\cdot, \zk) \) and \( \mu \)-strong-convexity of \( f \) (see \autoref{lemma:interpolation-to-sgc}).
Thus, bounding \( \Ek\sbr{\norm{\grad(\wk, \zk)}^2} \) using strong growth and then following the typical, deterministic proof strategy equates to using smoothness and strong-convexity \emph{twice} and leads to an unnecessarily loose bound.

The above conclusion only holds when \( \rbr{f, \oracle} \) satisfies interpolation and \oracle{} is individually smooth. 
The two convergence speeds cannot be directly compared when \oracle{} is more general because of the cyclic relationship between the strong/weak growth parameters. 
Recall from \autoref{lemma:sgc-wgc-relationship} that strong growth implies weak growth with constant \( \alpha \leq 2 \rho L \), while weak growth implies strong growth with constant \( \rho \leq \alpha \frac{L}{\mu} \).
As a result, no order can be established on \( \alpha \) and \( \rho \) and it is not clear which rate is superior.

\subsection{Individually Smooth and Convex Oracles}~\label{sec:sgd-sgc-ind-convex}

It is possible to improve upon the result from \citet{vaswani2019fast} when \oracle{} further satisfies individual convexity.
In particular, we are able to obtain the same convergence speed with a looser bound on the step-size.
A case analysis in \autoref{app:sgd-additional-lemmas} shows that the step-size permitted by \citet[Theorem 5]{vaswani2019fast} is \( \eta \leq \frac{\mu + L}{\alpha L^2} \), which is \( \frac{\mu + L}{L \Lmax}\) in the worst-case.
If \( \mu < L \), then \( \frac{\mu + L}{L \Lmax} < \frac{2}{\Lmax} \) and the following theorem improves upon the original result. 

\begin{restatable}{theorem}{sgcIndConvex}~\label{thm:sgc-ind-convex}
    Let \( f \) be a \( \mu \)-strongly-convex, \( L \)-smooth function and \( \oracle{} \) an \( \Lmax \) individually-smooth and convex \ac{SFO} such that \( \rbr{f, \oracle{}} \) satisfies minimizer interpolation.
    Then stochastic gradient descent with fixed step-size \( \eta < \frac{2}{\Lmax} \) converges as
    \[ \E\sbr{\norm{\w_{K} - \wopt}^2} \leq \rbr{1 - \mu \, \eta \rbr{2 - \eta \Lmax}}^K \norm{\w_0 - \wopt}^2. \]
\end{restatable}
\noindent See \autoref{app:sgd-sc} for proof.\hfill \break

\begin{table}[t]
    \centering
    \begin{tabular}{c l l c c  }\toprule
        \multirow{2}{*}{Assumptions} & \multicolumn{2}{c}{Max. Step-Size}  & \multicolumn{2}{c}{Best Convergence Rate}\\%
        \cmidrule(lr){2-3} \cmidrule(l){4-5}
                 & \multicolumn{1}{c}{Ours} & \multicolumn{1}{c}{SLR} & \multicolumn{1}{c}{Ours} & \multicolumn{1}{c}{SLR}\\ \midrule
     — & \( \eta \leq \frac{2}{\sgc\rbr{\mu + L}} \)% 
                 & \( \eta \leq \frac{2}{\sgc L} \)%
                 & \( O\rbr{\frac{\rho{(\mu + L)}^2}{4\mu L} \log\rbr{\inv{\epsilon}}} \)% 
                 & \( O\rbr{\frac{\rho L}{\mu} \log\rbr{\inv{\epsilon}}} \) \\ \addlinespace
    \makecell{Ind. Smooth \\ \& Convex}%
                 & \( \eta < \frac{2}{\Lmax} \)%
                 & \multicolumn{1}{c}{N/A}%
                 & \( O\rbr{\frac{\Lmax}{\mu}\log\rbr{\inv{\epsilon}}} \)%
                 & \multicolumn{1}{c}{N/A}\\ \addlinespace 
    \makecell{Ind. Smooth \\ \& \(\mu_z\)-SC}% 
                 & \( \eta \leq \frac{2}{\mumax + \Lmax} \)%
                 & \multicolumn{1}{c}{N/A}% 
                 & \( O\rbr{\frac{\mumax + \Lmax}{4 \delta_{\text{min}}} \log\rbr{\frac{1}{\epsilon}}} \)%
                 & \multicolumn{1}{c}{N/A} \\ \bottomrule 
    \end{tabular}
    \caption[Comparison of iteration complexities of fixed step-size stochastic gradient descent for strongly-convex functions under strong growth.]%
    {Comparison of iteration complexities of fixed step-size \ac{SGD} for \( \mu \)-SC \( f \) under the strong growth condition.
     For each result, we report the complexity obtained with the optimal step-size according to that analysis.
    Recall from \autoref{thm:sgc-ind-sc} that \( \delta_{\text{min}} = \min_{\z \in \calZ} \frac{\mu_\z L_\z}{\mu_\z + L_\z} \).
     The first row shows our results for general \oracle{} are tighter than SLR \citep{schmidt2013fast} because they allow for larger step-sizes.}%
    \label{table:sgd-sc-comparison}
\end{table}

A key feature of \autoref{thm:sgc-ind-convex} is that it requires only the full function \( f \) to be strongly-convex; the stochastic functions \( f(\cdot, \z) \) may be merely convex, as is typically the case in the finite-sum setting.
To illustrate this, consider the linear regression problem
\[ \min_{\w \in R^d} \sum_{i=0}^n \rbr{\abr{\w, x_i} - y_i}^2, \]
which satisfies interpolation if \( y \in \text{span}(\cbr{x_i}_{i=0}^n) \).
The objective \( f \) is \( \mu \)-strongly-convex if \( n \geq d \) and the data matrix is full-rank, while the individual functions \( f_i(\w) = \rbr{\abr{\w, x_i} - y_i}^2 \) are \emph{not} strongly-convex unless \( d = 1 \).%
\footnote{Consider \( w \) and \( \w' = \w + v \), where \( v \) is orthogonal to \( x_i \), to see the \( f_i \) are not strongly-convex when \( d > 1 \).}
Learning problems of this form occur in non-parametric kernel regression, such as when using radial basis functions with a small bandwidth~\citep{hastie2009elements}.
However, in the unlikely case that \oracle{} is also individually-strongly-convex, we can show that \ac{SGD} will converge almost surely, as established in the following theorem.

\begin{restatable}{theorem}{sgcIndSC}~\label{thm:sgc-ind-sc}
    Let \( f \) be a \( \mu \)-strongly-convex, \( L \)-smooth function and \( \oracle{} \) an \( \Lmax \) individually-smooth and \( \mumax \)-strongly-convex \ac{SFO} such that \( \rbr{f, \oracle{}} \) satisfies minimizer interpolation. 
    Then stochastic gradient descent with fixed step-size \( \eta \leq \frac{2}{\mumax + \Lmax} \) converges almost surely at the rate 
    \[ \norm{\w_{K} - \wopt}^2 \leq \rbr{1 - 2 \eta \, \delta_{\text{min}} }^K \norm{\w_0 - \wopt}^2, \] 
    where \( \delta_{\text{min}} = \min_{\z \in \calZ} \frac{\mu_\z L_\z}{\mu_\z + L_\z} \).
\end{restatable}%
\noindent See \autoref{app:sgd-sc} for proof.\hfill \break

\autoref{thm:sgc-ind-sc} is not surprising; strongly-convex functions have only one minimizer, meaning that gradient-descent on a single stochastic function \( f(\cdot, \zk) \) is sufficient to recover the global solution to the optimization problem.
Choosing the best-conditioned sub-function function yields convergence to \( \wopt \) as \( O\rbr{\exp\cbr{ - 2 \eta \, \delta_{\text{max}} \, K}} \), where \( \delta_{\text{max}} = \max_{\z \in \calZ} \frac{\mu_z L_\z}{\mu_z + L_\z} \) \citep{bubeck2015convex}.
Notice that we have exchanged worst-case performance for best-case (\( \delta_{\text{min}} \) vs \( \delta_{\text{max}} \)) by optimizing \( f(\cdot, \z) \) directly.
\ac{SGD} is clearly sub-optimal when \oracle{} is individually-strongly-convex and the optimization procedure has direct access to \( f(\cdot, \z ) \) for each \( \z \in \calZ \), such as in the finite-sum setting. 
This illustrates the dangers of assuming individual strong-convexity and interpolation hold simultaneously.

\section{Convergence for Convex Functions}~\label{sec:sgd-convex}

Convex functions are significantly more interesting than strongly-convex functions when interpolation is satisfied. 
Minimizer interpolation for convex functions implies \( \calX^* \subseteq \calX^*_{\z} \) --- the optimal set for \( f \) is a subset of the optimal set for \( f(\cdot, \z) \).
This condition intuitively feels milder than the requirement for a single shared optimal point \( \wopt \). 
Moreover, assuming individual convexity does not lead to degenerate optimization problems such as those seen in the previous section.
Convex functions also require a major shift in analysis; now we shall show concentration of the optimality gap \( f(\w) - f(\wopt) \), rather than shrinking distance to a specific minimizer, \( \norm{\w - \wopt}^2 \).

We first establish the convergence rate of \ac{SGD} for convex functions under the weak growth condition. 
As before, it is strictly more general to analyze the complexity of \ac{SGD} under weak growth than in the setting where \oracle{} is individually-smooth and minimizer interpolation holds. 
Remember that \autoref{lemma:interpolation-to-wgc} guarantees \( \wgc \leq \frac{\Lmax}{L} \). 
The following result improves on that given by \citet{vaswani2019fast} by constant factors and allows for a larger step-size (see \autoref{table:sgd-comparison}).  
Moreover, the proof in \autoref{app:sgd-convex} is simpler and far shorter. 

\begin{restatable}{theorem}{wgcConvex}~\label{thm:wgc-convex}
    Let \( f \) be a convex, \( L \)-smooth function and \oracle{} a \ac{SFO} such that \( \rbr{f, \oracle{}} \) satisfies the weak growth condition with parameter \( \wgc \).
    Then stochastic gradient descent with fixed step-size \( \eta < \frac{1}{\wgc L} \) converges as
    \[ \E\sbr{\f(\bar \w_K)} - f(\wopt) \leq \frac{1}{2 \eta \rbr{1 - \eta \wgc L} \, K} \norm{\w_0 - \wopt}^2, \]
    where \( \bar \w_K = \frac{1}{K} \sum_{k=0}^{K-1} \wk \) and \( \wopt = \Pi_{\calX^*}(\w_0) \). 
\end{restatable}

\noindent In fact, an even larger step-size and faster convergence rate can be obtained when \( \oracle{} \) is individually-smooth.
We show this now.

\begin{restatable}{theorem}{wgcConvexIndSmooth}~\label{thm:wgc-convex-ind-smooth}
    Let \( f \) be a convex, \( L \)-smooth function and \oracle{} a \ac{SFO} such that \( \rbr{f, \oracle{}} \) satisfies the weak growth condition with parameter \( \wgc \).
    Moreover, suppose \oracle{} is \( \Lmax \) individually-smooth. 
    Then stochastic gradient descent with fixed step-size \( \eta < \frac{1}{\wgc L} + \frac{1}{\Lmax} \) converges as
    \[ \E\sbr{f(\bar w_K)} - f(\wopt) \leq \frac{1}{2\eta \, \delta \, K} \norm{\w_0 - \wopt}^2,   \]
    where \( \bar \w_K = \frac{1}{K} \sum_{k=0}^{K-1} \wk \), \( \wopt = \Pi_{\calX^*}(\w_0) \), and \( \delta = \min \cbr{ 1, 1 + \wgc L \rbr{\frac{1}{\Lmax} - \eta }} \).
\end{restatable}
\noindent See \autoref{app:sgd-convex} for proof. \hfill \break

\autoref{thm:wgc-convex-ind-smooth} is tight with the deterministic case in the following sense: if \( f(\cdot, \z) = f \) for each \( z \in \calZ \), then \( \wgc = 1 \), \( \Lmax = L \), and the rate given is comparable to the best known results in the deterministic setting (cf. \citet[Theorem 3.3]{bubeck2015convex}). 
This result also further illustrates the benefits of directly assuming the weak growth condition, since the maximum step-size satisfies 
\( \frac{1}{\wgc L} + \frac{1}{\Lmax} \geq \frac{2}{\Lmax}, \)
where \( \frac{2}{\Lmax} \) is the condition obtained when deriving weak growth directly from individual smoothness and minimizer interpolation.\\

\begin{table}[t]
    \centering
    \begin{tabular}{c l l c c  }\toprule
        \multirow{2}{*}{Assumptions} & \multicolumn{2}{c}{Max. Step-Size}  & \multicolumn{2}{c}{Best Convergence Rate}\\%
        \cmidrule(lr){2-3} \cmidrule(l){4-5}
                 & \multicolumn{1}{c}{Ours} & \multicolumn{1}{c}{VBS} & \multicolumn{1}{c}{Ours} & \multicolumn{1}{c}{VBS}\\ \midrule
    —            & \( \eta < \frac{1}{\wgc L} \)%
                 & \( \eta < \frac{1}{2 \wgc L} \)%
                 & \( O\rbr{\frac{\wgc L }{\epsilon}} \)%
                 & \( O\rbr{\frac{4 \rbr{1 + \wgc} L}{\epsilon} } \)\\ \addlinespace 
    \makecell{Convex + \\ Ind. Smooth}% 
                 & \( \eta \leq \frac{1}{\wgc L} + \frac{1}{\Lmax} \)%
                 & \multicolumn{1}{c}{N/A}% 
                 & \( O\rbr{\frac{\Lmax}{2 \epsilon}} \)%
                 & \multicolumn{1}{c}{N/A} \\ \bottomrule 
    \end{tabular}
    \caption[Comparison of iteration complexities of fixed step-size stochastic gradient descent for convex functions under weak growth. ]%
    {Comparison of iteration complexities of fixed step-size \ac{SGD} for convex \( f \) under the weak growth condition.
    For each result, we report the complexity obtained with the optimal step-size according to that analysis.
    Our results are tighter than VBS~\citep{vaswani2019fast} and allow for larger step-sizes.}%
    \label{table:sgd-comparison}
\end{table}


\section{Almost Sure Convergence}~\label{sec:almost-sure}

Now we briefly change paradigms and consider the asymptotic behavior of \ac{SGD} with a fixed step-size.
Our goal in this section is to show the almost-sure (with probability 1) convergence of \ac{SGD} to a minimizer or stationary point of \( f \) when \( \rbr{f, \oracle{}} \) satisfy weak or strong growth, respectively.
In the latter scenario, this means we want to show the random variable \( \lim_{\iter \rightarrow \infty} \norm{\grad(\wk)}^2 \) exists and is almost-surely zero.
We will need some tools from measure-theoretic probability to accomplish this.

To start, observe that each iterate \( \w_k \) can be written as a deterministic Borel function of the stochastic gradients \( \cbr{\grad(\wk, \zk)}_{\iter=0}^{K} \) by unrolling the \ac{SGD} update.
Formally, we also assume a probability space \( \rbr{\Omega,  \calF, P} \) in the background and say the sequence \( \seq{\wk} \) is \emph{adapted} to the filtration generated by the stochastic gradients,
\begin{align*}
    \calF_K = \sigma \rbr{\bigcup_{\iter = 0}^{K-1} \sigma \grad(\wk, \zk)}.
\end{align*}
The sequences of function and gradient values are functions of \( \seq{\wk} \) and so are also adapted to \( \seq{\calF_\iter} \).
See \citet{ccinlar2011probability} for additional details on filtrations.

Our main tool to show convergence of sequences of random variables will be supermartingale theory~\citep{ccinlar2011probability}.
Supermartingales are one of two classic tools used to analyze the convergence of \ac{SGD}, the other being Lyapunov functions~\citep{bertsekas2000gradient}.
In particular, recent authors have made use of convergence of discrete-time, positive supermartingales~\citep{bertsekas2011incremental, nguyen2018sgd}.
This theorem is due to~\citet{neveu1975discrete} and will be the cornerstone of our analyses; we reproduce it here for convenience.

\begin{theorem}[Convergence of Positive Supermartingales]\label{thm:positive_supermartingales}
    Let \( \seq{Y_\iter} \), \( \seq{X_\iter} \), and \( \seq{A_\iter} \) be discrete, non-negative random processes indexed by \( \iter \in \bbN \) and adapted to the filtration \( \seq{\calF_\iter} \).
    Suppose that
    \begin{align*}
        \forall \iter \in \bbN, \: \E \sbr{Y_{\iter+1} \mid \calF_\iter} \leq Y_\iter - X_\iter + A_\iter,
        \quad \text{ and } \quad 
        \sum_{k=0}^{\infty} A_\iter < \infty,
    \end{align*}
    almost surely.
    Then the sequence \( \seq{Y_\iter} \) converges almost surely to a non-negative random variable \( Y_\infty \) and \( \sum_{k=0}^{\infty} X_\iter < \infty \) almost surely.
\end{theorem}
\noindent With the necessary measure-theoretic background complete, we are now ready to study the almost-sure convergence of stochastic gradient descent.

\begin{restatable}{theorem}{wgcAlmostSure}~\label{thm:wgc-almost-sure}
    Let \( \f \) be a convex, \( L \)-smooth function with at least one finite minimizer and \oracle{} an \( \Lmax \) individually-smooth \ac{SFO} such that \( \rbr{f, \oracle{}} \) satisfies the weak growth condition with parameter \( \wgc \).
Then the sequence \( \seq{f(\wk)} \) generated by stochastic gradient descent with fixed step-size \( \eta < \frac{1}{\wgc L} + \frac{1}{\Lmax} \) converges to the optimal function value \( f(\wopt) \) almost surely.
\end{restatable}

The proof is given in \autoref{app:almost-sure-convergence} and follows a straightforward argument.
First, we establish that the sequence of distances to an arbitrary finite minimizer \( \seq{\norm{\wk - \wopt}^2} \) satisfies the conditions of Theorem~\ref{thm:positive_supermartingales}.
As a by-product, we obtain that the series \( \sum_{k=0}^{\infty} \rbr{f(\wk) - f(\wopt)} \) is convergent (although \( \wk \rightarrow \wopt \) does not necessarily hold almost surely) and then deduce
\[ \lim_{k\rightarrow\infty} f(\wk) \equas f(\wopt). \] 
It is straightforward to prove an alternative version of \autoref{thm:wgc-almost-sure} which does not require individual smoothness.
In this case, convergence is established for \( \eta < \frac{1}{\wgc L} \) using the same progress condition as in \autoref{thm:wgc-convex}, namely \autoref{eq:cwg-alternate-progress}.

\autoref{thm:wgc-almost-sure} holds for the \emph{final} iterate generated by the \ac{SGD} procedure.
This should be contrasted with Theorems~\ref{thm:wgc-convex} and~\ref{thm:wgc-convex-ind-smooth}, which apply only to the averaged iterate \( \bar \wk \).
While the existence of an asymptotic result suggests that non-asymptotic, final-iterate convergence for constant step-size \ac{SGD} under the weak growth condition is possible, we do not establish such a result in this work. 
Convergence (or non-convergence) of the final iterate remains an interesting and surprisingly simple open problem in optimization under interpolation.

The final result of this chapter shows almost-sure convergence to a stationary point for general non-convex functions \( f \) such that \( \rbr{f, \oracle{}} \) satisfy the strong growth condition.
The proof is presented in \autoref{app:almost-sure-convergence} and follows a similar structure to the proof of \autoref{thm:wgc-almost-sure}.

\begin{restatable}{theorem}{sgcAlmostSure}~\label{thm:sgc-almost-sure}
    Let \( \f \) be an \( L \)-smooth function with at least one finite minimizer \( \wopt \) and \oracle{} a SFO such that \( \rbr{f, \oracle{}} \) satisfies the strong growth condition with parameter \( \sgc \).
    Then the sequence of gradient-norms \( \seq{\norm{\grad(\wk)}^2} \) generated by stochastic gradient descent with fixed step-size \(\eta < \frac{2}{\sgc L} \) converges to \( 0 \) almost surely.
\end{restatable}

\section{Conclusions}~\label{sec:sgd-conclusions}

\autoref{thm:sgc-almost-sure} ends our study of fixed step-size \ac{SGD} for strongly-convex and convex minimization under the strong/weak growth conditions.
Building on work by \citet{schmidt2013fast}, we established a faster linear convergence rate for \( \mu \)-strongly-convex functions when using a larger step-size that requires knowledge of \( \mu \);
this result attains the deterministic rate when \( \sgc = 1 \)~\citep{bubeck2015convex}. 
Unfortunately, the subsequent discussion showed that \autoref{thm:sgc-convex} can still be slower then the corresponding result of \citet{vaswani2019fast} when minimizer interpolation holds and the worst-case values of \( \wgc \) and \( \sgc \) are attained. 

Inspired by this discrepancy, we then leveraged additional properties of \oracle{}, such as individual smoothness and convexity, to derive convergence rates that match those of \citet{vaswani2019fast}, but permit a larger range of step-sizes (\autoref{thm:sgc-ind-convex}). 
We also proved that \ac{SGD} converges almost-surely if \oracle{} is individually-strongly-convex (\autoref{thm:sgc-ind-sc}). 
In this case, the optimization problem is degenerate, meaning it can be solved by directly minimizing any \( f(\cdot, \z) \). 
Moreover, the conditioning of the optimization problem can be improved from worst-case to best-case if we have direct access to \( f(\cdot, \z) \) for all \( \z \).

The remainder of the chapter established improved convergence rates for convex functions (Theorems~\ref{thm:wgc-convex} and~\ref{thm:wgc-convex-ind-smooth}).
These theorems improve over existing rates by constant factors.
Individual smoothness of \oracle{} again allowed us to show \ac{SGD} converges with a slightly wider range of larger step-sizes.
Finally, we concluded with asymptotic convergence results for \ac{SGD} applied to convex and non-convex functions (Theorems~\ref{thm:wgc-almost-sure} and~\ref{thm:sgc-almost-sure}).



\endinput

