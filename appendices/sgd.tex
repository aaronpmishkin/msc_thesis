%! TEX root = ../main.tex

\section{SGD: Proofs}~\label{app:sgd}

\subsection{Convergence for Strongly Convex Functions}~\label{app:sgd-sc}

\sgcConvex*
\begin{proof}
    \begin{align*}
        \norm{\wkk - \wopt}^2 &= \norm{(\wkk - \wk) + (\wk - \wopt)}^2\\
                             &= \norm{\wkk - \wk}^2 + 2 \abr{\wkk - \wk, \wk - \wopt} + \norm{\wk - \wopt}^2\\
                             &= \eta^2 \norm{\grad(\wk, \zk)}^2 - 2 \eta \abr{\grad(\wk, \zk), \wk - \wopt} + \norm{\wk - \wopt}^2.
                             \intertext{Taking expectations with respect to \( \zk \), }
       \E \sbr{\norm{\wkk - \wopt}^2} &= \eta^2 \E \sbr{\norm{\grad(\wk, \zk)}^2} - 2 \eta \E \sbr{\abr{\grad(\wk, \zk), \wk - \wopt}} + \norm{\wk - \wopt}^2\\
                                      &= \eta^2 \E \sbr{\norm{\grad(\wk, \zk)}^2} - 2 \eta \abr{\grad(\wk), \wk - \wopt} + \norm{\wk - \wopt}^2.
                                      \intertext{Now we use the strong growth condition to control \( \E \sbr{\norm{\grad(\wk, \zk)}^2} \), which yields}
       \E\sbr{\norm{\wkk - \wk}^2} &\leq \eta^2 \rho \norm{\grad(\wk)}^2 - 2 \eta \abr{\grad(\wk), \wk - \wopt} + \norm{\wk - \wopt}^2.
                                      \intertext{Coercivity of the gradient (\autoref{lemma:coercivity}) implies}
       \E\sbr{\norm{\wkk - \wk}^2} &\leq \eta^2 \rho \norm{\grad(\wk)}^2 - 2 \eta \rbr{\frac{\mu L }{\mu + L} \norm{\wk - \wopt}^2 + \frac{1}{\mu + L}\norm{\grad(\wk)}^2} + \norm{\wk - \wopt}^2\\
                                   &= \eta \rbr{\eta \rho  - \frac{2}{\mu + L}}\norm{\grad(\wk)}^2 + \rbr{1 - \frac{2 \eta \mu L}{\mu + L}}\norm{\wk - \wopt}^2.
                                   \intertext{If \( \eta \leq \frac{2}{\rho \rbr{\mu + L}} \) then \( \eta \rho - \frac{2}{\mu + L} \leq 0 \) and we obtain}
       \E \sbr{\norm{\wkk - \wopt}^2} &\leq \rbr{1 - \frac{2 \eta \mu L}{\mu + L}}\norm{\wk - \wopt}^2.
       \intertext{Taking expectations and recursing on this inequality,}
       \implies \E \sbr{\norm{\wkk - \wopt}^2} &\leq \rbr{1 - \frac{2 \eta \mu L}{\mu + L}}^k \norm{\w_0 - \wopt}^2.
       \intertext{Application of \autoref{lemma:iterate-bounds} completes the proof:}
       \implies \frac{2}{L} \rbr{\E \sbr{f(\wkk)} - f(\wopt)} &\leq \frac{2}{\mu} \rbr{1 - \frac{2 \eta \mu L}{\mu + L}}^k \rbr{f(\w_0) - f(\wopt)}\\
       \implies \E \sbr{f(\wkk)} - f(\wopt) &\leq \frac{L}{\mu} \rbr{1 - \frac{2 \eta \mu L}{\mu + L}}^k \rbr{f(\w_0) - f(\wopt)}.
    \end{align*}
\end{proof}

\sgcIndSC*
\begin{proof}
    \begin{align*}
        \norm{\wkk - \wopt}^2 &= \norm{(\wkk - \wk) + (\wk - \wopt)}^2\\
                             &= \norm{\wkk - \wk}^2 + 2 \abr{\wkk - \wk, \wk - \wopt} + \norm{\wk - \wopt}^2\\
                             &= \eta^2 \norm{\grad(\wk, \zk)}^2 - 2 \eta \abr{\grad(\wk, \zk), \wk - \wopt} + \norm{\wk - \wopt}^2.
                                     \intertext{Coercivity of the stochastic gradient (\autoref{lemma:coercivity}) implies}
        \norm{\wkk - \wk}^2 &\leq \eta^2 \norm{\grad(\wk, \zk)}^2 - 2 \eta \rbr{\frac{\mu_\z L_\z }{\mu_\z + L_\z} \norm{\wk - \wopt}^2 + \frac{1}{\mu_\z + L_\z}\norm{\grad(\wk, \zk)}^2} \\ & \hspace{2cm} + \norm{\wk - \wopt}^2\\
                            &\leq \eta^2 \norm{\grad(\wk, \zk)}^2 - 2 \eta \rbr{\frac{\mumin \Lmin }{\mumax + \Lmax} \norm{\wk - \wopt}^2 + \frac{1}{\mumax + \Lmax}\norm{\grad(\wk, \zk)}^2} \\ & \hspace{2cm} + \norm{\wk - \wopt}^2\\
                                   &= \eta \rbr{\eta - \frac{2}{\mumax + \Lmax}}\norm{\grad(\wk)}^2 + \rbr{1 - \frac{2 \eta \mumin \Lmin}{\mumin + \Lmax}}\norm{\wk - \wopt}^2.
                                   \intertext{If \( \eta \leq \frac{2}{\rbr{\mumax + \Lmax}} \) then \( \eta - \frac{2}{\mumax + \Lmax} \leq 0 \) and we obtain}
       \E \sbr{\norm{\wkk - \wopt}^2} &\leq \rbr{1 - \frac{2 \eta \mumin \Lmin}{\mumax + \Lmax}}\norm{\wk - \wopt}^2.
       \intertext{Taking expectations and recursing on this inequality,}
       \implies E\sbr{\norm{\wkk - \wopt}^2} &\leq \rbr{1 - \frac{2 \eta \mumin \Lmin}{\mumax + \Lmax}}^k \norm{\w_0 - \wopt}^2.
       \intertext{Application of \autoref{lemma:iterate-bounds} completes the proof:}
       \implies \frac{2}{L} \rbr{\E\sbr{f(\wkk)} - f(\wopt)} &\leq \frac{2}{\mu} \rbr{1 - \frac{2 \eta \mumin \Lmin}{\mumax + \Lmax}}^k \rbr{f(\w_0) - f(\wopt)}\\
       \implies \E\sbr{f(\wkk)} - f(\wopt) &\leq \frac{L}{\mu} \rbr{1 - \frac{2 \eta \mumin \Lmin}{\mumax + \Lmax}}^k \rbr{f(\w_0) - f(\wopt)}.
    \end{align*}
\end{proof}



\subsection{Convergence for Convex Functions}~\label{app:sgd-convex}

\wgcConvex*
\begin{proof}
    \begin{align*}
       \norm{\wkk - \wopt}^2 &= \norm{(\wkk - \wk) + (\wk - \wopt)}^2\\
                             &= \norm{\wkk - \wk}^2 + 2 \abr{\wkk - \wk, \wk - \wopt} + \norm{\wk - \wopt}^2\\
                             &= \eta^2 \norm{\grad(\wk, \zk)}^2 - 2 \eta \abr{\grad(\wk, \zk), \wk - \wopt} + \norm{\wk - \wopt}^2.
                             \intertext{Taking expectations with respect to \( \zk \), }
       \E \sbr{\norm{\wkk - \wopt}^2} &= \eta^2 \E \sbr{\norm{\grad(\wk, \zk)}^2} - 2 \eta \E \sbr{\abr{\grad(\wk, \zk), \wk - \wopt}} + \norm{\wk - \wopt}^2\\
                                      &= \eta^2 \E \sbr{\norm{\grad(\wk, \zk)}^2} - 2 \eta \abr{\grad(\wk), \wk - \wopt} + \norm{\wk - \wopt}^2.
                             \intertext{By convexity of \( f \) and the weak growth condition,}
       \E \sbr{\norm{\wkk - \wopt}^2} &\leq \eta^2 \norm{\grad(\wk, \zk)}^2 - 2 \eta \rbr{f(\wk) - f(\wopt)} + \norm{\wk - \wopt}^2\\
                             &\leq 2 \eta^2 \rho L \rbr{f(\wk) - f(\wopt)} - 2 \eta \rbr{f(\wk) - f(\wopt)} + \norm{\wk - \wopt}^2\\
                             &= - 2 \eta \rbr{1 - \eta \rho L}\rbr{f(\wk) - f(\wopt)} + \norm{\wk - \wopt}^2.
   \end{align*}
   Rearranging the expression to put the optimality gap on the left-hand side, 
   \begin{align*}
       2 \eta \rbr{1 - \eta \rho L} \rbr{f(\wk) - f(\wopt)} &\leq \norm{\wk - \wopt}^2 - \E \sbr{\norm{\wkk - \wopt}^2}.
       \intertext{If \( \eta < \frac{1}{\rho L} \) then \( 1 - \eta \rho L > 0 \). We obtain }
       f(\wk) - f(\wopt) &\leq \frac{1}{2 \eta \rbr{1 - \eta \rho L}} \rbr{\norm{\wk - \wopt}^2 - \E\sbr{\norm{\wkk - \wopt}^2}}.
       \intertext{Taking expectations and summing from \( k = 0 \) to \( K - 1 \) now gives}
   \frac{1}{K} \sum_{k=0}^{K-1} \E \sbr{f(\wk)} - f(\wopt) &\leq \frac{1}{2 \eta \rbr{1 - \eta \rho L} \, K} \sum_{k=0}^{K-1} \rbr{\E \sbr{\norm{\wk - \wopt}^2} - \E \sbr{\norm{\wkk - \wopt}^2}}\\
                                                           &\leq \frac{1}{2 \eta \rbr{1 - \eta \rho L} \, K} \rbr{\norm{\w_0 - \wopt}^2 - \norm{\w_K - \wopt}^2}\\
                                                           &\leq \frac{1}{2 \eta \rbr{1 - \eta \rho L} \, K} \norm{\w_0 - \wopt}^2.
                                                           \intertext{Noting \( \frac{1}{K} \sum_{k=0}^{K-1} f(\wk) \geq f(\bar \w_K) \) by convexity leads to the final result,}
   \E\sbr{f(\bar \w_K)} - f(\wopt) &\leq \frac{1}{2 \eta \rbr{1 - \eta \rho L} \, K} \norm{\w_0 - \wopt}^2.
   \end{align*}
\end{proof}


\wgcConvexIndSmooth*

\begin{proof}
   Starting from \autoref{lemma:convex-intermediate},
   \begin{align*}
    f(\wk) - f(\wopt) &\leq \frac{1}{2\eta \, C} \rbr{\norm{\wk - \wopt}^2 - \E \sbr{\norm{\wkk - \wopt}^2}}\\
    \intertext{Taking expectations and summing from \( k = 0 \) to \( K - 1 \) now gives}
    \implies \frac{1}{K} \sum_{k=0}^{K -1} \E \sbr{f(\wk) - f(\wopt)} &\leq \frac{1}{2\eta \, C \, K}\sum_{k=0}^{K-1}\rbr{\E\sbr{\norm{\wk - \wopt}^2} - \E\sbr{\norm{\wkk - \wopt}^2}}\\
                                                         &= \frac{1}{2\eta \, C \, K}\rbr{\norm{\w_0 - \wopt}^2 - \E\sbr{\norm{\w_{K} - \wopt}^2}}\\
                                                         &\leq \frac{1}{2\eta \, C \, K} \norm{\w_0 - \wopt}^2\\
\intertext{Noting \( \frac{1}{K}\sum_{k=0}^{K-1} f(\wk) \geq f(\bar \w_K) \) by convexity leads to the final result,}
   \implies \E\sbr{f(\bar w_K)} - f(\wopt) &\leq \frac{1}{2\eta \, C \, K}\norm{\w_0 - \wopt}^2.
\end{align*}
\end{proof}

\todo{Can we prove a final iterate version of this using the strategy in Bubeck's ConvexOpt? If not, can we prove that SGD is not guaranteed to make progress at every iteration even when WGC holds?}

\subsection{Almost Sure Convergence}~\label{app:almost-sure-convergece}

\wgcAlmostSure*
\begin{proof}
    We proceed by showing that the sequence of distances to the minimizer \( \seq{\norm{\wk - \wopt}^2} \) satisfies the conditions of Theorem~\ref{thm:positive_supermartingales}.
    Moreover, as by-product, we obtain that the series \( \sum_{k=0}^{\infty} \rbr{f(\wk) - f(\wopt)} \) is convergent and thus \( f(\wk) \rightarrow \fopt \).

    Lemma~\ref{lemma:convex-intermediate} gives the decrease condition
    \begin{align*}
        \E \sbr{\norm{\wkk - \wopt}^2 \mid \calF_\iter} \leq  \norm{\wk - \wopt}^2 - 2 \eta C \rbr{f(\wk) - f(\wopt)},
    \end{align*}
    where \( C \geq 1 \) since \( \eta < \frac{1}{\rho L} + \frac{1}{\Lmax} \)
    The conditions of Theorem~\ref{thm:positive_supermartingales} are satisfied with \( A_\iter = 0 \) for all \( \iter \).
    So, the sequence \( \seq{\norm{\wk - \wopt}^2} \) converges to a non-negative random variable \(\lim_{\iter \rightarrow \infty} \norm{\wk - \wopt}^2 \) almost surely.
    We also have
    \begin{align*}
        \sum_{k=0}^{\infty} 2\eta C \sbr{f(\wk) - f(\wopt)} < \infty
        \implies \sum_{k=0}^{\infty} \sbr{f(\wk) - f(\wopt)} < \infty,
    \end{align*}
    almost surely.
    Therefore,
    \[ \lim_{\iter \rightarrow \infty} f(\wk) - f(\wopt) \equas 0, \]
    and the proof is complete.
\end{proof}

\sgcAlmostSure*
\begin{proof}
    Consider the stochastic process defined by the optimality gaps \( \seq{f(\wk) - f(\wopt)} \).
    We show that this sequence satisfies the conditions of Theorem~\ref{thm:positive_supermartingales} and, as a by-product, establish almost sure convergence of the final iterate to a stationary point.

    Lemma~\ref{lemma:sgc-decrease-condition} gives the decrease condition
    \begin{align*}
        \E \sbr{f(\wkk) - f(\wopt) \mid \calF_\iter} &\leq \rbr{f(\wk) - f(\wopt)} - \eta \rbr{1 - \frac{\rho L \eta}{2}}\norm{\grad(\wk)}^2.
    \end{align*}
    Since \( \eta < \frac{2}{\rho L} \),
    \begin{align*}
        \eta \rbr{1 - \frac{\eta \rho L}{2}}\norm{\grad(\wk)}^2 > 0,
    \end{align*}
    and the conditions of Theorem~\ref{thm:positive_supermartingales} are satisfied with \( A_\iter = 0 \) for all \( \iter \).
    The sequence \( \seq{f(\wk) - f(\wopt)} \) converges almost surely to a non-negative random variable \(\lim_{\iter \rightarrow \infty} f(\wk) - f(\wopt) \) almost surely.
    Of more interest is that
    \begin{align*}
        \sum_{k=0}^{\infty} \eta \rbr{1 - \frac{\eta \rho L}{2}} \norm{\grad(\wk)}^2 < \infty
        \implies \sum_{k=0}^{\infty} \norm{\grad(\wk)}^2 < \infty,
    \end{align*}
    almost surely.
    Accordingly, the sequence of gradient norms satisfies
    \[ \lim_{\iter \rightarrow \infty} \norm{\grad(\wk)}^2 \equas 0, \]
    and we conclude that the final iterate of stochastic gradient descent converges almost surely to a stationary point.
\end{proof}











%% Old Proof with WGC

\iffalse
\begin{proof}
   \begin{align*}
       \norm{\wkk - \wopt}^2 &= \norm{(\wkk - \wk) + (\wk - \wopt)}^2\\
                             &= \norm{\wkk - \wk}^2 + 2 \abr{\wkk - \wk, \wk - \wopt} + \norm{\wk - \wopt}^2\\
                             &= \eta^2 \norm{\grad(\wk, \zk)}^2 - 2 \eta \abr{\grad(\wk, \zk), \wk - \wopt} + \norm{\wk - \wopt}^2\\ 
                             \intertext{The weak growth condition implies \( \grad(\wopt, \z) = 0 \) for all \( \z \). We may thus use co-coercivity of the gradient (\autoref{lemma:co-coercivity}) at \( \wk \) and \( \wopt \) to obtain}
                             &\leq \eta^2 \norm{\grad(\wk, \zk)}^2 + 2 \eta \rbr{f(\wopt, \zk) - f(\wk, \zk) - \frac{1}{2L} \norm{\grad(\wk, \zk)}^2}  + \norm{\wk - \wopt}^2\\ 
                             &\leq \rbr{\eta^2 - \frac{\eta}{L}} \norm{\grad(\wk, \zk)}^2 + 2 \eta \rbr{f(\wopt, \zk) - f(\wk, \zk)} + \norm{\wk - \wopt}^2\\
                             \intertext{Taking expectations with respect to \( \zk \):}
                             &\leq \rbr{\eta^2 - \frac{\eta}{L}} \E \sbr{\norm{\grad(\wk, \zk)}^2} + 2 \eta \E\sbr{f(\wopt, \zk) - f(\wk, \zk)} + \norm{\wk - \wopt}^2\\
                             &\leq \rbr{\eta^2 - \frac{\eta}{L}} \E \sbr{\norm{\grad(\wk, \zk)}^2} + 2 \eta \rbr{f(\wopt) - f(\wk)} + \norm{\wk - \wopt}^2\\
                             \intertext{If \( \eta \leq \frac{1}{L} \) then \( \eta^2 - \frac{\eta}{L} \geq 0 \) and we apply the weak growth condition as follows: }
                             &\leq 2 \rho \rbr{\eta^2- \frac{\eta}{L}}\rbr{\f(\wk) - f(\wopt)} + 2 \eta\rbr{f(\wopt) - f(\wk)} + \norm{\wk - \wopt}^2\\ 
                             &= 2\eta \rho \rbr{\eta - \frac{1}{L} - \frac{1}{\rho}}\rbr{f(\wk) - f(\wopt)} + \norm{\wk - \wopt}^2
\end{align*}
Rearranging the expression to put the optimality gap on the left-hand side,
\begin{align*}
    2 \eta \rho \rbr{\frac{1}{L} + \frac{1}{\rho} - \eta}\rbr{f(\wk) - f(\wopt)} &\leq \norm{\wk - \wopt}^2 - \E\sbr{\norm{\wkk - \wopt}^2}\\ 
\intertext{Noting that \( \frac{1}{L} + \frac{1}{\rho} - \eta \geq 0 \) since \( \eta \leq \frac{1}{L} \), }
\implies f(\wk) - f(\wopt) &\leq \rbr{\frac{L + \rho - \eta \rho L}{2\eta\rho^2L}} \rbr{\norm{\wk - \wopt}^2 - \norm{\wkk - \wopt}^2}\\
\intertext{Taking expectations and summing over iterations,}
\implies \sum_{k=0}^{K -1} \E \sbr{f(\wk) - f(\wopt)} &\leq \sum_{k=0}^{K-1}\rbr{\frac{L + \rho - \eta \rho L}{2\eta\rho^2L}} \rbr{\E\sbr{\norm{\wk - \wopt}^2} - \E\sbr{\norm{\wkk - \wopt}^2}}\\
                                                      &\leq \rbr{\frac{L + \rho - \eta \rho L}{2\eta\rho^2L}}\rbr{\norm{\w_0 - \wopt}^2 - \E\sbr{\norm{\w_{K} - \wopt}^2}}\\
                                                      &\leq \rbr{\frac{L + \rho - \eta \rho L}{2\eta\rho^2L}}\norm{\w_0 - \wopt}^2\\
\implies \frac{1}{K} \sum_{k=0}^{K-1} \E\sbr{f(\wk)} - f(\wopt) &\leq \rbr{\frac{L + \rho - \eta \rho L}{2K\eta\rho^2L}}\norm{\w_0 - \wopt}^2.
\intertext{Noting \( \frac{1}{K}\sum_{k=0}^{K-1} f(\wk) \geq f(\bar \wk) \) by convexity gives the final result,}
\implies \E\sbr{f(\bar w_K)} - f(\wopt) &\leq \rbr{\frac{L + \rho -\eta \rho L}{2K\eta \rho^2 L}}\norm{\w_0 - \wopt}^2.
\end{align*}
\end{proof}
\fi

