%! TEX root = ../main.tex

\chapter{Stochastic Gradient Descent: Proofs}~\label{app:sgd}

\section{Convergence for Strongly Convex Functions}~\label{app:sgd-sc}

\begin{restatable}{lemma}{sgcDecreaseCondition}~\label{lemma:sgc-decrease-condition}
    Let \( f \) be an \( L \)-smooth function satisfying the strong growth condition with parameter \( \sgc \).
    Then stochastic gradient descent with fixed step-size \( \eta \) satisfies the following expected decrease condition:
    \[ \Ek \sbr{f(\wkk)} \leq f(\wk) - \eta \rbr{1 - \frac{\sgc L \eta}{2}}\norm{\grad(\wk)}^2. \]
\end{restatable}

\begin{proof}
    Starting from \( L \)-smoothness of \( f \),
    \begin{align*}
        f(\wkk) &\leq f(\wk) + \abr{\grad(\wk), \wkk - \wk} + \frac{L}{2}\norm{\wkk - \wk}^2\\
        &= f(\wk) - \eta \abr{\grad(\wk), \grad(\wk, \zk)} + \frac{L \eta^2}{2}\norm{\grad(\wk, \zk)}^2\\
        \intertext{Taking expectations with respect to \( \zk \):}
        \implies \Ek[f(\wkk)] &\leq f(\wk) - \eta \abr{\Ek\sbr{\grad(\wk, \zk)}, \grad(\wk)} + \frac{L \eta^2}{2}\Ek \sbr{\norm{\grad(\wk, \zk)}^2}\\
        &= f(\wk) - \eta \norm{\grad(\wk)}^2 + \frac{L \eta^2}{2}\Ek \sbr{\norm{\grad(\wk, \zk)}^2}.
        \intertext{Using the strong growth condition,}
        \implies \Ek\sbr{f(\wkk) - f(\wk)} &\leq f(\wk) - \eta \norm{\grad(\wk)}^2 + \frac{\sgc L \eta^2}{2} \norm{\grad(\wk)}^2\\
        &\leq f(\wk) - \eta \rbr{1 - \frac{\sgc L \eta}{2}}\norm{\grad(\wk)}^2.
    \end{align*}
\end{proof}

\newpage

\sgcConvex*
\begin{proof}
    \begin{align*}
        \norm{\wkk - \wopt}^2 &= \norm{\wk - \eta \grad(\wk, \zk) + - \wopt}^2\\
                             &= \eta^2 \norm{\grad(\wk, \zk)}^2 - 2 \eta \abr{\grad(\wk, \zk), \wk - \wopt} + \norm{\wk - \wopt}^2.
                             \intertext{Taking expectations with respect to \( \zk \), }
       \implies \Ek \sbr{\norm{\wkk - \wopt}^2} &= \eta^2 \Ek \sbr{\norm{\grad(\wk, \zk)}^2} - 2 \eta \Ek \sbr{\abr{\grad(\wk, \zk), \wk - \wopt}} + \norm{\wk - \wopt}^2\\
                                      &= \eta^2 \Ek \sbr{\norm{\grad(\wk, \zk)}^2} - 2 \eta \abr{\grad(\wk), \wk - \wopt} + \norm{\wk - \wopt}^2.
                                      \intertext{Now we use the strong growth condition to control \( \Ek \sbr{\norm{\grad(\wk, \zk)}^2} \), which yields}
       \Ek\sbr{\norm{\wkk - \wopt}^2} &\leq \eta^2 \sgc \norm{\grad(\wk)}^2 - 2 \eta \abr{\grad(\wk), \wk - \wopt} + \norm{\wk - \wopt}^2.
                                      \intertext{Coercivity of the gradient (\autoref{lemma:coercivity}) implies}
       \Ek\sbr{\norm{\wkk - \wopt}^2} &\leq \eta^2 \sgc \norm{\grad(\wk)}^2 - 2 \eta \rbr{\frac{\mu L }{\mu + L} \norm{\wk - \wopt}^2 + \frac{1}{\mu + L}\norm{\grad(\wk)}^2} + \norm{\wk - \wopt}^2\\
                                   &= \eta \rbr{\eta \sgc  - \frac{2}{\mu + L}}\norm{\grad(\wk)}^2 + \rbr{1 - \frac{2 \eta \mu L}{\mu + L}}\norm{\wk - \wopt}^2.
                                   \intertext{If \( \eta \leq \frac{2}{\sgc \rbr{\mu + L}} \) then \( \eta \sgc - \frac{2}{\mu + L} \leq 0 \) and we obtain}
       \Ek \sbr{\norm{\wkk - \wopt}^2} &\leq \rbr{1 - \frac{2 \eta \mu L}{\mu + L}}\norm{\wk - \wopt}^2.
       \intertext{Taking expectations and recursing on this inequality,}
       \implies \E \sbr{\norm{\wkk - \wopt}^2} &\leq \rbr{1 - \frac{2 \eta \mu L}{\mu + L}}^k \norm{\w_0 - \wopt}^2.
    \end{align*}
\end{proof}

\newpage

\sgcIndConvex*
\begin{proof}
   \begin{align*}
       \norm{\wkk - \wopt}^2 &= \norm{(\wkk - \wk) + (\wk - \wopt)}^2\\
                             &= \norm{\wkk - \wk}^2 + 2 \abr{\wkk - \wk, \wk - \wopt} + \norm{\wk - \wopt}^2\\
                             &= \eta^2 \norm{\grad(\wk, \zk)}^2 - 2 \eta \abr{\grad(\wk, \zk), \wk - \wopt} + \norm{\wk - \wopt}^2.
                             \intertext{Recall \( f(\cdot, \zk) \) is convex, \( \Lmax \)-smooth, and \( \grad(\wopt, \zk) = 0 \) by interpolation. \autoref{lemma:co-coercivity} yields }
       \norm{\wkk - \wopt}^2 &\leq \eta^2 \Lmax \abr{\grad(\wk, \zk), \wk - \wopt} - 2 \eta \abr{\grad(\wk, \zk), \wk - \wopt} + \norm{\wk - \wopt}^2\\
       \norm{\wkk - \wopt}^2 &= - \eta \rbr{2 - \eta \Lmax} \abr{\grad(\wk, \zk), \wk - \wopt} + \norm{\wk - \wopt}^2.
       \intertext{Taking expectations with respect to \( \zk \): }
       \implies \Ek\sbr{\norm{\wkk - \wopt}^2} &\leq - \eta \rbr{2 - \eta \Lmax} \Ek\sbr{\abr{\grad(\wk, zk), \wk - \wopt}} + \norm{\wk - \wopt}^2\\
                                               &= - \eta \rbr{2 - \eta \Lmax} \abr{\grad(\wk), \wk - \wopt} + \norm{\wk - \wopt}^2.
                                               \intertext{If \( \eta \leq \frac{2}{\Lmax} \), then \( 2 - \eta \Lmax \geq 0 \) and \autoref{lemma:inner-to-iterates} implies }
       \Ek \sbr{\norm{\wkk - \wopt}^2} &\leq  - \mu \, \eta \rbr{2 - \eta \Lmax} \norm{\wk - \wopt}^2 + \norm{\wk - \wopt}^2\\
                                       &= \rbr{1 - \mu \, \eta \rbr{2 - \eta \Lmax}}\norm{\wk - \wopt}^2.
                             \intertext{Taking expectations and recursing on this inequality yields the final result,}
       \implies \E\sbr{\norm{\wkk - \wopt}}^2 &\leq \rbr{1 - \mu \, \eta \rbr{2 - \eta \Lmax}}^k \norm{\w_0 - \wopt}^2.
   \end{align*}
\end{proof}

\newpage

\sgcIndSC*
\begin{proof}
    \begin{align*}
        \norm{\wkk - \wopt}^2 &= \norm{\wk - \eta \grad(\wk, \zk) - \wopt}^2\\
                             &= \eta^2 \norm{\grad(\wk, \zk)}^2 - 2 \eta \abr{\grad(\wk, \zk), \wk - \wopt} + \norm{\wk - \wopt}^2.
                            \intertext{Coercivity of the stochastic gradient (\autoref{lemma:coercivity}) implies}
        \norm{\wkk - \wopt}^2 &\leq \eta^2 \norm{\grad(\wk, \zk)}^2 - 2 \eta \rbr{\frac{\mu_\z L_\z }{\mu_\z + L_\z} \norm{\wk - \wopt}^2 + \frac{1}{\mu_\z + L_\z}\norm{\grad(\wk, \zk)}^2} \\ & \hspace{2cm} + \norm{\wk - \wopt}^2\\
        \intertext{Let \( \delta_{\text{min}} = \min_{z \in \calZ} \frac{\mu_z L_z}{\mu_z + L_\z} \). Then we have} 
        \norm{\wkk - \wopt}^2 &\leq \eta^2 \norm{\grad(\wk, \zk)}^2 - 2 \eta \rbr{\delta_{\text{min}} \, \norm{\wk - \wopt}^2 + \frac{1}{\mumax + \Lmax}\norm{\grad(\wk, \zk)}^2} \\ & \hspace{2cm} + \norm{\wk - \wopt}^2\\
                              &= \eta \rbr{\eta - \frac{2}{\mumax + \Lmax}}\norm{\grad(\wk, \zk)}^2 + \rbr{1 - 2 \eta \, \delta_{\text{min}}}\norm{\wk - \wopt}^2.
                                   \intertext{If \( \eta \leq \frac{2}{\rbr{\mumax + \Lmax}} \) then \( \eta - \frac{2}{\mumax + \Lmax} \leq 0 \) and we obtain}
       \norm{\wkk - \wopt}^2 &\leq \rbr{1 - 2 \eta \, \delta_{\text{min}}}\norm{\wk - \wopt}^2.
       \intertext{Recursing on this inequality,}
       \implies \norm{\wkk - \wopt}^2 &\leq \rbr{1 - 2 \eta \, \delta_{\text{min}}}^k \norm{\w_0 - \wopt}^2.
    \end{align*}
\end{proof}

\newpage

\section{Convergence for Convex Functions}~\label{app:sgd-convex}

\begin{restatable}{lemma}{convexIntermediate}~\label{lemma:convex-intermediate}
    Let \( f \) be a convex, \( L \)-smooth function and \oracle{} an \( \Lmax \) individually-smooth \ac{SFO} such that \( \rbr{f, \oracle{}} \) satisfies minimizer interpolation. 
    Then stochastic gradient descent with step-size \( \eta \) satisfies the following inequality: 
    \[ f(\wk) - f(\wopt) \leq \frac{1}{2\eta \, \delta} \rbr{\norm{\wk - \wopt}^2 - \Ek \sbr{\norm{\wkk - \wopt}^2}}, \]
    where \(  \delta = \min \cbr{ 1, 1 + \wgc L \rbr{\frac{1}{\Lmax} - \eta }} \). 
    Furthermore, if \( \eta \leq \frac{1}{\Lmax} \), then 
    \[ f(\wk) - f(\wopt) \leq \frac{1}{2 \eta}\rbr{\norm{\wk - \wopt}^2 - \Ek\sbr{\norm{\wkk - \wopt}^2}}. \]
\end{restatable}

\begin{proof}
    \begin{align*}
       \norm{\wkk - \wopt}^2 &= \norm{\wk - \etak \grad(\wk, \zk) + - \wopt}^2\\
                             &= \eta^2 \norm{\grad(\wk, \zk)}^2 - 2 \eta \abr{\grad(\wk, \zk), \wk - \wopt} + \norm{\wk - \wopt}^2. 
                             \intertext{The weak growth condition implies \( \grad(\wopt, \z) = 0 \) for all \( \z \in \calZ \). We may thus use \autoref{lemma:pre-coercivity} at \( \wk \) and \( \wopt \) to obtain}
                       \norm{\wkk - \wopt}^2 &\leq \eta^2 \norm{\grad(\wk, \zk)}^2 - 2 \eta \rbr{f(\wk, \zk) - f(\wopt, \zk) + \frac{1}{2\Lmax} \norm{\grad(\wk, \zk)}^2} \\ 
                                             & \hspace{5em} + \norm{\wk - \wopt}^2\\
                         &\leq \rbr{\eta^2 - \frac{\eta}{\Lmax}} \norm{\grad(\wk, \zk)}^2 - 2 \eta \rbr{f(\wk, \zk) - f(\wopt, \zk)} + \norm{\wk - \wopt}^2.
     \intertext{Taking expectations with respect to \( \zk \):}
                      \Ek \sbr{\norm{\wkk - \wopt}^2} &\leq \rbr{\eta^2 - \frac{\eta}{\Lmax}} \Ek \sbr{\norm{\grad(\wk, \zk)}^2} - 2 \eta \Ek\sbr{f(\wk, \zk) - f(\wopt, \zk)} + \norm{\wk - \wopt}^2,\\
                         &\leq \rbr{\eta^2 - \frac{\eta}{\Lmax}} \Ek \sbr{\norm{\grad(\wk, \zk)}^2} - 2 \eta \rbr{f(\wk) - f(\wopt)} + \norm{\wk - \wopt}^2.
                     \intertext{\textbf{Case 1}: If \( \eta \leq \frac{1}{\Lmax} \) then \( \rbr{\eta^2 - \frac{\eta}{\Lmax}}\rbr{f(\wk, \zk) - f(\wopt, \zk)} \leq 0 \) by interpolation and we obtain }
                     \Ek \sbr{\norm{\wkk - \wopt}^2} &\leq - 2 \eta\rbr{f(\wk) - f(\wopt)} + \norm{\wk - \wopt}^2\\
                     \implies f(\wk) - f(\wopt) &\leq \frac{1}{2\eta} \sbr{\norm{\wk - \wopt}^2 - \Ek \norm{\wkk - \wopt}^2}. 
                     \intertext{\textbf{Case 2}: If \( \eta > \frac{1}{\Lmax} \) then \( \eta^2 - \frac{\eta}{\Lmax} > 0 \) and the weak growth condition implies}
                      \Ek \sbr{\norm{\wkk - \wopt}^2} &\leq 2 \eta \wgc L \rbr{\eta - \frac{1}{\Lmax}}\rbr{\f(\wk) - f(\wopt)} - 2 \eta\rbr{f(\wk) - f(\wopt)} + \norm{\wk - \wopt}^2\\ 
                                           &= -2 \eta \rbr{1 + \wgc L \rbr{\frac{1}{\Lmax} - \eta }}\rbr{f(\wk) - f(\wopt)} + \norm{\wk - \wopt}^2.
                     \intertext{If \( \eta < \inv{\wgc L} + \inv{\Lmax} \), then \( 1 + \wgc L \rbr{\frac{1}{\Lmax} - \eta } > 0 \), }
                      \implies f(\wk) - f(\wopt) &\leq \frac{1}{2 \eta \rbr{1 + \wgc L \rbr{\frac{1}{\Lmax} - \eta}}} \rbr{\norm{\wk - \wopt}^2 - \Ek\sbr{\norm{\wkk - \wopt}^2}}. 
          \intertext{Let us combine the cases by taking the worst-case bound. 
                     Let \( \delta = \min \cbr{ 1,1 + \wgc L \rbr{\frac{1}{\Lmax} - \eta }} \) to obtain: }
    f(\wk) - f(\wopt) &\leq \frac{1}{2\eta \, \delta} \rbr{\norm{\wk - \wopt}^2 - \Ek \sbr{\norm{\wkk - \wopt}^2}}.
\end{align*}
Note that this bound is tight with Case 1 since \( 1 + \wgc L \rbr{\inv{\Lmax} - \eta} \geq 1\) when \( \eta \leq \inv{\Lmax} \). 
\end{proof}



\wgcConvex*
\begin{proof}
    \begin{align*}
        \norm{\wkk - \wopt}^2 &= \norm{\wk - \eta \grad(\wk, \zk) - \wopt}^2\\
                             &= \eta^2 \norm{\grad(\wk, \zk)}^2 - 2 \eta \abr{\grad(\wk, \zk), \wk - \wopt} + \norm{\wk - \wopt}^2.
                             \intertext{Taking expectations with respect to \( \zk \), }
       \Ek \sbr{\norm{\wkk - \wopt}^2} &= \eta^2 \Ek \sbr{\norm{\grad(\wk, \zk)}^2} - 2 \eta \Ek \sbr{\abr{\grad(\wk, \zk), \wk - \wopt}} + \norm{\wk - \wopt}^2\\
                                      &= \eta^2 \Ek \sbr{\norm{\grad(\wk, \zk)}^2} - 2 \eta \abr{\grad(\wk), \wk - \wopt} + \norm{\wk - \wopt}^2.
                             \intertext{By convexity of \( f \) and the weak growth condition,}
       \Ek \sbr{\norm{\wkk - \wopt}^2} &\leq \eta^2 \Ek \sbr{\norm{\grad(\wk, \zk)}^2} - 2 \eta \rbr{f(\wk) - f(\wopt)} + \norm{\wk - \wopt}^2\\
                             &\leq 2 \eta^2 \wgc L \rbr{f(\wk) - f(\wopt)} - 2 \eta \rbr{f(\wk) - f(\wopt)} + \norm{\wk - \wopt}^2\\
                             &= - 2 \eta \rbr{1 - \eta \wgc L}\rbr{f(\wk) - f(\wopt)} + \norm{\wk - \wopt}^2.\addnumber~\label{eq:cwg-alternate-progress}
   \end{align*}
   Rearranging the expression to put the optimality gap on the left-hand side, 
   \begin{align*}
       2 \eta \rbr{1 - \eta \wgc L} \rbr{f(\wk) - f(\wopt)} &\leq \norm{\wk - \wopt}^2 - \Ek \sbr{\norm{\wkk - \wopt}^2}.
       \intertext{If \( \eta < \frac{1}{\wgc L} \) then \( 1 - \eta \wgc L > 0 \), }
       \implies f(\wk) - f(\wopt) &\leq \frac{1}{2 \eta \rbr{1 - \eta \wgc L}} \rbr{\norm{\wk - \wopt}^2 - \Ek\sbr{\norm{\wkk - \wopt}^2}}.
       \intertext{Taking expectations and summing from \( k = 0 \) to \( K - 1 \) now gives}
   \frac{1}{K} \sum_{k=0}^{K-1} \E \sbr{f(\wk)} - f(\wopt) &\leq \frac{1}{2 \eta \rbr{1 - \eta \wgc L} \, K} \sum_{k=0}^{K-1} \rbr{\E \sbr{\norm{\wk - \wopt}^2} - \E \sbr{\norm{\wkk - \wopt}^2}}\\
                                                           &= \frac{1}{2 \eta \rbr{1 - \eta \wgc L} \, K} \rbr{\norm{\w_0 - \wopt}^2 - \E\sbr{\norm{\w_K - \wopt}^2}}\\
                                                           &\leq \frac{1}{2 \eta \rbr{1 - \eta \wgc L} \, K} \norm{\w_0 - \wopt}^2.
                                                           \intertext{Noting \( \frac{1}{K} \sum_{k=0}^{K-1} f(\wk) \geq f(\bar \w_K) \) by convexity leads to the final result,}
   \E\sbr{f(\bar \w_K)} - f(\wopt) &\leq \frac{1}{2 \eta \rbr{1 - \eta \wgc L} \, K} \norm{\w_0 - \wopt}^2.
   \end{align*}
\end{proof}


\wgcConvexIndSmooth*

\begin{proof}
   Starting from \autoref{lemma:convex-intermediate},
   \begin{align*}
    f(\wk) - f(\wopt) &\leq \frac{1}{2\eta \, C} \rbr{\norm{\wk - \wopt}^2 - \Ek \sbr{\norm{\wkk - \wopt}^2}}\\
    \intertext{Taking expectations and summing from \( k = 0 \) to \( K - 1 \) now gives}
    \implies \frac{1}{K} \sum_{k=0}^{K -1} \E \sbr{f(\wk) - f(\wopt)} &\leq \frac{1}{2\eta \, \delta \, K}\sum_{k=0}^{K-1}\rbr{\E\sbr{\norm{\wk - \wopt}^2} - \E\sbr{\norm{\wkk - \wopt}^2}}\\
                                                         &= \frac{1}{2\eta \, \delta \, K}\rbr{\norm{\w_0 - \wopt}^2 - \E\sbr{\norm{\w_{K} - \wopt}^2}}\\
                                                         &\leq \frac{1}{2\eta \, \delta \, K} \norm{\w_0 - \wopt}^2\\
\intertext{Noting \( \frac{1}{K}\sum_{k=0}^{K-1} f(\wk) \geq f(\bar \w_K) \) by convexity leads to the final result,}
   \implies \E\sbr{f(\bar w_K)} - f(\wopt) &\leq \frac{1}{2\eta \, \delta \, K}\norm{\w_0 - \wopt}^2.
\end{align*}
\end{proof}


\section{Almost Sure Convergence}~\label{app:almost-sure-convergence}

\wgcAlmostSure*
\begin{proof}
    Lemma~\ref{lemma:convex-intermediate} gives the decrease condition
    \begin{align*}
        \E\sbr{\norm{\wkk - \wopt}^2 \mid \calF_\iter} \leq  \norm{\wk - \wopt}^2 - 2 \eta \delta \rbr{f(\wk) - f(\wopt)},
    \end{align*}
    where \( \delta \geq 1 \) since \( \eta < \frac{1}{\wgc L} + \frac{1}{\Lmax} \)
    The conditions of Theorem~\ref{thm:positive_supermartingales} are satisfied with \( A_\iter = 0 \) for all \( \iter \) and the sequence \( \seq{\norm{\wk - \wopt}^2} \) converges to a non-negative random variable \(\lim_{\iter \rightarrow \infty} \norm{\wk - \wopt}^2 \) almost surely.
    The theorem also guarantees
    \begin{align}
        \sum_{k=0}^{\infty} 2\eta \delta \rbr{f(\wk) - f(\wopt)} &< \infty \nonumber \\
        \implies \sum_{k=0}^{\infty} \rbr{f(\wk) - f(\wopt)} &< \infty~\label{eq:func_convergence}\\
        \implies \lim_{k \rightarrow \infty} f(\wk) \equas f(\wopt). \nonumber
    \end{align}
    That is, stochastic gradient descent converges to the optimal function value. 
    Let us extend this convergence to the sequence of iterates \( \seq{\wk} \).
    The weak growth condition implies 
    \begin{align*}
        \E\sbr{\sum_{k=0}^{K} \norm{\wkk - \wk}^2} &= \E\sbr{\sum_{k=0}^{K} \eta^2 \norm{\grad(\wk, \zk)}^2}\\
                                                   &\leq \sum_{k=0}^{K} \wgc \eta^2 \E\sbr{f(\wk) - f(\wopt)}\\
        \implies \lim_{K\rightarrow \infty} \E \sbr{\sum_{k=0}^{K} \norm{\wkk - \wk}^2} &\leq \lim_{K\rightarrow\infty} \E \sbr{\sum_{k=0}^{K} \wgc \eta^2 \rbr{f(\wk) - f(\wopt)}}\\
        \intertext{The partial sums are increasing \( K \), so we may apply the monotone convergence theorem~\citep[Theorem 4.8]{ccinlar2011probability} to obtain} 
        \E \sbr{\sum_{k=0}^{\infty} \norm{\wkk - \wk}^2} &\leq  \E \sbr{\sum_{k=0}^{\infty} \wgc \eta^2 \rbr{f(\wk) - f(\wopt)}}\\
                                                                  &< \infty. \tag{by \autoref{eq:func_convergence}}
    \end{align*}
    The series \(  \sum_{k=0}^{\infty} \norm{\wkk - \wk}^2 \) is a non-negative random variable with finite expectation and so converges almost surely: 
    \[ \sum_{k=0}^{\infty} \norm{\wkk - \wk}^2 < \infty. \]
    Thus, \( \lim_{k \rightarrow \infty} \norm{\wkk - \wk}^2 \equas 0 \) and the Cauchy criterion now guarantees \( \lim_{k \rightarrow \infty} \wk = \w_\infty \) for some random variable \( \w_\infty \).
    Continuity of \( f \) and the continuous mapping theorem imply 
    \[ f(\w_\infty) \equas \lim_{k\rightarrow \infty} f(\wk) \equas f(\wopt). \]
    We conclude \( \w_\infty \in \argmin_\w f(\w) \) almost surely.
\end{proof}

\sgcAlmostSure*
\begin{proof}
    Lemma~\ref{lemma:sgc-decrease-condition} gives the decrease condition
    \begin{align*}
        \E \sbr{f(\wkk) - f(\wopt) \mid \calF_\iter} &\leq \rbr{f(\wk) - f(\wopt)} - \eta \rbr{1 - \frac{\sgc L \eta}{2}}\norm{\grad(\wk)}^2.
    \end{align*}
    Since \( \eta < \frac{2}{\sgc L} \),
    \begin{align*}
        \eta \rbr{1 - \frac{\eta \sgc L}{2}}\norm{\grad(\wk)}^2 > 0,
    \end{align*}
    and the conditions of Theorem~\ref{thm:positive_supermartingales} are satisfied with \( A_\iter = 0 \) for all \( \iter \).
    The sequence \( \seq{f(\wk) - f(\wopt)} \) converges to a non-negative random variable.
    Of more interest is that
    \begin{align}
        \sum_{k=0}^{\infty} \eta \rbr{1 - \frac{\eta \sgc L}{2}} \norm{\grad(\wk)}^2 &< \infty \nonumber\\
        \implies \sum_{k=0}^{\infty} \norm{\grad(\wk)}^2 &< \infty, \label{eq:stationary-convergence}
    \end{align}
    almost surely.
    Accordingly, the sequence of gradient norms satisfies
    \[ \lim_{\iter \rightarrow \infty} \norm{\grad(\wk)}^2 \equas 0, \]
    and we conclude that the sequence of gradients converges to a stationary point.
   
    Let us extend this convergence to the sequence of iterates \( \seq{\wk} \).
    The strong growth condition implies 
    \begin{align*}
        \E\sbr{\sum_{k=0}^{K} \norm{\wkk - \wk}^2} &= \E\sbr{\sum_{k=0}^{K} \eta^2 \norm{\grad(\wk, \zk)}^2}\\
                                                   &\leq \E\sbr{\sum_{k=0}^{K} \sgc \eta^2 \norm{\grad(\wk)}^2}\\
        \implies \lim_{K\rightarrow \infty} \E \sbr{\sum_{k=0}^{K} \norm{\wkk - \wk}^2} &\leq \lim_{K\rightarrow\infty} \E \sbr{\sum_{k=0}^{K} \sgc \eta^2 \norm{\grad(\wk)}^2}\\
        \intertext{The partial sums are increasing in \( K \), so we may apply the monotone convergence theorem~\citep[Theorem 4.8]{ccinlar2011probability} to obtain} 
        \E \sbr{\sum_{k=0}^{\infty} \norm{\wkk - \wk}^2} &\leq  \E \sbr{\sum_{k=0}^{\infty} \sgc \eta^2 \norm{\grad(\wk)}^2}\\
                                                                  &< \infty. \tag{by \autoref{eq:stationary-convergence}}
    \end{align*}
    The series \(  \sum_{k=0}^{\infty} \norm{\wkk - \wk}^2 \) is a non-negative random variable with finite expectation and so converges almost surely: 
    \[ \sum_{k=0}^{\infty} \norm{\wkk - \wk}^2 < \infty. \]
    Thus, \( \lim_{k \rightarrow \infty} \norm{\wkk - \wk}^2 \equas 0 \) and the Cauchy criterion now guarantees \( \lim_{k \rightarrow \infty} \wk = \w_\infty \) for some random variable \( \w_\infty \).
    The mapping \( \w \mapsto \norm{\grad(\w)}^2 \) is a composition of continuous functions and thus is continuous. 
    The continuous mapping theorem implies  
    \[ \norm{\grad(\w_\infty)}^2  \equas \lim_{k\rightarrow \infty} \norm{\grad(\wk)}^2 \equas 0, \]
    which completes the proof.
\end{proof}

\newpage

\section{Additional Lemmas}\label{app:sgd-additional-lemmas}

\begin{lemma}\label{lemma:wgc-fast-extended}
   Consider the setting of Theorem 5 from \citet{vaswani2019fast}: \( f \) is \( \mu \)-strongly-convex, \( L \)-smooth, and \( \rbr{f, \oracle{}} \) satisfies weak growth with parameter \( \alpha \).
   Then Theorem 5 can be modified to allow for a maximum step-size of \( \eta \leq \frac{\mu + L}{\wgc L^2} \) and the following convergence rate:
   \[  \E\sbr{\norm{\w_{K+1} - \wopt}^2}\leq \rbr{1 - \mu \, \eta + \max\cbr{\eta \mu \rbr{\eta \alpha L - 1}, \eta L \rbr{\eta \alpha L - 1}}}^K \norm{\w_0 - \wopt}^2. \]
   Moreover, this is tight with the original result when \( \eta = \frac{1}{\alpha L} \).
\end{lemma}
\begin{proof}
    Starting from the seventh line of the proof of Theorem 5 from \citet{vaswani2019fast},
   \begin{align*}
       \Ek\sbr{\norm{\wkk - \wopt}^2} &\leq 2 \eta \rbr{\eta \alpha L - 1} \rbr{f(\wk) - f(\wopt)} + \rbr{1 - \mu \, \eta} \norm{\wk - \wopt}^2. 
       \intertext{\textbf{Case 1}: \(\eta \leq \inv{\alpha L} \). Then \( 2 \eta \rbr{\eta \alpha L - 1} \rbr{f(\wk) - f(\wopt)} \leq 0\) and \autoref{lemma:gradient-bounds} implies }
       \Ek \sbr{\norm{\wkk - \wopt}^2} &\leq \eta \mu \rbr{\eta \alpha L - 1}\norm{\wk - \wopt}^2 + \rbr{1 - \mu \, \eta} \norm{\wk - \wopt}^2.
       \intertext{\textbf{Case 2}: \( \frac{\mu + L}{\alpha L^2} > \eta > \inv{\alpha L} \). Then the other direction of \autoref{lemma:gradient-bounds} implies}
       \Ek \sbr{\norm{\wkk - \wopt}^2} &\leq \eta L \rbr{\eta \alpha L - 1}\norm{\wk - \wopt}^2 + \rbr{1 - \mu \, \eta} \norm{\wk - \wopt}^2.
       \intertext{Note that the upper bound on \( \eta \) is required to make progress in this case. 
       We combine cases by taking the worst-case bound as follows:}
       \implies \Ek \sbr{\norm{\wkk - \wopt}^2} &\leq \rbr{1 - \mu \, \eta} \norm{\wk - \wopt}^2 + \max\cbr{\eta \mu \rbr{\eta \alpha L - 1}, \eta L \rbr{\eta \alpha L - 1}}\norm{\wk - \wopt}^2\\
                                                &= \rbr{1 - \mu \, \eta + \max\cbr{\eta \mu \rbr{\eta \alpha L - 1}, \eta L \rbr{\eta \alpha L - 1}}} \norm{\wk - \wopt}^2,
       \intertext{Taking expectations and recursing on the inequality gives the final results,}
        \implies \E \sbr{\norm{\wkk - \wopt}^2} &\leq \rbr{1 - \mu \, \eta + \max\cbr{\eta \mu \rbr{\eta \alpha L - 1}, \eta L \rbr{\eta \alpha L - 1}}}^k \norm{\w_0 - \wopt}^2.
   \end{align*}
   Tightness with the original result from \citet{vaswani2019fast} is immediate when \( \eta = \frac{1}{\alpha L} \) in the last expression above.
\end{proof}

