%!TEX root = ../main.tex

\section{Acceleration: Proofs}~\label{app:acceleration}

\begin{restatable}{lemma}{wgcDecreaseCondition}~\label{lemma:wgc-decrease-condition}
    Let \( f \) be an \( L \)-smooth, convex function satisfying the weak growth condition with parameter \( \rho \). 
    Moreover, suppose that the stochastic functions \( f(\cdot, \z) \) are \( L_\z \)-smooth.
    Then the following decrease condition holds for stochastic gradient descent with step-size \( \eta \leq \frac{1}{\Lmax} \):
    \[ \E_{\zk} \sbr{f(\wkk) + \rho L^2 \eta \norm{\wkk - \wopt}^2} \leq f(\wk) + \rho L^2 \eta \norm{\wk - \wopt}^2 - \eta \norm{\grad(\wk)}^2. \]
\end{restatable}

\begin{proof}
    By \( L \)-smoothness of \( f \) we obtain
    \begin{align*}
        f(\wkk) &\leq f(\wk) + \abr{\grad(\wk), \wkk - \wk} + \frac{L}{2}{\wkk - \wk}^2\\
                &= f(\wk) - \eta \abr{\grad(\wk), \grad(\wk, \zk)} + \frac{L \eta^2}{2} \norm{\grad(\wk, \zk)}^2. 
                \intertext{Taking expectations with respect to \( \zk \):}
        \E_{\zk} \sbr{f(\wkk)} &\leq f(\wk) - \eta \E_{\zk} \sbr{\abr{\grad(\wk), \grad(\wk, \zk)}} + \frac{L \eta^2}{2} \E_{\zk} \sbr{\norm{\grad(\wk, \zk)}}^2\\
                             &= f(\wk) - \eta \norm{\grad(\wk)}^2 + \frac{L \eta^2}{2} \E_{\zk} \sbr{\norm{\grad(\wk, \zk)}}^2.
                             \intertext{The weak growth condition implies}
        \E_{\zk} \sbr{f(\wkk)} &\leq f(\wk) - \eta \norm{\grad(\wk)}^2 + 2 \rho L^2 \eta^2 \rbr{f(\wk) - f(\wopt)}.
        \intertext{Using \autoref{lemma:convex-intermediate} and the fact that \( \eta \leq \frac{1}{\Lmax} \), }
        \E_{\zk} \sbr{f(\wkk)} &\leq f(\wk) - \eta \norm{\grad(\wk)}^2 + \rho L^2 \eta \rbr{\norm{\wk - \wopt}^2 - \E_{\zk} \sbr{\norm{\wkk - \wopt}^2}}\\
        \implies \E_{\zk} \sbr{f(\wkk) + \rho L^2 \eta \norm{\wkk - \wopt}^2} &\leq f(\wk) + \rho L^2 \eta \norm{\wk - \wopt}^2 - \eta \norm{\grad(\wk)}^2.
    \end{align*}
\end{proof}


\stochasticEstimatingSeq*
\begin{proof}
    The proof continues largely as before; the main difference is that we use Lemma~\ref{lemma:sgc-decrease-condition} instead of the typical Descent Lemma.
    Notice that \( \gamma_0 \ind \cbr{\zk}_{k=0}^\infty \) implies the \( \cbr{\gamma_k}_{k=0}^\infty \), \( \cbr{\alpha_k}_{k=0}^\infty \), and \( \cbr{\lambda_k}_{k=0}^\infty \) evolve \emph{independently} of the stochastic processes \( \cbr{\yk}_{k=0}^\infty \), \( \cbr{\wk}_{k=0}^\infty \), and \( \cbr{v_k}_{k=0}^\infty \).
    This will be key in our analysis.\\

    As before, \( \phi^*_0 = f(x_0) \) implies that \( \inf \phi_0(x) \geq f(\w_0) \) deterministically, which establishes our base case.
    Now, assume that \( \E[\inf \phi_k(x)] \geq \E[ f(\wk) ] \).
    By Lemma~\ref{lemma:cannonical_form}, \( \phi_{k+1}^* \) is given by
    \begin{align*}
        \phi_{k+1}^* &= (1- \alpha_k) \phi^*_k + \alpha_k f(\yk) - \frac{\alpha_k^2}{2\gamma_{k+1}}\norm{\grad(\yk)}^2 + \frac{\alpha_k(1-\alpha_k)\gamma_k}{\gamma_{k+1}}\rbr{\frac{\mu}{2} \norm{\yk - v_k}^2 + \abr{\grad(\yk), v_k - \yk}} \\
        \intertext{Taking expectations with respect to \( \z_{k}, \ldots, \z_0 \) and using linearity of expectation: }
        \E[\phi_{k+1}^*] &= (1- \alpha_k) \E[\phi^*_k] + \E\sbr{\alpha_k f(\yk) - \frac{\alpha_k^2}{2\gamma_{k+1}}\norm{\grad(\yk)}^2} + \E\sbr{\frac{\alpha_k(1-\alpha_k)\gamma_k}{\gamma_{k+1}}\rbr{\frac{\mu}{2} \norm{\yk - v_k}^2 + \abr{\grad(\yk), v_k - \yk}}} \\
        &\geq (1- \alpha_k) \E\sbr{f(\wk)} + \E\sbr{\alpha_k f(\yk) - \frac{\alpha_k^2}{2\gamma_{k+1}}\norm{\grad(\yk)}^2} + \E\sbr{\frac{\alpha_k(1-\alpha_k)\gamma_k}{\gamma_{k+1}}\rbr{\frac{\mu}{2} \norm{\yk - v_k}^2 + \abr{\grad(\yk), v_k - \yk}}}
        &= (1- \alpha_k) f(\wk) + \alpha_k f(\yk) - \frac{1}{2\rho L}\norm{\grad(\yk)}^2 + \frac{\alpha_k(1-\alpha_k)\gamma_k}{\gamma_{k+1}}\rbr{\frac{\mu}{2} \norm{\yk - v_k}^2 + \abr{\grad(\yk), v_k - \yk}}, \\
        \intertext{where the last inequality follows from the inductive assumption. Convexity of \( f \) implies \( f(\wk) - \abr{ \grad(\yk), \wk - \yk } \geq f(\yk) \). This inequality holds in expectation, allowing us to obtain }
        \E[\phi_{k+1}^*] &\geq (1- \alpha_k) \E[\rbr{f(\wk) - \abr{\grad(\yk), \wk - \yk}}] + \E\sbr{(1-\alpha_k) \abr{\grad(\yk), \wk - \yk} + \alpha_k f(\yk) - \frac{1}{2\rho L}\norm{\grad(\yk)}^2} \\
        & \quad \quad+ \E\sbr{\frac{\alpha_k(1-\alpha_k)\gamma_k}{\gamma_{k+1}}\rbr{\frac{\mu}{2} \norm{\yk - v_k}^2 + \abr{\grad(\yk), v_k - \yk}}} \\
        &\geq  (1-\alpha_k) \E[f(\yk)] + \E\sbr{\alpha_k f(\yk) - \frac{1}{2\rho L} \norm{\grad(\yk)}^2} \\
        & \quad \quad + (1-\alpha_k) \E\sbr{\abr{\grad(\yk), \wk - \yk} + \frac{\alpha_k\gamma_k}{\gamma_{k+1}}\rbr{\frac{\mu}{2} \norm{\yk - v_k}^2 + \abr{\grad(\yk), v_k - \yk}}} \\
        &=  \E\sbr{f(\yk) - \frac{1}{2\rho L} \norm{\grad(\yk)}^2} + (1-\alpha_k) \E\sbr{\abr{\grad(\yk), \wk - \yk} + \frac{\alpha_k\gamma_k}{\gamma_{k+1}}\rbr{\frac{\mu}{2} \norm{\yk - v_k}^2 + \abr{\grad(\yk), v_k - \yk}}} \\
        \intertext{Lemma~\ref{lemma:sgc-decrease-condition} with step-size \( \eta = \frac{1}{\rho L} \) implies }
        \E[\phi_{k+1}^*] &\geq \E\sbr{f(\wkk)} + (1-\alpha_k) \E\sbr{\abr{\grad(\yk), \wk - \yk} + \frac{\alpha_k\gamma_k}{\gamma_{k+1}}\rbr{\frac{\mu}{2} \norm{\yk - v_k}^2 + \abr{\grad(\yk), v_k - \yk}}} \\
        \intertext{The remainder of the proof is largely unchanged from the determnistic case:}
        \phi_{k+1}^*  &=  \E[f(\wkk)] + (1-\alpha_k) \E\sbr{\frac{\alpha_k}{\gamma_k + \alpha_k \mu} \abr{\grad(\yk), \nabla\phi_k(\wk)} + \frac{\alpha_k\gamma_k}{\gamma_{k+1}}\rbr{\frac{\mu}{2} \norm{\yk - v_k}^2 + \abr{\grad(\yk), v_k - \yk}}} \\
        \intertext{Noting that \( \nabla \phi_k(\wk) = \gamma_k \rbr{\wk - v_k} \) and \( v_k - \yk = \frac{\gamma_{k+1}}{\gamma_k + \alpha_k \mu}\rbr{v_k - \wk} \) gives }
        &= \E[f(\wkk)] +(1-\alpha_k) \E\sbr{\frac{\alpha_k \gamma_k}{\gamma_k + \alpha_k \mu} \abr{\grad(\yk), \wk - v_k} + \frac{\alpha_k\gamma_k}{\gamma_{k+1}}\rbr{\frac{\mu}{2} \norm{\yk - v_k}^2 + \abr{\grad(\yk), v_k - \yk}}} \\
        &= \E[f(\wkk)] + (1-\alpha_k) \E\sbr{\frac{\alpha_k \gamma_k}{\gamma_k + \alpha_k \mu} \abr{\grad(\yk), \wk - v_k} + \frac{\alpha_k\gamma_k}{\gamma_{k+1}}\rbr{\frac{\mu}{2} \norm{\yk - v_k}^2 + \abr{\grad(\yk), \frac{\gamma_{k+1}}{\gamma_k + \alpha_k \mu}(v_k - \wk)}}} \\
        &= \E[f(\wkk)] + \frac{\mu \alpha_k(1-\alpha_k)\gamma_k}{2 \gamma_{k+1}} \E\sbr{\norm{\yk - v_k}^2} \\
        &\geq \E[f(\wkk)].
    \end{align*}
\end{proof}


\scAcceleration*
\begin{proof}
    We can obtain the following inequality immediately:
    \begin{align*}
        \E\sbr{f(\wk)} &\leq \E[\inf_x \phi_k(x)] \tag{by Lemma~\ref{lemma:characterization_of_stochastic_estimating_sequences}}\\
        &\leq \E[\phi_k(\wopt)] \\
        &\leq \rbr{1 - \lambda_k} f(\wopt) + \lambda_k \phi_0(\wopt) \tag{by Definition~\ref{def:estimating_sequences}}\\
        \implies \E\sbr{f(\wk)} - f(\wopt) &\leq \lambda_k (\phi_0(\wopt) - f(\wopt)).
    \end{align*}
    The choice of \( \gamma_0 = \mu \) implies
    \begin{itemize}
        \item \(  \gamma_{k}  = (1 - \alpha_{k-1}) \gamma_{k-1} + \alpha_{k-1} \mu = \mu \);
        \item \( \alpha_{k} = \sqrt{\frac{\gamma_{k+1}}{\rho L}} = \sqrt{\frac{\mu}{\rho L}} \);
        \item \( \lambda_{k} = (1 - \alpha_{k-1}) \lambda_{k-1} = \rbr{1 - \sqrt{\frac{\mu}{\rho L}}}^{k} \).
    \end{itemize}
    Altogether, we obtain
    \[ \E\sbr{f(\wk)} - f(\wopt) \leq \rbr{1 - \sqrt{\frac{\mu}{\rho L}}}^{k} \rbr{ f(w_0) - f(\wopt) + \frac{\mu}{2} \norm{\wopt - \w_0}^2 }. \]
\end{proof}
