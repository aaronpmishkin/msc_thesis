%! TEX root = ../main.tex

\chapter{Acceleration: Proofs}~\label{app:acceleration}

\section{Estimating Sequences}\label{app:estimating-sequences}

\agdEquivalence*
\begin{proof}
    We expand the updated for \( \yk \) using the canonical form of \( \phi_k \) as follows: 
    \begin{align*}
        \yk &= \wk - \frac{\alphak}{\gamma_k + \alphak \mu} \nabla \phi_k(\wk)\\
            &= \wk - \frac{\alphak}{\gamma_k + \alphak \mu} \nabla \rbr{\phi^*_k + \frac{\gamma_k}{2}\norm{\wk - \vk}^2}\\
            &= \wk - \frac{\alphak \gamma_k }{\gamma_k + \alphak \mu} \rbr{\wk - \vk}\\
            &= \rbr{\frac{(1 - \alphak)\gamma_k + \alphak \mu}{\gamma_k + \alphak \mu}} \wk + \frac{\alphak \gamma_k}{\gamma_k + \alphak \mu} \vk.
            \intertext{Recalling \( \gamma_{k+1} = (1 - \alphak)\gamma_k + \alphak \mu \) gives}
        \yk &= \frac{\gamma_{k+1}}{\gamma_k + \alphak \mu} \wk + \frac{\alphak \gamma_k}{\gamma_k + \alphak \mu} \vk\\
            &= \frac{1}{\gamma_k + \alphak \mu} \rbr{\gamma_{k+1} \wk + \alphak \gamma_k \vk}.
    \end{align*}
    This is identical to update Step (b) of \textbf{Constant Step-Size Scheme I} given by \citet[Eq. 2.2.19]{nesterov2004lectures}.
    The equivalence of \textbf{Constant Step-Size Scheme I} and \ac{AGD} (called \textbf{Constant Step-Size Scheme II}) is established by \citet[Page 92]{nesterov2004lectures} ---  albeit with a different step-size for the gradient step ---  which completes the proof.
\end{proof}

\newpage

\estimatingSequenceBound*
\begin{proof}
    First we establish a few preliminaries.
    The condition \( \gamma_0 \ind \seq{\zk} \) implies the \( \seq{\gamma_k}_{k=0}^\infty \), \( \seq{\alpha_k}_{k=0}^\infty \), and \( \seq{\lambda_k}_{k=0}^\infty \) sequences evolve \emph{independently} of the stochastic processes \( \seq{\yk}_{k=0}^\infty \), \( \seq{\wk}_{k=0}^\infty \), and \( \seq{v_k}_{k=0}^\infty \).
    This will be necessary in later in the proof.
    Next, instantiating \autoref{lemma:sgc-decrease-condition} with \( \eta = \frac{1}{\sgc L} \) yields
    \begin{align}
        \Ek \sbr{f(\wkk)} &\leq f(\wk) - \frac{1}{2 \sgc L} \norm{\grad(\wk)}^2.\nonumber\\
            \intertext{Taking expectations with respect to \( z_0, \ldots, z_{k-1} \),}
        \implies \E \sbr{f(\wkk)} &\leq \E\sbr{f(\wk)} - \frac{1}{2 \sgc L} \E \sbr{\norm{\grad(\wk)}^2}.\label{eq:expected-decrease}
    \end{align}
    Now we move on to the main argument, which proceeds by induction.\hfill \break

    \noindent The choice of \( \phi^*_0 = f(x_0) \) ensures \( \inf \phi_0(\w) = f(\w_0) \) deterministically, which is the base case for induction.
    The inductive assumption is \( \E[\inf \phi_k(\w)] \geq \E[ f(\wk) ] \); let us use this to show 
    \[ \E\sbr{\inf_{\w} \phi_{k+1}(\w)} \geq \E\sbr{f(\wkk)}. \]
    Recall the explicit form of the minimizer \( \inf \phi_{k+1}(\w) = \phi_{k+1}^* \) is 
    \begin{align*}
        \phi_{k+1}^* &= (1- \alpha_k) \phi^*_k + \alpha_k f(\yk) - \frac{\alpha_k^2}{2\gamma_{k+1}}\norm{\grad(\yk)}^2 + \frac{\alpha_k(1-\alpha_k)\gamma_k}{\gamma_{k+1}}\rbr{\frac{\mu}{2} \norm{\yk - v_k}^2 + \abr{\grad(\yk), v_k - \yk}} \\
        \intertext{Taking expectations with respect to \( \z_{0}, \ldots, \z_k \) and using \( \gamma_0 \ind \seq{\zk} \) plus linearity of expectation: }
        \E[\phi_{k+1}^*] &= (1- \alpha_k) \E[\phi^*_k] + \E\sbr{\alpha_k f(\yk) - \frac{\alpha_k^2}{2\gamma_{k+1}}\norm{\grad(\yk)}^2} \\ & \hspace{5em} + \E\sbr{\frac{\alpha_k(1-\alpha_k)\gamma_k}{\gamma_{k+1}}\rbr{\frac{\mu}{2} \norm{\yk - v_k}^2 + \abr{\grad(\yk), v_k - \yk}}} \\
                         &\geq (1- \alpha_k) \E\sbr{f(\wk)} + \E\sbr{\alpha_k f(\yk) - \frac{\alpha_k^2}{2\gamma_{k+1}}\norm{\grad(\yk)}^2} \\ & \hspace{5em} + \E\sbr{\frac{\alpha_k(1-\alpha_k)\gamma_k}{\gamma_{k+1}}\rbr{\frac{\mu}{2} \norm{\yk - v_k}^2 + \abr{\grad(\yk), v_k - \yk}}},
        \intertext{where the inequality follows from the inductive assumption. 
        Convexity of \( f \) implies \( f(\wk) \geq f(\yk) + \abr{ \grad(\yk), \wk - \yk } \). 
        This inequality holds in expectation, allowing us to obtain }
        \E[\phi_{k+1}^*] &\geq (1- \alpha_k) \E[\rbr{f(\yk) + \abr{\grad(\yk), \wk - \yk}}] + \E\bigg[\alpha_k f(\yk) - \frac{1}{2\sgc L}\norm{\grad(\yk)}^2 \bigg] \\ & \hspace{5em} + \E\sbr{\frac{\alpha_k(1-\alpha_k)\gamma_k}{\gamma_{k+1}}\rbr{\frac{\mu}{2} \norm{\yk - v_k}^2 + \abr{\grad(\yk), v_k - \yk}}} \\
                         &= \E\sbr{(1- \alpha_k) f(\yk) + \alpha_k f(\yk)} + (1-\alpha_k) \E[\abr{\grad(\yk), \wk - \yk}] + \E\bigg[\frac{1}{2\sgc L}\norm{\grad(\yk)}^2 \bigg] \\ & \hspace{5em} + \E\sbr{\frac{\alpha_k(1-\alpha_k)\gamma_k}{\gamma_{k+1}}\rbr{\frac{\mu}{2} \norm{\yk - v_k}^2 + \abr{\grad(\yk), v_k - \yk}}} \\
        &=  \E\sbr{f(\yk) - \frac{1}{2\sgc L} \norm{\grad(\yk)}^2} + (1-\alpha_k) \E\bigg[\abr{\grad(\yk), \wk - \yk} \\ & \hspace{5em} + \frac{\alpha_k\gamma_k}{\gamma_{k+1}}\rbr{\frac{\mu}{2} \norm{\yk - v_k}^2 + \abr{\grad(\yk), v_k - \yk}}\bigg] \\
        \intertext{\autoref{eq:expected-decrease} now implies}
        \E[\phi_{k+1}^*] &\geq \E\sbr{f(\wkk)} + (1-\alpha_k) \E\sbr{\abr{\grad(\yk), \wk - \yk} + \frac{\alpha_k\gamma_k}{\gamma_{k+1}}\rbr{\frac{\mu}{2} \norm{\yk - v_k}^2 + \abr{\grad(\yk), v_k - \yk}}}. \\
        \intertext{The remainder of the proof is largely unchanged from the deterministic case:}
        \E\sbr{\phi_{k+1}^*}  &=  \E[f(\wkk)] + (1-\alpha_k) \E\sbr{\frac{\alpha_k}{\gamma_k + \alpha_k \mu} \abr{\grad(\yk), \nabla\phi_k(\wk)} + \frac{\alpha_k\gamma_k}{\gamma_{k+1}}\rbr{\frac{\mu}{2} \norm{\yk - v_k}^2 + \abr{\grad(\yk), v_k - \yk}}} \\
        \intertext{Noting that \( \nabla \phi_k(\wk) = \gamma_k \rbr{\wk - v_k} \) and \( v_k - \yk = \frac{\gamma_{k+1}}{\gamma_k + \alpha_k \mu}\rbr{v_k - \wk} \) gives }
        \E\sbr{\phi_{k+1}^*} &= \E[f(\wkk)] +(1-\alpha_k) \E\sbr{\frac{\alpha_k \gamma_k}{\gamma_k + \alpha_k \mu} \abr{\grad(\yk), \wk - v_k} + \frac{\alpha_k\gamma_k}{\gamma_{k+1}}\rbr{\frac{\mu}{2} \norm{\yk - v_k}^2 + \abr{\grad(\yk), v_k - \yk}}} \\
        &= \E[f(\wkk)] + (1-\alpha_k) \E\bigg[\frac{\alpha_k \gamma_k}{\gamma_k + \alpha_k \mu} \abr{\grad(\yk), \wk - v_k} + \frac{\alpha_k\gamma_k}{\gamma_{k+1}}\Big(\frac{\mu}{2} \norm{\yk - v_k}^2 \\
        & \hspace{5em} + \frac{\gamma_{k+1}}{\gamma_k + \alpha_k \mu} \abr{\grad(\yk), v_k - \wk}\Big)\bigg] \\
        &= \E[f(\wkk)] + \frac{\mu \alpha_k(1-\alpha_k)\gamma_k}{2 \gamma_{k+1}} \E\sbr{\norm{\yk - v_k}^2} \\
        \intertext{Non-negativity of \( \frac{\mu \alpha_k(1-\alpha_k)\gamma_k}{2 \gamma_{k+1}} \) gives the final result,} 
        \E\sbr{\phi_{k+1}^*} &\geq \E[f(\wkk)].
    \end{align*}
    We conclude that \( \E[\inf \phi_{k}(\w)] \geq \E[f(\wk)] \) holds for all \( k \in \bbN \) by induction. 
\end{proof}

\newpage

\section{Convergence for Strongly-Convex Functions}\label{app:agd-sc}

\scAGD*
\begin{proof}
    We obtain the following inequality immediately:
    \begin{align}
        \E\sbr{f(\wk)} &\leq \E[\inf_x \phi_k(x)] \tag{\autoref{lemma:estimating-sequence-bound}}\\
        &= \E[\phi_k(\wopt)] \nonumber \\
        &\leq \rbr{1 - \lambda_k} f(\wopt) + \lambda_k \phi_0(\wopt) \tag{\autoref{lemma:estimating-sequence}} \nonumber\\
        \implies \E\sbr{f(\wk)} - f(\wopt) &\leq \lambda_k (\phi_0(\wopt) - f(\wopt)) \nonumber\\
                                           &= \lambda_k \rbr{f(\w_0) - f(\wopt) + \frac{\mu}{2}\norm{\w_0 - \wopt}^2 }.\label{eq:lambda-rate}
    \end{align}
    The convergence rate of \ac{RSAGD} is determined by the speed at which \( \lambda_k \) converges to 0, which we shall now derive.
    The canonical form of \( \phi_k \) (\autoref{eq:canonical-form}) gives the following expression for \( \gamma_k \): 
    \begin{align*}
        \gamma_{k} &= (1 - \alpha_{k-1}) \gamma_{k-1} + \alpha_{k-1} \mu.
        \intertext{The choice \( \gamma_0 = \mu \) now yields }
        \gamma_{k} &= (1 - \alpha_{k-1}) \mu + \alpha_{k-1} \mu = \mu,
    \end{align*}
    by induction on \( k \).
    The definition of \ac{RSAGD} (\autoref{procedure:reformulated-agd}) gives 
    \begin{align*}
        \alpha_k^2 &= \frac{\gamma_{k+1}}{\rho L} = \frac{\mu}{\rho L},\\
        \implies \alpha_k &= \sqrt{\frac{\mu}{\rho L}}. 
    \end{align*}
    Finally, the choice of estimating sequences in \autoref{lemma:estimating-sequence} gives \( \lambda_0 = 1 \) and
    \begin{align*}
        \lambda_k &= \rbr{1 - \alpha_k} \lambda_{k-1}\\ 
                  &= \rbr{1 - \sqrt{\frac{\mu}{\rho L}}} \lambda_{k-1}.
                  \intertext{for every \( k > 0 \). Recursing on this inequality gives}
        \lambda_k &= \rbr{1 - \sqrt{\frac{\mu}{\rho L}}}^k \lambda_0\\
                  &= \rbr{1 - \sqrt{\frac{\mu}{\rho L}}}^k.
    \end{align*}
    Substituting this expression into \autoref{eq:lambda-rate} yields the final result: 
    \[ \E\sbr{f(\wk)} - f(\wopt) \leq \rbr{1 - \sqrt{\frac{\mu}{\sgc L}}}^{k} \rbr{ f(w_0) - f(\wopt) + \frac{\mu}{2} \norm{\wopt - \w_0}^2 }. \]
\end{proof}

\section{Convergence for Convex Functions}\label{app:agd-convex}

\convexAGD*
\begin{proof}
    We obtain the following inequality immediately:
    \begin{align}
        \E\sbr{f(\wk)} &\leq \E[\inf_x \phi_k(x)] \tag{by Lemma~\ref{lemma:estimating-sequence-bound}} \nonumber\\
        &\leq \E[\phi_k(\wopt)] \nonumber \\
        &\leq \rbr{1 - \lambda_k} f(\wopt) + \lambda_k \phi_0(\wopt) \tag{by Definition~\ref{def:estimating_sequences}} \nonumber\\
        \implies \E\sbr{f(\wk)} - f(\wopt) &\leq \lambda_k (\phi_0(\wopt) - f(\wopt)) \nonumber\\
                                           &= \lambda_k \rbr{f(\w_0) - f(\wopt) + \sgc L \norm{\w_0 - \wopt}^2 } \label{eq:convex-lambda-rate}.
    \end{align}
    We can see that the convergence rate of \ac{RSAGD} is controlled by the \( \lambda_k \) sequence.
    Let us analyze its rate of convergence to \( 0 \).
    The canonical form of \( \phi_k \) (\autoref{eq:canonical-form}) gives the following expression for \( \gamma_k \): 
    \begin{align*}
        \gamma_{k} &= (1 - \alpha_{k-1}) \gamma_{k-1} + \alpha_{k-1} \mu\\
        \gamma_{k} &= (1 - \alpha_{k-1}) \gamma_{k-1}, 
    \end{align*}
    since \( \mu = 0 \).
    The definition of \ac{RSAGD} (\autoref{procedure:reformulated-agd}) and our choice of estimating sequences (\autoref{lemma:estimating-sequence} gives 
    \begin{align*}
        \alpha_k^2 &= \frac{\gamma_{k+1}}{\rho L},\\
        \lambda_k & = (1 - \alpha_k) \lambda_{k-1}. 
    \end{align*}
    Now we must upper-bound \( \lambda_k \), given that \( \lambda_0 = 2 \tilde L \).
    Let \( \tilde L = \sgc L \). 
    Invoking Lemma 2.2.4 of \citet{nesterov2004lectures} with \( \tilde L \) implies 
    \begin{align*}
        \lambda_k &\leq \frac{4 \tilde L}{\gamma_0 \rbr{k + 1}^2}\\ 
                  &= \frac{2}{\rbr{k + 1}^2}.
    \end{align*}
    Substituting this into \autoref{eq:convex-lambda-rate} yields the final result: 
    \[  \E\sbr{f(\wk)} - f(\wopt) \leq \frac{2}{\rbr{k+1}^2}\rbr{f(\w_0) - f(\wopt) + \sgc L \norm{\w_0 - \wopt}^2 }. \]
\end{proof}

\iflong%
\section{Additional Lemmas}\label{app:agd-additional-lemmas}

\wgcDecreaseCondition*
\begin{proof}
    By \( L \)-smoothness of \( f \) we obtain
    \begin{align*}
        f(\wkk) &\leq f(\wk) + \abr{\grad(\wk), \wkk - \wk} + \frac{L}{2}{\wkk - \wk}^2\\
                &= f(\wk) - \eta \abr{\grad(\wk), \grad(\wk, \zk)} + \frac{L \eta^2}{2} \norm{\grad(\wk, \zk)}^2. 
                \intertext{Taking expectations with respect to \( \zk \):}
        \E_{\zk} \sbr{f(\wkk)} &\leq f(\wk) - \eta \E_{\zk} \sbr{\abr{\grad(\wk), \grad(\wk, \zk)}} + \frac{L \eta^2}{2} \E_{\zk} \sbr{\norm{\grad(\wk, \zk)}}^2\\
                             &= f(\wk) - \eta \norm{\grad(\wk)}^2 + \frac{L \eta^2}{2} \E_{\zk} \sbr{\norm{\grad(\wk, \zk)}}^2.
                             \intertext{The weak growth condition implies}
        \E_{\zk} \sbr{f(\wkk)} &\leq f(\wk) - \eta \norm{\grad(\wk)}^2 + 2 \wgc L^2 \eta^2 \rbr{f(\wk) - f(\wopt)}.
        \intertext{Using \autoref{lemma:convex-intermediate} and the fact that \( \eta \leq \frac{1}{\Lmax} \), }
        \E_{\zk} \sbr{f(\wkk)} &\leq f(\wk) - \eta \norm{\grad(\wk)}^2 + \wgc L^2 \eta \rbr{\norm{\wk - \wopt}^2 - \E_{\zk} \sbr{\norm{\wkk - \wopt}^2}}\\
        \implies \E_{\zk} \sbr{f(\wkk) + \wgc L^2 \eta \norm{\wkk - \wopt}^2} &\leq f(\wk) + \wgc L^2 \eta \norm{\wk - \wopt}^2 - \eta \norm{\grad(\wk)}^2.
    \end{align*}
\end{proof}

\fi
