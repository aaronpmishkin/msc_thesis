%! TEX root = ../main.tex

\chapter{Useful Lemmas}~\label{app:useful-lemmas}

\begin{restatable}{lemma}{iterateBounds}~\label{lemma:iterate-bounds}
    Let \( f \) be a \( \mu \)-strongly-convex, \( L \)-smooth function. 
    Then, for all \( \w \in \R^d \), the optimality gap and distance to the minimizer can be related as follows: 
    \[ \frac{\mu}{2} \norm{w - \wopt}^2 \leq f(\w) - f(\wopt) \leq \frac{L}{2} \norm{w - \wopt}^2. \]
\end{restatable}

The proof of Lemma~\ref{lemma:iterate-bounds} follows immediately from the definitions of Lipschitz-smoothness and strong convexity evaluated at \( \w \) and \( \wopt \).

\begin{restatable}{lemma}{gradientBounds}~\label{lemma:gradient-bounds}
    Let \( f \) be an \( L \)-smooth function. 
    Then \( f \) satisfies the following inequality for all  \( w \in \R^d \) as follows: 
    \[ \frac{1}{2L} \norm{\grad(\w)}^2 \leq f(\w) - f(\wopt). \]
    Similarly, if \( f \) is \( \mu \)-strongly-convex, then \( f \) satisfies the Polyak-≈Åojasiewicz condition, 
    \[ f(\w) - \f(\wopt) \leq \frac{1}{2 \mu}\norm{\grad(\w)}^2 \quad \quad \forall \, \w \in \R^d. \]
\end{restatable}
See \citet{karimi2016linear} for proof.

\begin{restatable}{lemma}{preCoercivity}~\label{lemma:pre-coercivity}
    Let \( f  \) be a convex, \( L  \)-smooth function. 
    Then \( f  \) satisfies the following inequality for all \( \w, v \in \R^d \):
    \[ f(\w) - f(v)  \leq  \abr{\grad(\w), \w - v}- \frac{1}{2L} \norm{\grad(\w) - \grad(v)}^2. \] 
\end{restatable}
See \citet[Lemma 3.5]{bubeck2015convex} for proof.

\begin{restatable}{lemma}{coercivity}~\label{lemma:coercivity}
    Let \( f \) be a \( \mu  \)-strongly-convex, \( L  \)-smooth function. 
    Then \( f \) satisfies the following inequality:
    \[ \abr{\grad(x) - \grad(y), x - y} \geq \frac{\mu L}{\mu + L}\norm{x - y}^2 + \frac{1}{\mu + L} \norm{\grad(x) - \grad(y)}^2.  \] 
\end{restatable}
See \citet[Lemma 3.11]{bubeck2015convex} for proof.
