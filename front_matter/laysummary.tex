%% The following is a directive for TeXShop to indicate the main file
%!TEX root = ../main.tex

%% https://www.grad.ubc.ca/current-students/dissertation-thesis-preparation/preliminary-pages
%%
%% LAY SUMMARY Effective May 2017, all theses and dissertations must
%% include a lay summary.  The lay or public summary explains the key
%% goals and contributions of the research/scholarly work in terms that
%% can be understood by the general public. It must not exceed 150
%% words in length.

\chapter{Lay Summary}

A major trend in machine learning is the use of flexible models which can exactly fit large quantities of data. 
For example, deep learning approaches can ``memorize'' datasets, meaning they achieve nearly perfect predictions on the samples used to fit the model. 
In this case, we say that the model \emph{interpolates} the dataset.
Interpolating models are particularly interesting from an optimization perspective because they can be fit very quickly using stochastic gradient methods. 
This contrasts with general models, where stochastic gradient methods are notoriously slow. 
In this thesis, we develop a rigorous definition of interpolation and study the speed of stochastic gradient methods for interpolating models.  
Our approach is more general than existing analyses and covers standard model-fitting using a dataset as a special case. 
For many model classes, we show stochastic gradient methods permit a wider range of parameters and are faster than previously known when interpolation is satisfied. 
