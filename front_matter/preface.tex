%% The following is a directive for TeXShop to indicate the main file
%!TEX root = ../main.tex

\chapter{Preface}

This thesis was conceived and written solely by the author, Aaron Mishkin. 
The basis for the theoretical work was developed in collaboration with Sharan Vaswani and Frederik Kunstner over the last two years.
The main theorems presented are original, unpublished work with the exception of \autoref{thm:non-convex-line-search}, which was published in \citet{vaswani2019painless}. A. Mishkin is a coauthor of this paper.

The extension of interpolation to general stochastic oracles in \autoref{ch:interpolation-gc} was conducted by the author alone, while the connection between interpolation and weak/strong growth was developed in collaboration with S. Vaswani, and F. Kunstner. 
The improved convergence theorems in Chapters~\ref{ch:sgd} and~\ref{ch:acceleration} build on previous work by \citet{vaswani2019fast} and were proved solely by the author.  
Similarly, \autoref{ch:line-search} presents new improvements of existing work by \citet{vaswani2019painless} with the exception of the previously published \autoref{thm:non-convex-line-search}, as noted above.
The stochastic Armijo line-search and \autoref{lemma:step-size-bound} are due to \citep{vaswani2019painless}. 
The proof technique for \autoref{thm:non-convex-line-search} was developed and suggested by S. Vaswani, while the specific result was proved by A. Mishkin. 

\autoref{thm:sc-line-search}, which improves the dependency on the strong-convexity parameter from \( \bar \mu \) to \( \mu \), was suggested by S. Vaswani with reference to a similar result for the stochastic Polyak step-size by \citet{loizou2020sps}. 
A. Mishkin proved the theorem alone. 
The analysis of stochastic gradient descent for \( \ell_2 \)-regularized functions satisfying interpolation in \autoref{ch:beyond-interpolation} was inspired by a conversation with Eduard Gorbunov.

All figures are the original product of the author, A. Mishkin.
