### Optimization Monographs ###

@article{bubeck2015convex,
  author    = {S{\'{e}}bastien Bubeck},
  title     = {Convex Optimization: Algorithms and Complexity},
  journal   = {Found. Trends Mach. Learn.},
  volume    = {8},
  number    = {3-4},
  pages     = {231--357},
  year      = {2015},
}

@book{nesterov2004lectures,
  author    = {Yurii E. Nesterov},
  title     = {Introductory Lectures on Convex Optimization - {A} Basic Course},
  series    = {Applied Optimization},
  volume    = {87},
  publisher = {Springer},
  year      = {2004},
 }

@book{nocedal1999numerical,
  author    = {Jorge Nocedal and
               Stephen J. Wright},
  title     = {Numerical Optimization},
  publisher = {Springer},
  year      = {1999},
}

### Curvature Conditions ###

@inproceedings{karimi2016linear,
  title={Linear convergence of gradient and proximal-gradient methods under the polyak-{\l}ojasiewicz condition},
  author={Karimi, Hamed and Nutini, Julie and Schmidt, Mark},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={795--811},
  year={2016},
  organization={Springer}
}

@article{zhang2013gradient,
  title={Gradient methods for convex minimization: better rates under weaker conditions},
  author={Zhang, Hui and Yin, Wotao},
  journal={arXiv preprint arXiv:1303.4645},
  year={2013}
}

### Growth Conditions ###

%% Strong Growth + Noise
@article{poljak1973pseudogradient,
  title={Pseudogradient adaptation and training algorithms},
  author={Polyak, BT and Tsypkin, Ya Z},
  journal={Automation and Remote Control},
  volume={34},
  pages={45--67},
  year={1973}
}

@article{khaled2020better,
  title={Better Theory for SGD in the Nonconvex World},
  author={Khaled, Ahmed and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2002.03329},
  year={2020}
}

@article{tseng1998incremental,
  author    = {Paul Tseng},
  title     = {An Incremental Gradient(-Projection) Method with Momentum Term and
               Adaptive Stepsize Rule},
  journal   = {{SIAM} Journal on Optimization},
  volume    = {8},
  number    = {2},
  pages     = {506--531},
  year      = {1998}
}

@article{solodov1998incremental,
  author    = {Mikhail V. Solodov},
  title     = {Incremental Gradient Algorithms with Stepsizes Bounded Away from Zero},
  journal   = {Comp. Opt. and Appl.},
  volume    = {11},
  number    = {1},
  pages     = {23--35},
  year      = {1998}
}

@article{schmidt2013fast,
  title={Fast convergence of stochastic gradient descent under a strong growth condition},
  author={Schmidt, Mark and Roux, Nicolas Le},
  journal={arXiv preprint arXiv:1308.6370},
  year={2013}
}


### Interpolation ###

@article{bassily2018exponential,
  title={On exponential convergence of sgd in non-convex over-parametrized learning},
  author={Bassily, Raef and Belkin, Mikhail and Ma, Siyuan},
  journal={arXiv preprint arXiv:1811.02564},
  year={2018}
}

@inproceedings{
    Liu2020Accelerating,
    title={Accelerating SGD with momentum for over-parameterized learning},
    author={Chaoyue Liu and Mikhail Belkin},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=r1gixp4FPH}
}

@inproceedings{vaswani2019fast,
  author    = {Sharan Vaswani and
               Francis Bach and
               Mark W. Schmidt},
  editor    = {Kamalika Chaudhuri and
               Masashi Sugiyama},
  title     = {Fast and Faster Convergence of {SGD} for Over-Parameterized Models
               and an Accelerated Perceptron},
  booktitle = {The 22nd International Conference on Artificial Intelligence and Statistics,
               {AISTATS} 2019, 16-18 April 2019, Naha, Okinawa, Japan},
  series    = {Proceedings of Machine Learning Research},
  volume    = {89},
  pages     = {1195--1204},
  publisher = {{PMLR}},
  year      = {2019}
}


@inproceedings{vaswani2019painless,
  author    = {Sharan Vaswani and
               Aaron Mishkin and
               Issam H. Laradji and
               Mark Schmidt and
               Gauthier Gidel and
               Simon Lacoste{-}Julien},
  editor    = {Hanna M. Wallach and
               Hugo Larochelle and
               Alina Beygelzimer and
               Florence d'Alch{\'{e}}{-}Buc and
               Emily B. Fox and
               Roman Garnett},
  title     = {Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence
               Rates},
  booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
               on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14
               December 2019, Vancouver, BC, Canada},
  pages     = {3727--3740},
  year      = {2019},
}

### Acceleration ###

@inproceedings{flammarion2015averaging,
  author    = {Nicolas Flammarion and
               Francis R. Bach},
  editor    = {Peter Gr{\"{u}}nwald and
               Elad Hazan and
               Satyen Kale},
  title     = {From Averaging to Acceleration, There is Only a Step-size},
  booktitle = {Proceedings of The 28th Conference on Learning Theory, {COLT} 2015,
               Paris, France, July 3-6, 2015},
  series    = {{JMLR} Workshop and Conference Proceedings},
  volume    = {40},
  pages     = {658--695},
  publisher = {JMLR.org},
  year      = {2015},
}

@inproceedings{schmidt2011convergence,
  author    = {Mark Schmidt and
               Nicolas Le Roux and
               Francis R. Bach},
  editor    = {John Shawe{-}Taylor and
               Richard S. Zemel and
               Peter L. Bartlett and
               Fernando C. N. Pereira and
               Kilian Q. Weinberger},
  title     = {Convergence Rates of Inexact Proximal-Gradient Methods for Convex
               Optimization},
  booktitle = {Advances in Neural Information Processing Systems 24: 25th Annual
               Conference on Neural Information Processing Systems 2011. Proceedings
               of a meeting held 12-14 December 2011, Granada, Spain},
  pages     = {1458--1466},
  year      = {2011},
}


% Almost Sure References
# almost sure convergence under weakest assumptions (according to Orabona)
@article{bertsekas2000gradient,
  title={Gradient convergence in gradient methods with errors},
  author={Bertsekas, Dimitri P. and Tsitsiklis, John N.},
  journal={SIAM Journal on Optimization},
  volume={10},
  number={3},
  pages={627--642},
  year={2000},
  publisher={SIAM}
}

# survey of almost sure convergence of stochastic gradient, subgradient, and proximal methods for finite-sum functions.
@article{bertsekas2011incremental,
  title={Incremental gradient, subgradient, and proximal methods for convex optimization: A survey},
  author={Bertsekas, Dimitri P},
  journal={Optimization for Machine Learning},
  volume={2010},
  number={1-38},
  pages={3},
  year={2011},
  publisher={MIT press}
}

# almost sure convergence of SGD with diminishing step size; larger bound on step-size than previous work.
@article{nguyen2018sgd,
  title={S{GD} and {H}ogwild! convergence without the bounded gradients assumption},
  author={Nguyen, Lam M. and Nguyen, Phuong Ha and van Dijk, Marten and Richt{\'a}rik, Peter and Scheinberg, Katya and Tak{\'a}{\v{c}}, Martin},
  journal={arXiv preprint arXiv:1802.03801},
  year={2018}
}

@book{ccinlar2011probability,
  title={Probability and stochastics},
  author={{\c{C}}{\i}nlar, Erhan},
  volume={261},
  year={2011},
  publisher={Springer Science \& Business Media}
}

@book{neveu1975discrete,
  title={Discrete-parameter martingales},
  author={Neveu, Jacques},
  volume={10},
  year={1975},
  publisher={Elsevier}
}


%% Lower Bounds

@book{nemirovsky1983problem,
  title={Problem complexity and method efficiency in optimization},
  author={Nemirovsky, Arkadi{\u\i} Semenovich and Yudin, David Borisovich},
  year={1983},
  publisher={Wiley-Interscience Series in Discrete Mathematics}
}

@inproceedings{nesterov1983method,
  title={A method for unconstrained convex minimization problem with the rate of convergence \( {O}(1/k^{2}) \)},
  author={Nesterov, Yurii},
  booktitle={Doklady an {USSR}},
  volume={269},
  pages={543--547},
  year={1983}
} 

@article{nemirovskii1985optimal,
  title={Optimal methods of smooth convex minimization},
  author={Nemirovskii, Arkadi S and Nesterov, Yu E},
  journal={USSR Computational Mathematics and Mathematical Physics},
  volume={25},
  number={2},
  pages={21--30},
  year={1985},
  publisher={Elsevier}
}


@article{nesterov1988approach,
  title={On an approach to the construction of optimal methods of minimization of smooth convex functions},
  author={Nesterov, Yurii},
  journal={Ekonomika i Mateaticheskie Metody},
  volume={24},
  number={3},
  pages={509--517},
  year={1988}
}

@article{devolder2014first,
  title={First-order methods of smooth convex optimization with inexact oracle},
  author={Devolder, Olivier and Glineur, Fran{\c{c}}ois and Nesterov, Yurii},
  journal={Mathematical Programming},
  volume={146},
  number={1-2},
  pages={37--75},
  year={2014},
  publisher={Springer}
}


@article{allen2014linear,
  title={Linear coupling: An ultimate unification of gradient and mirror descent},
  author={Allen-Zhu, Zeyuan and Orecchia, Lorenzo},
  journal={arXiv preprint arXiv:1407.1537},
  year={2014}
}
