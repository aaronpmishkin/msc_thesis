    ### Optimization Monographs ###

@article{bubeck2015convex,
  author    = {S{\'{e}}bastien Bubeck},
  title     = {Convex Optimization: Algorithms and Complexity},
  journal   = {Found. Trends Mach. Learn.},
  volume    = {8},
  number    = {3-4},
  pages     = {231--357},
  year      = {2015},
}

@book{nesterov2004lectures,
  author    = {Yurii E. Nesterov},
  title     = {Introductory Lectures on Convex Optimization - {A} Basic Course},
  series    = {Applied Optimization},
  volume    = {87},
  publisher = {Springer},
  year      = {2004},
 }

@book{nocedal1999numerical,
  author    = {Jorge Nocedal and
               Stephen J. Wright},
  title     = {Numerical Optimization},
  publisher = {Springer},
  year      = {1999},
}

    ### SGD ###

@article{robbins1951sgd,
    author      = {Robbins, Herbert and Monro, Sutton},
    journal     = {Ann. Math. Statist.},
    month       = {09},
    number      = {3},
    pages       = {400--407},
    publisher   = {The Institute of Mathematical Statistics},
    title       = {A Stochastic Approximation Method},
    volume      = {22},
    year        = {1951}
}

@incollection{bengio2012practical,
  author    = {Yoshua Bengio},
  _editor    = {Gr{\'{e}}goire Montavon and
               Genevieve B. Orr and
               Klaus{-}Robert M{\"{u}}ller},
  title     = {Practical Recommendations for Gradient-Based Training of Deep Architectures},
  booktitle = {Neural Networks: Tricks of the Trade - Second Edition},
  series    = {Lecture Notes in Computer Science},
  volume    = {7700},
  pages     = {437--478},
  publisher = {Springer},
  year      = {2012},
}

%% SGD vs Variance-Reduction
@inproceedings{defazio2019effectiveness,
  author    = {Aaron Defazio and
               L{\'{e}}on Bottou},
  _editor    = {Hanna M. Wallach and
               Hugo Larochelle and
               Alina Beygelzimer and
               Florence d'Alch{\'{e}}{-}Buc and
               Emily B. Fox and
               Roman Garnett},
  title     = {On the Ineffectiveness of Variance Reduced Optimization for Deep Learning},
  booktitle = {Advances in Neural Information Processing Systems 32: {NeurIPS} 2019},
  pages     = {1753--1763},
  year      = {2019},
}

@article{choi2019empirical,
  title     = {On empirical comparisons of optimizers for deep learning},
  author    = {Choi, Dami and Shallue, Christopher J and Nado, Zachary and Lee, Jaehoon and Maddison, Chris J and Dahl, George E},
  journal   = {arXiv preprint arXiv:1910.05446},
  year      = {2019}
}

# Generalization Properties of SGD #

@inproceedings{zhang2017understanding,
  author    = {Chiyuan Zhang and
               Samy Bengio and
               Moritz Hardt and
               Benjamin Recht and
               Oriol Vinyals},
  title     = {Understanding deep learning requires rethinking generalization},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017},
  publisher = {OpenReview.net},
  year      = {2017},
}

% SGD for non-parametric methods
@inproceedings{belkin2019datainterp,
  author    = {Mikhail Belkin and
               Alexander Rakhlin and
               Alexandre B. Tsybakov},
  _editor    = {Kamalika Chaudhuri and
               Masashi Sugiyama},
  title     = {Does data interpolation contradict statistical optimality?},
  booktitle = {The 22nd International Conference on Artificial Intelligence and Statistics,
               {AISTATS} 2019},
  series    = {Proceedings of Machine Learning Research},
  volume    = {89},
  pages     = {1611--1619},
  publisher = {{PMLR}},
  year      = {2019},
}

@article{liang2018just,
  title     = {Just interpolate: {K}ernel ``ridgeless'' regression can generalize},
  author    = {Liang, Tengyuan and Rakhlin, Alexander},
  journal   = {arXiv preprint arXiv:1808.00387},
  year      = {2018}
}

@article{belkin2019reconciling,
  title={Reconciling modern machine-learning practice and the classical bias--variance trade-off},
  author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={32},
  pages={15849--15854},
  year={2019},
  publisher={National Acad Sciences}
}

@inproceedings{schapire1997boosting,
  author    = {Robert E. Schapire and
               Yoav Freund and
               Peter Barlett and
               Wee Sun Lee},
  _editor    = {Douglas H. Fisher},
  title     = {Boosting the margin: {A} new explanation for the effectiveness of
               voting methods},
  booktitle = {Proceedings of the Fourteenth International Conference on Machine
               Learning ({ICML} 1997)},
  pages     = {322--330},
  publisher = {Morgan Kaufmann},
  year      = {1997},
}

    ## Hybrid deterministic-stochastic methods ##
@article{friedlander2012hybrid,
  author    = {Michael P. Friedlander and
               Mark Schmidt},
  title     = {Hybrid Deterministic-Stochastic Methods for Data Fitting},
  journal   = {{SIAM} J. Scientific Computing},
  volume    = {34},
  number    = {3},
  year      = {2012},
}

@article{byrd2012sample,
  author    = {Richard H. Byrd and
               Gillian M. Chin and
               Jorge Nocedal and
               Yuchen Wu},
  title     = {Sample size selection in optimization methods for machine learning},
  journal   = {Math. Program.},
  volume    = {134},
  number    = {1},
  pages     = {127--155},
  year      = {2012},
}

@article{de2016big,
  title     = {Big batch {SGD}: Automated inference using adaptive batch sizes},
  author    = {De, Soham and Yadav, Abhay and Jacobs, David and Goldstein, Tom},
  journal   = {arXiv preprint arXiv:1610.05792},
  year      = {2016}
}
%% SGD for large-scale learning 
@inproceedings{bottou2007large,
  author    = {L{\'{e}}on Bottou and
               Olivier Bousquet},
  _editor    = {John C. Platt and
               Daphne Koller and
               Yoram Singer and
               Sam T. Roweis},
  title     = {The Tradeoffs of Large Scale Learning},
  booktitle = {Advances in Neural Information Processing Systems 20: {NeurIPS} 2007},
  pages     = {161--168},
  publisher = {Curran Associates, Inc.},
  year      = {2007},
}

%% SGD for over-parameterized models
@inproceedings{arora2018overparameterization,
  author    = {Sanjeev Arora and
               Nadav Cohen and
               Elad Hazan},
  _editor    = {Jennifer G. Dy and
               Andreas Krause},
  title     = {On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning,
               {ICML} 2018},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  pages     = {244--253},
  publisher = {{PMLR}},
  year      = {2018},
}

@inproceedings{zhou2019analysis,
  author    = {Difan Zou and
               Quanquan Gu},
  _editor    = {Hanna M. Wallach and
               Hugo Larochelle and
               Alina Beygelzimer and
               Florence d'Alch{\'{e}}{-}Buc and
               Emily B. Fox and
               Roman Garnett},
  title     = {An Improved Analysis of Training Over-parameterized Deep Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems 32: {NeurIPS} 2019},
  pages     = {2053--2062},
  year      = {2019},
}

 # workhorse quotes #

 @inproceedings{qian2019improvedrates,
  author    = {Xun Qian and
               Peter Richt{\'{a}}rik and
               Robert M. Gower and
               Alibek Sailanbayev and
               Nicolas Loizou and
               Egor Shulgin},
  _editor    = {Kamalika Chaudhuri and
               Ruslan Salakhutdinov},
  title     = {{SGD} with Arbitrary Sampling: General Analysis and Improved Rates},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning,
               {ICML} 2019},
  series    = {Proceedings of Machine Learning Research},
  volume    = {97},
  pages     = {5200--5209},
  publisher = {{PMLR}},
  year      = {2019},
}

@inproceedings{assran2019sgpush,
  author    = {Mahmoud Assran and
               Nicolas Loizou and
               Nicolas Ballas and
               Michael Rabbat},
  _editor    = {Kamalika Chaudhuri and
               Ruslan Salakhutdinov},
  title     = {Stochastic Gradient Push for Distributed Deep Learning},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning,
               {ICML} 2019},
  series    = {Proceedings of Machine Learning Research},
  volume    = {97},
  pages     = {344--353},
  publisher = {{PMLR}},
  year      = {2019},
}

@inproceedings{grosse2015scaling,
  author    = {Roger B. Grosse and
               Ruslan Salakhutdinov},
  _editor    = {Francis R. Bach and
               David M. Blei},
  title     = {Scaling up Natural Gradient by Sparsely Factorizing the Inverse Fisher
               Matrix},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning,
               {ICML} 2015},
  series    = {{JMLR} Workshop and Conference Proceedings},
  volume    = {37},
  pages     = {2304--2313},
  publisher = {JMLR.org},
  year      = {2015},
}
    ### Curvature Conditions ###

%% PL Condition
@inproceedings{karimi2016linear,
  author    = {Hamed Karimi and
               Julie Nutini and
               Mark Schmidt},
  _editor    = {Paolo Frasconi and
               Niels Landwehr and
               Giuseppe Manco and
               Jilles Vreeken},
  title     = {Linear Convergence of Gradient and Proximal-Gradient Methods Under
               the Polyak-{\L}ojasiewicz Condition},
  booktitle = {Machine Learning and Knowledge Discovery in Databases - European Conference,
               {ECML} {PKDD} 2016},
  series    = {Lecture Notes in Computer Science},
  volume    = {9851},
  pages     = {795--811},
  publisher = {Springer},
  year      = {2016},
}

%% Restricted Secant Inequality (RSI)
@article{zhang2013gradient,
  title     = {Gradient methods for convex minimization: better rates under weaker conditions},
  author    = {Zhang, Hui and Yin, Wotao},
  journal   = {arXiv preprint arXiv:1303.4645},
  year      = {2013}
}
    ### Growth Conditions ###

%% Strong Growth + Noise
@article{poljak1973pseudogradient,
  title={Pseudogradient adaptation and training algorithms},
  author={Polyak, BT and Tsypkin, Ya Z},
  journal={Automation and Remote Control},
  volume={34},
  pages={45--67},
  year={1973}
}

%% Maximal Strong Growth 
@article{tseng1998incremental,
  author    = {Paul Tseng},
  title     = {An Incremental Gradient(-Projection) Method with Momentum Term and
               Adaptive Stepsize Rule},
  journal   = {{SIAM} Journal on Optimization},
  volume    = {8},
  number    = {2},
  pages     = {506--531},
  year      = {1998}
}
%% Maximal Strong Growth 
@article{solodov1998incremental,
  author    = {Mikhail V. Solodov},
  title     = {Incremental Gradient Algorithms with Stepsizes Bounded Away from Zero},
  journal   = {Comp. Opt. and Appl.},
  volume    = {11},
  number    = {1},
  pages     = {23--35},
  year      = {1998}
}

%% Strong Growth 
@article{schmidt2013fast,
  title     = {Fast convergence of stochastic gradient descent under a strong growth condition},
  author    = {Schmidt, Mark and {Le Roux}, Nicolas},
  journal   = {arXiv preprint arXiv:1308.6370},
  year      = {2013}
}


%% Strong Growth is necessary for linear convergence; alternative weak growth condition (SGC + noise);
%% convergence to neighbourhood for (proximal-) gradient descent under WGC + noise.
@article{cevher2018linear,
  author    = {Volkan Cevher and
               Bang C{\^{o}}ng Vu},
  title     = {On the linear convergence of the stochastic gradient method with constant
               step-size},
  journal   = {Optim. Lett.},
  volume    = {13},
  number    = {5},
  pages     = {1177--1187},
  year      = {2019},
}


%% Strong Growth + Weak Growth + Noise
@article{khaled2020better,
  title     = {Better Theory for {SGD} in the Nonconvex World},
  author    = {Khaled, Ahmed and Richt{\'a}rik, Peter},
  journal   = {arXiv preprint arXiv:2002.03329},
  year      = {2020}
}


    ### Interpolation ###

# Belkin's Group #

@inproceedings{ma2018power,
  author    = {Siyuan Ma and
               Raef Bassily and
               Mikhail Belkin},
  _editor    = {Jennifer G. Dy and
               Andreas Krause},
  title     = {The Power of Interpolation: Understanding the Effectiveness of {SGD}
               in Modern Over-parametrized Learning},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning,
               {ICML} 2018},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  pages     = {3331--3340},
  publisher = {{PMLR}},
  year      = {2018},
}

@article{bassily2018exponential,
  title     = {On exponential convergence of {SGD} in non-convex over-parametrized learning},
  author    = {Bassily, Raef and Belkin, Mikhail and Ma, Siyuan},
  journal   = {arXiv preprint arXiv:1811.02564},
  year      = {2018}
}

@inproceedings{liu2020accelerating,
  author    = {Chaoyue Liu and
               Mikhail Belkin},
  title     = {Accelerating {SGD} with momentum for over-parameterized learning},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020},
  publisher = {OpenReview.net},
  year      = {2020},
}

# Oxford Group #
% Bounded stochastic Polyak step-size
@article{berrada2019training,
  title     = {Training neural networks for and by interpolation},
  author    = {Berrada, Leonard and Zisserman, Andrew and Kumar, M Pawan},
  journal   = {arXiv preprint arXiv:1906.05661},
  year      = {2019}
}

# UBC Group #

@inproceedings{vaswani2019fast,
  author    = {Sharan Vaswani and
               Francis Bach and
               Mark W. Schmidt},
  _editor    = {Kamalika Chaudhuri and
               Masashi Sugiyama},
  title     = {Fast and Faster Convergence of {SGD} for Over-Parameterized Models
               and an Accelerated Perceptron},
  booktitle = {The 22nd International Conference on Artificial Intelligence and Statistics,
               {AISTATS} 2019},
  series    = {Proceedings of Machine Learning Research},
  volume    = {89},
  pages     = {1195--1204},
  publisher = {{PMLR}},
  year      = {2019}
}


@inproceedings{vaswani2019painless,
  author    = {Sharan Vaswani and
               Aaron Mishkin and
               Issam H. Laradji and
               Mark Schmidt and
               Gauthier Gidel and
               Simon Lacoste{-}Julien},
  _editor    = {Hanna M. Wallach and
               Hugo Larochelle and
               Alina Beygelzimer and
               Florence d'Alch{\'{e}}{-}Buc and
               Emily B. Fox and
               Roman Garnett},
  title     = {Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence
               Rates},
  booktitle = {Advances in Neural Information Processing Systems 32: {NeurIPS} 2019},
  pages     = {3727--3740},
  year      = {2019},
}

@inproceedings{meng2020fastandfurious,
  author    = {Si Yi Meng and
               Sharan Vaswani and
               Issam Hadj Laradji and
               Mark Schmidt and
               Simon Lacoste{-}Julien},
  _editor    = {Silvia Chiappa and
               Roberto Calandra},
  title     = {Fast and Furious Convergence: Stochastic Second Order Methods under
               Interpolation},
  booktitle = {The 23rd International Conference on Artificial Intelligence and Statistics,
               {AISTATS} 2020},
  series    = {Proceedings of Machine Learning Research},
  volume    = {108},
  pages     = {1375--1386},
  publisher = {{PMLR}},
  year      = {2020},
}
 
@article{loizou2020sps,
  title     = {Stochastic {P}olyak step-size for {SGD}: An adaptive learning rate for fast convergence},
  author    = {Loizou, Nicolas and Vaswani, Sharan and Laradji, Issam and Lacoste-Julien, Simon},
  journal   = {arXiv preprint arXiv:2002.10542},
  year      = {2020}
}

@article{vaswani2020adaptive,
  title     = {Adaptive Gradient Methods Converge Faster with Over-Parameterization (and you can do a line-search)},
  author    = {Vaswani, Sharan and Kunstner, Frederik and Laradji, Issam and Meng, Si Yi and Schmidt, Mark and Lacoste-Julien, Simon},
  journal   = {arXiv preprint arXiv:2006.06835},
  year      = {2020}
}

# Other Affiliations #

@article{wu2019global,
  title={Global convergence of adaptive gradient methods for an over-parameterized neural network},
  author={Wu, Xiaoxia and Du, Simon S and Ward, Rachel},
  journal={arXiv preprint arXiv:1902.07111},
  year={2019}
}

    ### Step-Size Selection ###

# Armijo Line-Search #

%% original Armijo line-search reference
@article{armijo1966ls,
    author      = {Armijo, Larry},
    journal     = {Pacific Journal of Mathematics},
    number      = {1},
    pages       = {1--3},
    publisher   = {Pacific Journal of Mathematics, A Non-profit Corporation},
    title       = {Minimization of functions having Lipschitz continuous first partial derivatives.},
    volume      = {16},
    year        = {1966}
}

%% golden-section search
@article{avriel1968golden,
  title     = {Golden block search for the maximum of unimodal functions},
  author    = {Avriel, Mordecai and Wilde, Douglass J},
  journal   = {Management Science},
  volume    = {14},
  number    = {5},
  pages     = {307--319},
  year      = {1968},
  publisher = {INFORMS}
}

@article{krejic2013line,
  author    = {Natasa Krejic and
               Natasa Krklec},
  title     = {Line search methods with variable sample size for unconstrained optimization},
  journal   = {J. Comput. Appl. Math.},
  volume    = {245},
  pages     = {213--231},
  year      = {2013},
}

@article{paquette2020stochastic,
  author    = {Courtney Paquette and
               Katya Scheinberg},
  title     = {A Stochastic Line Search Method with Expected Complexity Analysis},
  journal   = {{SIAM} J. Optim.},
  volume    = {30},
  number    = {1},
  pages     = {349--376},
  year      = {2020},
}

@inproceedings{fridovich2019choosing,
  title     = {Choosing the Step Size: Intuitive Line Search Algorithms with Efficient Convergence},
  author    = {Fridovich-Keil, Sara and Recht, Benjamin},
  booktitle = {The 11th Workshop on Optimization for Machine Learning ({OPT} 2019)},
  year      = {2019},
}

@article{mahsereci2017pls,
  author    = {Maren Mahsereci and
               Philipp Hennig},
  title     = {Probabilistic Line Searches for Stochastic Optimization},
  journal   = {J. Mach. Learn. Res.},
  volume    = {18},
  pages     = {119:1--119:59},
  year      = {2017},
}

# Adaptive Step-Sizes #

@inproceedings{li2019convergence,
  author    = {Xiaoyu Li and
               Francesco Orabona},
  _editor    = {Kamalika Chaudhuri and
               Masashi Sugiyama},
  title     = {On the Convergence of Stochastic Gradient Descent with Adaptive Stepsizes},
  booktitle = {The 22nd International Conference on Artificial Intelligence and Statistics,
               {AISTATS} 2019},
  series    = {Proceedings of Machine Learning Research},
  volume    = {89},
  pages     = {983--992},
  publisher = {{PMLR}},
  year      = {2019},
}

@inproceedings{orabona2017coin,
  author    = {Francesco Orabona and
               Tatiana Tommasi},
  _editor    = {Isabelle Guyon and
               Ulrike von Luxburg and
               Samy Bengio and
               Hanna M. Wallach and
               Rob Fergus and
               S. V. N. Vishwanathan and
               Roman Garnett},
  title     = {Training Deep Networks without Learning Rates Through Coin Betting},
  booktitle = {Advances in Neural Information Processing Systems 30: {NeurIPS} 2017},
  pages     = {2160--2170},
  year      = {2017},
}

@inproceedings{schaul2013no,
  author    = {Tom Schaul and
               Sixin Zhang and
               Yann LeCun},
  title     = {No more pesky learning rates},
  booktitle = {Proceedings of the 30th International Conference on Machine Learning,
               {ICML} 2013},
  series    = {{JMLR} Workshop and Conference Proceedings},
  volume    = {28},
  pages     = {343--351},
  publisher = {JMLR.org},
  year      = {2013},
}

    # hyper-gradient a meta-descent algorithms #

@inproceedings{baydin2018hypergradient,
  author    = {Atilim Gunes Baydin and
               Robert Cornish and
               David Mart{\'{\i}}nez{-}Rubio and
               Mark Schmidt and
               Frank Wood},
  title     = {Online Learning Rate Adaptation with Hypergradient Descent},
  booktitle = {6th International Conference on Learning Representations, {ICLR} 2018},
  publisher = {OpenReview.net},
  year      = {2018},
}

@article{schraudolph1999local,
  title     = {Local gain adaptation in stochastic gradient descent},
  author    = {Schraudolph, Nicol N},
  year      = {1999},
  publisher = {IET}
}

@inproceedings{sutton1992gain,
  title     = {Gain adaptation beats least squares},
  author    = {Sutton, Richard S},
  booktitle = {Proceedings of the 7th Yale workshop on adaptive and learning systems},
  volume    = {161168},
  year      = {1992}
}

@article{almeida1998parameter,
  title     = {Parameter adaptation in stochastic optimization},
  author    = {Almeida, Lu{\'\i}s B and Langlois, Thibault and Amaral, Jos{\'e} D and Plakhov, Alexander},
  journal   = {On-Line Learning in Neural Networks, Publications of the Newton Institute},
  pages     = {111--134},
  year      = {1998}
}

@incollection{plagianakos2001learning,
  title     = {Learning rate adaptation in stochastic gradient descent},
  author    = {Plagianakos, VP and Magoulas, GD and Vrahatis, MN},
  booktitle = {Advances in convex analysis and global optimization},
  pages     = {433--444},
  year      = {2001},
  publisher = {Springer}
}

@article{shao2000rates,
  title     = {Rates of convergence of adaptive step-size of stochastic approximation algorithms},
  author    = {Shao, S and Yip, Percy PC},
  journal   = {Journal of mathematical analysis and applications},
  volume    = {244},
  number    = {2},
  pages     = {333--347},
  year      = {2000},
  publisher = {Elsevier}
} 
 

    # heuristics #

@inproceedings{rolinek2018l4,
  author    = {Michal Rolinek and
               Georg Martius},
  _editor    = {Samy Bengio and
               Hanna M. Wallach and
               Hugo Larochelle and
               Kristen Grauman and
               Nicol{\`{o}} Cesa{-}Bianchi and
               Roman Garnett},
  title     = {{L4:} Practical loss-based step-size adaptation for deep learning},
  booktitle = {Advances in Neural Information Processing Systems 31: {NeurIPS} 2018},
  pages     = {6434--6444},
  year      = {2018},
}

%% Stochastic Barzilai Borwein step-sizes 
@inproceedings{tan2016bb,
  author    = {Conghui Tan and
               Shiqian Ma and
               Yu{-}Hong Dai and
               Yuqiu Qian},
  _editor    = {Daniel D. Lee and
               Masashi Sugiyama and
               Ulrike von Luxburg and
               Isabelle Guyon and
               Roman Garnett},
  title     = {Barzilai-Borwein Step Size for Stochastic Gradient Descent},
  booktitle = {Advances in Neural Information Processing Systems 29: {NeurIPS} 2016},
  pages     = {685--693},
  year      = {2016},
}


    ### Almost Sure Convergence of SGD ###

# Weakest Assumptions (according to Orabona)
@article{bertsekas2000gradient,
  title     = {Gradient convergence in gradient methods with errors},
  author    = {Bertsekas, Dimitri P. and Tsitsiklis, John N.},
  journal   = {SIAM Journal on Optimization},
  volume    = {10},
  number    = {3},
  pages     = {627--642},
  year      = {2000},
  publisher = {SIAM}
}

# survey of almost sure convergence of stochastic gradient, subgradient, and proximal methods for finite-sum functions.
@article{bertsekas2011incremental,
  title     = {Incremental gradient, subgradient, and proximal methods for convex optimization: A survey},
  author    = {Bertsekas, Dimitri P},
  journal   = {Optimization for Machine Learning},
  volume    = {2010},
  number    = {1-38},
  pages     = {3},
  year      = {2011},
  publisher = {MIT press}
}

@article{lei2019stochastic,
  title     = {Stochastic gradient descent for nonconvex learning without bounded gradient assumptions},
  author    = {Lei, Yunwen and Hu, Ting and Li, Guiying and Tang, Ke},
  journal   = {IEEE Transactions on Neural Networks and Learning Systems},
  year      = {2019},
  publisher = {IEEE}
}

# almost sure convergence of SGD with diminishing step size; larger bound on step-size than previous work.
@article{nguyen2018sgd,
  title     = {S{GD} and {H}ogwild! convergence without the bounded gradients assumption},
  author    = {Nguyen, Lam M. and Nguyen, Phuong Ha and van Dijk, Marten and Richt{\'a}rik, Peter and Scheinberg, Katya and Tak{\'a}{\v{c}}, Martin},
  journal   = {arXiv preprint arXiv:1802.03801},
  year      = {2018}
}

@phdthesis{bottou1991approche,
  title     = {Une approche théorique de l'apprentissage connexionniste et applications à la reconnaissance de la parole},
  author    = {Bottou, L{\'e}on},
  year      = {1991}
}

@book{ccinlar2011probability,
  title     = {Probability and stochastics},
  author    = {{\c{C}}{\i}nlar, Erhan},
  volume    = {261},
  year      = {2011},
  publisher = {Springer Science \& Business Media}
}

@book{neveu1975discrete,
  title     = {Discrete-parameter martingales},
  author    = {Neveu, Jacques},
  volume    = {10},
  year      = {1975},
  publisher = {Elsevier}
}


    ### Variance Reduction ###

%% Original SAG Paper
@inproceedings{leroux2012sag,
  author    = {Nicolas {Le Roux} and
               Mark Schmidt and
               Francis R. Bach},
  _editor    = {Peter L. Bartlett and
               Fernando C. N. Pereira and
               Christopher J. C. Burges and
               L{\'{e}}on Bottou and
               Kilian Q. Weinberger},
  title     = {A Stochastic Gradient Method with an Exponential Convergence Rate
               for Finite Training Sets},
  booktitle = {Advances in Neural Information Processing Systems 25: {NeurIPS} 2012},
  pages     = {2672--2680},
  year      = {2012},
}

%% SAG Journal Version
@article{schmidt2017sag,
  author    = {Mark Schmidt and
               Nicolas {Le Roux} and
               Francis R. Bach},
  title     = {Minimizing finite sums with the stochastic average gradient},
  journal   = {Math. Program.},
  volume    = {162},
  number    = {1-2},
  pages     = {83--112},
  year      = {2017},
}

%% Original SAGA Paper
@inproceedings{defazio2014saga,
  author    = {Aaron Defazio and
               Francis R. Bach and
               Simon Lacoste{-}Julien},
  _editor    = {Zoubin Ghahramani and
               Max Welling and
               Corinna Cortes and
               Neil D. Lawrence and
               Kilian Q. Weinberger},
  title     = {{SAGA:} {A} Fast Incremental Gradient Method With Support for Non-Strongly
               Convex Composite Objectives},
  booktitle = {Advances in Neural Information Processing Systems 27: {NeurIPS} 2014},
  pages     = {1646--1654},
  year      = {2014},
}

%% Original SVRG Paper
@inproceedings{johnson2013svrg,
  author    = {Rie Johnson and
               Tong Zhang},
  _editor    = {Christopher J. C. Burges and
               L{\'{e}}on Bottou and
               Zoubin Ghahramani and
               Kilian Q. Weinberger},
  title     = {Accelerating Stochastic Gradient Descent using Predictive Variance
               Reduction},
  booktitle = {Advances in Neural Information Processing Systems 26: {NeurIPS} 2013},
  pages     = {315--323},
  year      = {2013},
}

    ### Proximal-Gradient Methods ###

@article{ghadimi2012optimal1,
  author    = {Saeed Ghadimi and
               Guanghui Lan},
  title     = {Optimal Stochastic Approximation Algorithms for Strongly Convex Stochastic
               Composite Optimization {I:} {A} Generic Algorithmic Framework},
  journal   = {{SIAM} J. Optim.},
  volume    = {22},
  number    = {4},
  pages     = {1469--1492},
  year      = {2012},
}

@article{ghadimi2013optimal2,
  author    = {Saeed Ghadimi and
               Guanghui Lan},
  title     = {Optimal Stochastic Approximation Algorithms for Strongly Convex Stochastic
               Composite Optimization, {II:} Shrinking Procedures and Optimal Algorithms},
  journal   = {{SIAM} J. Optim.},
  volume    = {23},
  number    = {4},
  pages     = {2061--2089},
  year      = {2013},
}

 # Accelerated Proximal-Gradient #
@article{beck2009fista,
  author    = {Amir Beck and
               Marc Teboulle},
  title     = {A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse
               Problems},
  journal   = {{SIAM} J. Imaging Sci.},
  volume    = {2},
  number    = {1},
  pages     = {183--202},
  year      = {2009},
}

@article{nesterov2007proximalgradient,
  author    = {Yurii E. Nesterov},
  title     = {Gradient methods for minimizing composite functions},
  journal   = {Math. Program.},
  volume    = {140},
  number    = {1},
  pages     = {125--161},
  year      = {2013},
}


    ### "Adaptive" Methods ###

%% Original Adagrad Paper
@article{duchi2011adagrad,
  author    = {John C. Duchi and
               Elad Hazan and
               Yoram Singer},
  title     = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  journal   = {J. Mach. Learn. Res.},
  volume    = {12},
  pages     = {2121--2159},
  year      = {2011},
}

%% Original Adam Paper
@inproceedings{kingma2015adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  _editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015},
  year      = {2015},
}

%% AdaDelta
@article{zeiler2012adadelta,
  title     = {{AdaDelta}: an adaptive learning rate method},
  author    = {Zeiler, Matthew D},
  journal   = {arXiv preprint arXiv:1212.5701},
  year      = {2012}
}

%% RMSProp
@article{tieleman2012rmsprop,
  title     = {Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude},
  author    = {Tieleman, Tijmen and Hinton, Geoffrey},
  journal   = {Coursera: Neural networks for machine learning},
  year      = {2012}
}

%% AdaBound
@inproceedings{luo2019adabound,
  author    = {Liangchen Luo and
               Yuanhao Xiong and
               Yan Liu and
               Xu Sun},
  title     = {Adaptive Gradient Methods with Dynamic Bound of Learning Rate},
  booktitle = {7th International Conference on Learning Representations, {ICLR} 2019},
  publisher = {OpenReview.net},
  year      = {2019},
}

    ### Lower Bounds ###

    # deterministic setting #

@book{nemirovsky1983problem,
  title     = {Problem complexity and method efficiency in optimization},
  author    = {Nemirovsky, Arkadi{\u\i} Semenovich and Yudin, David Borisovich},
  year      = {1983},
  publisher = {Wiley-Interscience Series in Discrete Mathematics}
}

%% \epsilon-optimality for (strongly-) convex functions
@inproceedings{arjevani2016iteration,
  author    = {Yossi Arjevani and
               Ohad Shamir},
 _editor    = {Maria{-}Florina Balcan and
               Kilian Q. Weinberger},
  title     = {On the Iteration Complexity of Oblivious First-Order Optimization
               Algorithms},
  booktitle = {Proceedings of the 33nd International Conference on Machine Learning,
               {ICML} 2016},
  series    = {{JMLR} Workshop and Conference Proceedings},
  volume    = {48},
  pages     = {908--916},
  publisher = {JMLR.org},
  year      = {2016},
}

@article{carmon2019lower,
  title     = {Lower bounds for finding stationary points I},
  author    = {Carmon, Yair and Duchi, John C and Hinder, Oliver and Sidford, Aaron},
  journal   = {Mathematical Programming},
  pages     = {1--50},
  year      = {2019},
  publisher = {Springer}
}

    # stochastic setting #

@article{drori2019complexity,
  title     = {The complexity of finding stationary points with stochastic gradient descent},
  author    = {Drori, Yoel and Shamir, Ohad},
  journal   = {arXiv preprint arXiv:1910.01845},
  year      = {2019}
}

@article{arjevani2019lower,
  title     = {Lower bounds for non-convex stochastic optimization},
  author    = {Arjevani, Yossi and Carmon, Yair and Duchi, John C and Foster, Dylan J and Srebro, Nathan and Woodworth, Blake},
  journal   = {arXiv preprint arXiv:1912.02365},
  year      = {2019}
}

    ### Acceleration ###

# Deterministic Acceleration #

# original accelerated method
@inproceedings{nesterov1983method,
  title     = {A method for unconstrained convex minimization problem with the rate of convergence \( {O}(1/k^{2}) \)},
  author    = {Nesterov, Yurii},
  booktitle = {Doklady an {USSR}},
  volume    = {269},
  pages     = {543--547},
  year      = {1983}
} 

@article{nemirovsky1985optimal,
  title     = {Optimal methods of smooth convex minimization},
  author    = {Nemirovsky, Arkadi S and Nesterov, Yu E},
  journal   = {USSR Computational Mathematics and Mathematical Physics},
  volume    = {25},
  number    = {2},
  pages     = {21--30},
  year      = {1985},
  publisher = {Elsevier}
}


@article{nesterov1988approach,
  title     = {On an approach to the construction of optimal methods of minimization of smooth convex functions},
  author    = {Nesterov, Yurii},
  journal   = {Ekonomika i Mateaticheskie Metody},
  volume    = {24},
  number    = {3},
  pages     = {509--517},
  year      = {1988}
}

# linear coupling
@inproceedings{allen2014linear,
  author    = {Zeyuan {Allen Zhu} and
               Lorenzo Orecchia},
  _editor    = {Christos H. Papadimitriou},
  title     = {Linear {C}oupling: An Ultimate Unification of Gradient and Mirror Descent},
  booktitle = {8th Innovations in Theoretical Computer Science Conference, {ITCS}
               2017},
  series    = {LIPIcs},
  volume    = {67},
  pages     = {3:1--3:22},
  publisher = {Schloss Dagstuhl - Leibniz-Zentrum f{\"{u}}r Informatik},
  year      = {2017},
}


# Stochastic Acceleration #

@inproceedings{schmidt2011convergence,
  author    = {Mark Schmidt and
               Nicolas {Le Roux} and
               Francis R. Bach},
  _editor    = {John Shawe{-}Taylor and
               Richard S. Zemel and
               Peter L. Bartlett and
               Fernando C. N. Pereira and
               Kilian Q. Weinberger},
  title     = {Convergence Rates of Inexact Proximal-Gradient Methods for Convex
               Optimization},
  booktitle = {Advances in Neural Information Processing Systems 24: {NeurIPS} 2011},
  pages     = {1458--1466},
  year      = {2011},
}

@article{allen-zhu2017katyusha,
  author    = {Zeyuan Allen{-}Zhu},
  title     = {Katyusha: The First Direct Acceleration of Stochastic Gradient Methods},
  journal   = {J. Mach. Learn. Res.},
  volume    = {18},
  pages     = {221:1--221:51},
  year      = {2017},
  url       = {http://jmlr.org/papers/v18/16-410.html},
}

@inproceedings{allen-zhou2018katyushax,
  author    = {Zeyuan Allen{-}Zhu},
  _editor    = {Jennifer G. Dy and
               Andreas Krause},
  title     = {Katyusha {X}: Practical Momentum Method for Stochastic Sum-of-Nonconvex
               Optimization},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning,
               {ICML} 2018},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  pages     = {179--185},
  publisher = {{PMLR}},
  year      = {2018},
}

@inproceedings{tang2018restkatyusha,
  author    = {Junqi Tang and
               Mohammad Golbabaee and
               Francis Bach and
               Mike E. Davies},
  _editor    = {Samy Bengio and
               Hanna M. Wallach and
               Hugo Larochelle and
               Kristen Grauman and
               Nicol{\`{o}} Cesa{-}Bianchi and
               Roman Garnett},
  title     = {Rest-{K}atyusha: {E}xploiting the Solution's Structure via Scheduled Restart
               Schemes},
  booktitle = {Advances in Neural Information Processing Systems 31: {NeurIPS} 2018},
  pages     = {427--438},
  year      = {2018},
}
%% loopless SVRG and Katyusha.
@inproceedings{kovalev2020loopless,
  author    = {Dmitry Kovalev and
               Samuel Horv{\'{a}}th and
               Peter Richt{\'{a}}rik},
  _editor    = {Aryeh Kontorovich and
               Gergely Neu},
  title     = {Don't Jump Through Hoops and Remove Those Loops: {SVRG} and {K}atyusha
               are Better Without the Outer Loop},
  booktitle = {Algorithmic Learning Theory, {ALT} 2020},
  series    = {Proceedings of Machine Learning Research},
  volume    = {117},
  pages     = {451--467},
  publisher = {{PMLR}},
  year      = {2020},
}

% acceleration with errors
@article{aspremont2008approximate,
  author    = {Alexandre d'Aspremont},
  title     = {Smooth Optimization with Approximate Gradient},
  journal   = {{SIAM} J. Optim.},
  volume    = {19},
  number    = {3},
  pages     = {1171--1183},
  year      = {2008},
  url       = {https://doi.org/10.1137/060676386},
}

@article{devolder2014first,
  title     = {First-order methods of smooth convex optimization with inexact oracle},
  author    = {Devolder, Olivier and Glineur, Fran{\c{c}}ois and Nesterov, Yurii},
  journal   = {Mathematical Programming},
  volume    = {146},
  number    = {1-2},
  pages     = {37--75},
  year      = {2014},
  publisher={Springer}
}

%% acceleration with additive noise
@inproceedings{cohen2018acceleration,
  author    = {Michael Cohen and
               Jelena Diakonikolas and
               Lorenzo Orecchia},
  _editor    = {Jennifer G. Dy and
               Andreas Krause},
  title     = {On Acceleration with Noise-Corrupted Gradients},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning,
               {ICML} 2018},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  pages     = {1018--1027},
  publisher = {{PMLR}},
  year      = {2018},
}

%% catalyst: generic acceleration via proximal-point.
@article{lin2017catalyst,
  author    = {Hongzhou Lin and
               Julien Mairal and
               Za{\"{\i}}d Harchaoui},
  title     = {Catalyst Acceleration for First-order Convex Optimization: from Theory
               to Practice},
  journal   = {J. Mach. Learn. Res.},
  volume    = {18},
  pages     = {212:1--212:54},
  year      = {2017},
}

@inproceedings{frostig2015unregularizing,
  author    = {Roy Frostig and
               Rong Ge and
               Sham M. Kakade and
               Aaron Sidford},
  _editor    = {Francis R. Bach and
               David M. Blei},
  title     = {Un-regularizing: approximate proximal point and faster stochastic
               algorithms for empirical risk minimization},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning,
               {ICML} 2015},
  series    = {{JMLR} Workshop and Conference Proceedings},
  volume    = {37},
  pages     = {2540--2548},
  publisher = {JMLR.org},
  year      = {2015},
}

@article{assran2020convergence,
  title     = {On the Convergence of Nesterov's Accelerated Gradient Method in Stochastic Settings},
  author    = {Assran, Mahmoud and Rabbat, Michael},
  journal   = {arXiv preprint arXiv:2002.12414},
  year      = {2020}
}

@article{chen2020understanding,
  title     = {Understanding Accelerated Stochastic Gradient Descent via the Growth Condition},
  author    = {Chen, You-Lin and Kolar, Mladen},
  journal   = {arXiv preprint arXiv:2006.06782},
  year      = {2020}
}

@inproceedings{jain2018accelerating,
  author    = {Prateek Jain and
               Sham M. Kakade and
               Rahul Kidambi and
               Praneeth Netrapalli and
               Aaron Sidford},
  _editor    = {S{\'{e}}bastien Bubeck and
               Vianney Perchet and
               Philippe Rigollet},
  title     = {Accelerating Stochastic Gradient Descent for Least Squares Regression},
  booktitle = {Conference On Learning Theory, {COLT} 2018},
  series    = {Proceedings of Machine Learning Research},
  volume    = {75},
  pages     = {545--604},
  publisher = {{PMLR}},
  year      = {2018},
}
